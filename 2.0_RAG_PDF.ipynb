{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82b260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key is set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if os.environ['OPENAI_API_KEY']:\n",
    "    print(\"API Key is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc3fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af144af",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-5-nano\",temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce975d09",
   "metadata": {},
   "source": [
    "## **RAG IMPLEMENTATION WITH PDFs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64578884",
   "metadata": {},
   "source": [
    "#### **STEP 1: Extracting Text from PDFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13de89b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 0, 'page_label': '1'}, page_content='Tell us about your PDF experience.\\nOneLake in Microsoft Fabric\\ndocumentation\\nOneLake is a single, unified, logical data lake for the whole organization. OneLake comes\\nautomatically with every Microsoft Fabric tenant with no infrastructure to manage.\\nAbout OneLake\\nｅOVERVIEW\\nWhat is OneLake?\\nOneLake security\\nOneLake catalog\\nOneLake access and APIs\\n｀DEPLOY\\nImplement medallion lakehouse architecture\\nｂGET STARTED\\nCreate a lakehouse with OneLake\\nOneLake file explorer\\nFind data in the OneLake catalog\\nUse Iceberg tables in OneLake\\nOneLake shortcuts\\nｐCONCEPT\\nWhat are shortcuts?\\nｂGET STARTED\\nCreate a shortcut\\nｃHOW-TO GUIDE'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 1, 'page_label': '2'}, page_content='Access shortcuts\\nOneLake and Azure integration\\nｃHOW-TO GUIDE\\nIntegrate OneLake with Azure Databricks\\nIntegrate OneLake with Azure HDInsight\\nIntegrate OneLake with Azure Storage Explorer\\nIntegrate OneLake with Azure Synapse Analytics'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 2, 'page_label': '3'}, page_content=\"OneLake, the OneDrive for data\\nArticle• 07/25/2024\\nOneLake is a single, unified, logical data lake for your whole organization. A data Lake\\nprocesses large volumes of data from various sources. Like OneDrive, OneLake comes\\nautomatically with every Microsoft Fabric tenant and is designed to be the single place\\nfor all your analytics data. OneLake brings customers:\\nOne data lake for the entire organization\\nOne copy of data for use with multiple analytical engines\\nBefore OneLake, it was easier for customers to create multiple lakes for different\\nbusiness groups rather than collaborating on a single lake, even with the extra overhead\\nof managing multiple resources. OneLake focuses on removing these challenges by\\nimproving collaboration. Every customer tenant has exactly one OneLake. There can\\nnever be more than one and if you have Fabric, there can never be zero. Every Fabric\\ntenant automatically provisions OneLake, with no extra resources to set up or manage.\\nThe concept of a tenant is a unique benefit of a SaaS service. Knowing where a\\ncustomer’s organization begins and ends provides a natural governance and compliance\\nboundary, which is under the control of a tenant admin. Any data that lands in OneLake\\nis governed by default. While all data is within the boundaries set by the tenant admin,\\nit's important that this admin doesn't become a central gatekeeper preventing other\\nparts of the organization from contributing to OneLake.\\nWithin a tenant, you can create any number of workspaces. Workspaces enable different\\nparts of the organization to distribute ownership and access policies. Each workspace is\\npart of a capacity that is tied to a specific region and is billed separately.\\nOne data lake for the entire organization\\nGoverned by default with distributed ownership for\\ncollaboration\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 3, 'page_label': '4'}, page_content=\"Within a workspace, you can create data items and you access all data in OneLake\\nthrough data items. Similar to how Office stores Word, Excel, and PowerPoint files in\\nOneDrive, Fabric stores lakehouses, warehouses, and other items in OneLake. Items can\\ngive tailored experiences for each persona, such the Apache Spark developer experience\\nin a lakehouse.\\nFor more information on how to get started using OneLake, see Creating a lakehouse\\nwith OneLake.\\nOneLake is open at every level. OneLake is built on top of Azure Data Lake Storage\\n(ADLS) Gen2 and can support any type of file, structured or unstructured. All Fabric data\\nitems like data warehouses and lakehouses store their data automatically in OneLake in\\nDelta Parquet format. If a data engineer loads data into a lakehouse using Apache Spark,\\nand then a SQL developer uses T-SQL to load data in a fully transactional data\\nwarehouse, both are contributing to the same data lake. OneLake stores all tabular data\\nin Delta Parquet format.\\nOneLake supports the same ADLS Gen2 APIs and SDKs to be compatible with existing\\nADLS Gen2 applications, including Azure Databricks. You can address data in OneLake as\\nif it's one big ADLS storage account for the entire organization. Every workspace\\nappears as a container within that storage account, and different data items appear as\\nfolders within those containers.\\n\\uf80a\\nOpen at every level\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 4, 'page_label': '5'}, page_content='For more information on APIs and endpoints, see OneLake access and APIs. For\\nexamples of OneLake integrations with Azure, see Azure Synapse Analytics, Azure\\nstorage explorer, Azure Databricks, and Azure HDInsight articles.\\nOneLake is the OneDrive for data. Just like OneDrive, you can easily explore OneLake\\ndata from Windows using the OneLake file explorer for Windows. You can navigate all\\nyour workspaces and data items, easily uploading, downloading, or modifying files just\\nlike you do in Office. The OneLake file explorer simplifies working with data lakes,\\nallowing even nontechnical business users to use them.\\nFor more information, see OneLake file explorer.\\nOneLake aims to give you the most value possible out of a single copy of data without\\ndata movement or duplication. You no longer need to copy data just to use it with\\nanother engine or to break down silos so you can analyze the data with data from other\\nsources.\\n\\uf80a\\nOneLake file explorer for Windows\\nOne copy of data\\nShortcuts connect data across domains without data\\nmovement'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 5, 'page_label': '6'}, page_content=\"Shortcuts allow your organization to easily share data between users and applications\\nwithout having to move and duplicate information unnecessarily. When teams work\\nindependently in separate workspaces, shortcuts enable you to combine data across\\ndifferent business groups and domains into a virtual data product to fit a user’s specific\\nneeds.\\nA shortcut is a reference to data stored in other file locations. These file locations can be\\nwithin the same workspace or across different workspaces, within OneLake or external to\\nOneLake in ADLS, S3, or Dataverse — with more target locations coming soon. No\\nmatter the location, shortcuts make files and folders look like you have them stored\\nlocally.\\nFor more information on how to use shortcuts, see OneLake shortcuts.\\nWhile applications might have separation of storage and computing, the data is often\\noptimized for a single engine, which makes it difficult to reuse the same data for\\nmultiple applications. With Fabric, the different analytical engines (T-SQL, Apache Spark,\\nAnalysis Services, etc.) store data in the open Delta Parquet format to allow you to use\\nthe same data across multiple engines.\\nThere's no longer a need to copy data just to use it with another engine. You're always\\nable to choose the best engine for the job that you're trying to do. For example, imagine\\nyou have a team of SQL engineers building a fully transactional data warehouse. They\\ncan use the T-SQL engine and all the power of T-SQL to create tables, transform data,\\n\\uf80a\\nOne copy of data with multiple analytical engines\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 6, 'page_label': '7'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nand load the data to tables. If a data scientist wants to make use of this data, they no\\nlonger need to go through a special Spark/SQL driver. OneLake stores all data in Delta\\nParquet format. Data scientists can use the full power of the Spark engine and its open-\\nsource libraries directly over the data.\\nBusiness users can build Power BI reports directly on top of OneLake using the new\\nDirect Lake mode in the Analysis Services engine. The Analysis Services engine is what\\npowers Power BI semantic models, and it has always offered two modes of accessing\\ndata: import and direct query. Direct Lake mode gives users all the speed of import\\nwithout needing to copy the data, combining the best of import and direct query. For\\nmore information, see Direct Lake.\\nExample diagram showing loading data using Spark, querying using T-SQL, and viewing\\nthe data in a Power BI report.\\nCreating a lakehouse with OneLake\\n\\uf80a\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 7, 'page_label': '8'}, page_content=\"Bring your data to OneLake with\\nLakehouse\\nArticle• 03/13/2025\\nThis tutorial is a quick guide to creating a lakehouse and getting started with the basic\\nmethods of interacting with it. After completing this tutorial, you'll have a lakehouse\\nprovisioned inside of Microsoft Fabric working on top of OneLake.\\n1. Sign in to Microsoft Fabric .\\n2. Select Workspaces from the left-hand menu.\\n3. To open your workspace, enter its name in the search textbox located at the top\\nand select it from the search results.\\n4. In the upper left corner of the workspace home page, select New item and then\\nchoose Lakehouse from the Store data section.\\n5. Give your lakehouse a name and select Create.\\n6. A new lakehouse is created and, if this lakehouse is your first OneLake item,\\nOneLake is provisioned behind the scenes.\\nAt this point, you have a lakehouse running on top of OneLake. Next, add some data\\nand start organizing your lake.\\n1. In the file browser on the left, select more options (...) next to Files and then select\\nNew subfolder. Name your subfolder and select Create.\\nCreate a lakehouse\\n\\uf80a\\nLoad data into a lakehouse\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 8, 'page_label': '9'}, page_content='2. You can repeat this step to add more subfolders as needed.\\n3. Select more options (...) next to your folder, and then select Upload > Upload files\\nfrom the menu.\\n4. Choose the file you want from your local machine and then select Upload.\\n5. You now have data in OneLake. To add data in bulk or schedule data loads into\\nOneLake, use the Get data button to create pipelines. Find more details about\\noptions for getting data in Microsoft Fabric decision guide: copy activity, dataflow,\\nor Spark.\\n6. Select more options (...) for the file you uploaded and select Properties from the\\nmenu.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 9, 'page_label': '10'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nThe Properties screen shows the various details for the file, including the URL and\\nAzure Blob File System (ABFS) path for use with Notebooks. You can copy the ABFS\\ninto a Fabric Notebook to query the data using Apache Spark. To learn more about\\nnotebooks in Fabric, see Explore the data in your lakehouse with a notebook.\\nNow you have your first lakehouse with data stored in OneLake.\\nLearn how to connect to existing data sources with OneLake shortcuts.\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 10, 'page_label': '11'}, page_content='Transform data with Apache Spark and\\nquery with SQL\\nArticle• 03/13/2025\\nIn this guide, you will:\\nUpload data to OneLake with the OneLake file explorer.\\nUse a Fabric notebook to read data on OneLake and write back as a Delta table.\\nAnalyze and transform data with Spark using a Fabric notebook.\\nQuery one copy of data on OneLake with SQL.\\nBefore you begin, you must:\\nDownload and install OneLake file explorer.\\nCreate a workspace with a Lakehouse item.\\nDownload the WideWorldImportersDW dataset. You can use Azure Storage\\nExplorer to connect to\\nhttps://fabrictutorialdata.blob.core.windows.net/sampledata/WideWorldImporters\\nDW/csv/full/dimension_city and download the set of csv files. Or you can use your\\nown csv data and update the details as required.\\nIn this section, you upload test data into your lakehouse using OneLake file explorer.\\n1. In OneLake file explorer, navigate to your lakehouse and create a subdirectory\\nnamed dimension_city under the /Files directory.\\n2. Copy your sample csv files to the OneLake directory /Files/dimension_city using\\nOneLake file explorer.\\nPrerequisites\\nUpload data'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 11, 'page_label': '12'}, page_content=\"3. Navigate to your lakehouse in the Power BI or Fabric service and view your files.\\nIn this section, you convert the unmanaged CSV files into a managed table using Delta\\nformat.\\n1. In your lakehouse, select Open notebook, then New notebook to create a\\nnotebook.\\n\\uf80a\\nCreate a Delta table\\n７ Note\\nAlways create, load, or create a shortcut to Delta-Parquet data directly under the\\nTables section of the lakehouse. Don't nest your tables in subfolders under the\\nTables section. The lakehouse doesn't recognize subfolders as tables and labels\\nthem as Unidentified.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 12, 'page_label': '13'}, page_content='2. Using the Fabric notebook, convert the CSV files to Delta format. The following\\ncode snippet reads data from user created directory /Files/dimension_city and\\nconverts it to a Delta table dim_city.\\nCopy the code snippet into the notebook cell editor. Replace the placeholders with\\nyour own workspace details, then select Run cell or Run all.\\nPython\\n3. To see your new table, refresh your view of the /Tables directory. Select more\\noptions (...) next to the Tables directory, then select Refresh.\\nimport os\\nfrom pyspark.sql.types import *\\nfor filename in os.listdir(\"/lakehouse/default/Files/dimension_city\"):\\n    \\ndf=spark.read.format(\\'csv\\').options(header=\"true\",inferSchema=\"true\").l\\noad(\"abfss://<YOUR_WORKSPACE_NAME>@onelake.dfs.fabric.microsoft.com/<YO\\nUR_LAKEHOUSE_NAME>.Lakehouse/Files/dimension_city/\"+filename,on_bad_lin\\nes=\"skip\")\\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/dim_city\")\\n\\uea80 Tip\\nYou can retrieve the full ABFS path to your directory by right-clicking on the\\ndirectory name and selecting Copy ABFS path.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 13, 'page_label': '14'}, page_content='In this section, you use a Fabric notebook to interact with the data in your table.\\n1. Query your table with SparkSQL in the same Fabric notebook.\\nPython\\n2. Modify the Delta table by adding a new column named newColumn with data type\\ninteger. Set the value of 9 for all the records for this newly added column.\\nPython\\nYou can also access any Delta table on OneLake via a SQL analytics endpoint. A SQL\\nanalytics endpoint references the same physical copy of Delta table on OneLake and\\noffers the T-SQL experience.\\n1. Navigate to your lakehouse, then select Lakehouse > SQL analytics endpoint from\\nthe drop-down menu.\\n2. Select New SQL Query to query the table using T-SQL.\\n3. Copy and paste the following code into the query editor, then select Run.\\nSQL\\nQuery and modify data\\n%%sql\\nSELECT * from <LAKEHOUSE_NAME>.dim_city LIMIT 10;\\n%%sql\\nALTER TABLE <LAKEHOUSE_NAME>.dim_city ADD COLUMN newColumn int;\\nUPDATE <LAKEHOUSE_NAME>.dim_city SET newColumn = 9;\\nSELECT City,newColumn FROM <LAKEHOUSE_NAME>.dim_city LIMIT 10;'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 14, 'page_label': '15'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nConnect to ADLS using a OneLake shortcut\\nSELECT TOP (100) * FROM [<LAKEHOUSE_NAME>].[dbo].[dim_city];\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 15, 'page_label': '16'}, page_content='Connect to ADLS and transform the\\ndata with Azure Databricks\\nArticle• 11/29/2023\\nIn this guide, you will:\\nCreate a Delta table in your Azure Data Lake Storage (ADLS) Gen2 account using\\nAzure Databricks.\\nCreate a OneLake shortcut to a Delta table in ADLS.\\nUse Power BI to analyze data via the ADLS shortcut.\\nBefore you start, you must have:\\nA workspace with a Lakehouse item\\nAn Azure Databricks workspace\\nAn ADLS Gen2 account to store Delta tables\\n1. Using an Azure Databricks notebook, create a Delta table in your ADLS Gen2\\naccount.\\nPython\\nPrerequisites\\nCreate a Delta table, create a shortcut, and\\nanalyze the data\\n # Replace the path below to refer to your sample parquet data with \\nthis syntax \"abfss://<storage name>@<container \\nname>.dfs.core.windows.net/<filepath>\"\\n # Read Parquet files from an ADLS account\\n df = \\nspark.read.format(\\'Parquet\\').load(\"abfss://datasetsv1@olsdemo.dfs.core.\\nwindows.net/demo/full/dimension_city/\")\\n # Write Delta tables to ADLS account'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 16, 'page_label': '17'}, page_content='2. In your lakehouse, select the ellipses (…) next to Tables and then select New\\nshortcut.\\n3. In the New shortcut screen, select the Azure Data Lake Storage Gen2 tile.\\n4. Specify the connection details for the shortcut and select Next.\\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://datasetsv1@ols\\ndemo.dfs.core.windows.net/demo/adb_dim_city_delta/\")\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 17, 'page_label': '18'}, page_content='5. Specify the shortcut details. Provide a Shortcut Name and Sub path details and\\nthen select Create. The sub path should point to the directory where the Delta\\ntable resides.\\n6. The shortcut appears as a Delta table under Tables.\\n\\uf80a \\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 18, 'page_label': '19'}, page_content='7. You can now query this data directly from a notebook.\\nPython\\n8. To access and analyze this Delta table via Power BI, select New Power BI semantic\\nmodel.\\n9. Select the shortcut and then select Confirm.\\ndf = spark.sql(\"SELECT * FROM \\nlakehouse1.adls_shortcut_adb_dim_city_delta LIMIT 1000\")\\ndisplay(df)'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 19, 'page_label': '20'}, page_content='10. When the data is published, select Start from scratch.\\n11. In the report authoring experience, the shortcut data appears as a table along with\\nall its attributes.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 20, 'page_label': '21'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\n12. To build a Power BI report, drag the attributes to the pane on the left-hand side.\\nIngest data into OneLake and analyze with Azure Databricks\\nRelated content\\n\\ue8e1 Yes \\ue8e0 No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 21, 'page_label': '22'}, page_content='Ingest data into OneLake and analyze\\nwith Azure Databricks\\nArticle• 02/25/2025\\nIn this guide, you will:\\nCreate a pipeline in a workspace and ingest data into your OneLake in Delta\\nformat.\\nRead and modify a Delta table in OneLake with Azure Databricks.\\nBefore you start, you must have:\\nA workspace with a Lakehouse item.\\nA premium Azure Databricks workspace. Only premium Azure Databricks\\nworkspaces support Microsoft Entra credential passthrough. When creating your\\ncluster, enable Azure Data Lake Storage credential passthrough in the Advanced\\nOptions.\\nA sample dataset.\\n1. Navigate to your lakehouse in the Power BI service and select Get data and then\\nselect New data pipeline.\\nPrerequisites\\nIngest data and modify the Delta table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 22, 'page_label': '23'}, page_content='2. In the New Pipeline prompt, enter a name for the new pipeline and then select\\nCreate.\\n3. For this exercise, select the NYC Taxi - Green sample data as the data source.\\n4. On the preview screen, select Next.\\n5. For data destination, select the name of the lakehouse you want to use to store the\\nOneLake Delta table data. You can choose an existing lakehouse or create a new\\none.\\n6. Select where you want to store the output. Choose Tables as the Root folder. Enter\\n\"nycsample\" as the table name and select Next.\\n7. On the Review + Save screen, select Start data transfer immediately and then\\nselect Save + Run.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 23, 'page_label': '24'}, page_content='8. When the job is complete, navigate to your lakehouse and view the delta table\\nlisted under /Tables folder.\\n9. Right-click on the created table name, select Properties, and copy the Azure Blob\\nFilesystem (ABFS) path.\\n10. Open your Azure Databricks notebook. Read the Delta table on OneLake.\\nPython\\n11. Update the Delta table data by changing a field value.\\nPython\\nTransform data with Apache Spark and query with SQL\\nolsPath = \"abfss://<replace with workspace \\nname>@onelake.dfs.fabric.microsoft.com/<replace with item \\nname>.Lakehouse/Tables/nycsample\" \\ndf=spark.read.format(\\'delta\\').option(\"inferSchema\",\"true\").load(olsPath\\n)\\ndf.show(5)\\n%sql\\nupdate delta.`abfss://<replace with workspace \\nname>@onelake.dfs.fabric.microsoft.com/<replace with item \\nname>.Lakehouse/Tables/nycsample` set vendorID = 99999 where vendorID = \\n1;\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 24, 'page_label': '25'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 25, 'page_label': '26'}, page_content=\"Understand medallion lakehouse\\narchitecture for Microsoft Fabric with\\nOneLake\\nThe medallion lakehouse architecture, commonly known as medallion architecture, is a design\\npattern that's used to organize data in a lakehouse. It's the recommended design approach for\\nFabric. Since OneLake is the data lake for Fabric, medallion architecture is implemented by\\ncreating lakehouses in OneLake.\\nMedallion architecture comprises three distinct layers. The three medallion layers are: bronze\\n(raw data), silver (enriched data), and gold (curated data). Each layer indicates the quality of\\ndata stored in the lakehouse, with higher levels representing higher quality.\\nMedallion architecture helps your data stay accurate and reliable according to the principles of\\natomicity, consistency, isolation, and durability (ACID). Your data starts in its raw form, and the\\noriginal copies are preserved as a source of truth while your pipelines of validations and\\ntransformations prepares the data for analytics.\\nFor more information, see What is the medallion lakehouse architecture?.\\nThis article introduces medallion lake architecture and describes how you can implement the\\ndesign pattern in Microsoft Fabric. It's targeted at multiple audiences:\\nData engineers: Technical staff who design, build, and maintain infrastructures and\\nsystems that enable their organization to collect, store, process, and analyze large\\nvolumes of data.\\nCenter of Excellence, IT, and BI teams: The teams that are responsible for overseeing\\nanalytics throughout the organization.\\nFabric administrators: The administrators who are responsible for overseeing Fabric in\\nthe organization.\\nThe goal of medallion architecture is to incrementally improve the structure and quality of\\ndata. Think of medallion architecture as a three-stage cleaning and organizing process for your\\ndata. Each layer makes your data more reliable and easier to use.\\n1. Bronze (Raw): Store everything exactly as it arrives. No changes are allowed.\\nAudience\\nWhat is medallion architecture?\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 26, 'page_label': '27'}, page_content=\"2. Silver (Enriched): Fix errors, standardize formats, and remove duplicates.\\n3. Gold (Curated): Organize for reports and dashboards.\\nKeep each layer separated in its own lakehouse or data warehouse in OneLake, with data\\nmoving between the layers as it's transformed and refined.\\nIn a typical medallion architecture implementation in Fabric, the bronze layer stores the data in\\nthe same format as the data source. When the data source is a relational database, Delta tables\\nare a good choice. The silver and gold layers should contain Delta tables.\\nConsider the following example of an e-commerce company that applies medallion\\narchitecture to its data:\\nBronze Layer:\\nStore raw sales data from website (JSON)\\nStore raw inventory data from warehouse (CSV)\\nStore raw customer data from CRM (SQL export)\\nSilver Layer:\\nStandardize date formats across all sources\\nConvert all currency to USD\\n\\uea80 Tip\\nTo learn how to create a lakehouse, work through the Lakehouse end-to-end scenario\\ntutorial.\\nReal-world example\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 27, 'page_label': '28'}, page_content=\"Remove test transactions\\nMatch customer records across systems\\nGold Layer:\\nCreate daily sales dashboard table\\nBuild customer lifetime value table\\nGenerate inventory forecasting table\\nThe basis of a modern data warehouse is a data lake. Microsoft OneLake is a single, unified,\\nlogical data lake for your entire organization. It comes automatically provisioned with every\\nFabric tenant, and it's the single location for all your analytics data.\\nTo store data in OneLake, you create a lakehouse in Fabric. A lakehouse is a data architecture\\nplatform for storing, managing, and analyzing structured and unstructured data in a single\\nlocation. It can scale to large data volumes of all file types and sizes, and because the data is\\nstored in a single location, it can be shared and reused across the organization.\\nFor more information, see What is a lakehouse in Microsoft Fabric?.\\nWhen you create a lakehouse in OneLake, two physical storage locations are provisioned\\nautomatically:\\nTables stores tables of all formats in Apache Spark (CSV, Parquet, or Delta).\\nFiles stores data in any file format. If you want to create a table based on data in the files\\narea, you can create a shortcut that points to the folder that contains the table files.\\nIn the bronze layer, you store data in its original format, which might be either tables or files. If\\nthe source data is from OneLake, Azure Data Lake Store Gen2 (ADLS Gen2), Amazon S3, or\\nGoogle, create a shortcut in the bronze layer instead of copying the data across.\\nIn the silver and gold layers, you typically store data in Delta tables. However, you can also\\nstore data in Parquet or CSV files. If you do that, you must explicitly create a shortcut or an\\nexternal table with a location that points to the unmanaged folder that contains the Delta Lake\\nfiles in Apache Spark.\\nIn Microsoft Fabric, the Lakehouse explorer provides a unified graphical representation of the\\nwhole Lakehouse for users to navigate, access, and update their data.\\nMedallion architecture in OneLake\\nTables and files\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 28, 'page_label': '29'}, page_content=\"Delta Lake is an optimized storage layer that provides the foundation for storing data and\\ntables. It supports ACID transactions for big data workloads, and for this reason it's the default\\nstorage format in a Fabric lakehouse.\\nDelta Lake delivers reliability, security, and performance in the lakehouse for both streaming\\nand batch operations. Internally, it stores data in Parquet file format, however, it also maintains\\ntransaction logs and statistics that provide features and performance improvement over the\\nstandard Parquet format.\\nDelta Lake format delivers the following benefits compared to generic file formats:\\nSupport for ACID properties, especially durability to prevent data corruption.\\nFaster read queries.\\nIncreased data freshness.\\nSupport for both batch and streaming workloads.\\nSupport for data rollback by using Delta Lake time travel.\\nEnhanced regulatory compliance and audit by using Delta Lake table history.\\nFabric standardizes storage file format with Delta Lake. By default, every workload engine in\\nFabric creates Delta tables when you write data to a new table. For more information, see\\nLakehouse and Delta Lake tables.\\nTo implement medallion architecture in Fabric, you can either use lakehouses (one for each\\nlayer), a data warehouse, or combination of both. Your decision should be based on your\\npreference and the expertise of your team. With Fabric, you can use different analytic engines\\nthat work on the one copy of your data in OneLake.\\nHere are two patterns to consider:\\nPattern 1: Create each layer as a lakehouse. In this case, business users access data by\\nusing the SQL analytics endpoint.\\nPattern 2: Create the bronze and silver layers as lakehouses, and the gold layer as a data\\nwarehouse. In this case, business users access data by using the data warehouse\\nendpoint.\\nWhile you can create all lakehouses in a single Fabric workspace, we recommend that you\\ncreate each lakehouse in its own, separate workspace. This approach provides you with more\\ncontrol and better governance at the layer level.\\nDelta Lake storage\\nDeployment model\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 29, 'page_label': '30'}, page_content='For the bronze layer, we recommend that you store the data in its original format, or use\\nParquet or Delta Lake. Whenever possible, keep the data in its original format. If the source\\ndata is from OneLake, Azure Data Lake Store Gen2 (ADLS Gen2), Amazon S3, or Google, create\\na shortcut in the bronze layer instead of copying the data across.\\nFor the silver and gold layers, we recommend that you use Delta tables because of the extra\\ncapabilities and performance enhancements they provide. Fabric standardizes on Delta Lake\\nformat, and by default every engine in Fabric writes data in this format. Further, these engines\\nuse V-Order write-time optimization to the Parquet file format. That optimization enables fast\\nreads by Fabric compute engines, such as Power BI, SQL, Apache Spark, and others. For more\\ninformation, see Delta Lake table optimization and V-Order.\\nLastly, today many organizations face massive growth in data volumes, together with an\\nincreasing need to organize and manage that data in a logical way while facilitating more\\ntargeted and efficient use and governance. That can lead you to establish and manage a\\ndecentralized or federated data organization with governance. To meet this objective, consider\\nimplementing a data mesh architecture. Data mesh is an architectural pattern that focuses on\\ncreating data domains that offer data as a product.\\nYou can create a data mesh architecture for your data estate in Fabric by creating data\\ndomains. You might create domains that map to your business domains, for example,\\nmarketing, sales, inventory, human resources, and others. You can then implement medallion\\narchitecture by setting up data layers within each of your domains. For more information about\\ndomains, see Domains.\\nMaterialized lake views in Microsoft Fabric help you to implement medallion architecture in\\nyour lakehouse. Rather than building complex pipelines to transform data between bronze,\\nsilver, and gold layers, you can define materialized lake views that automatically manage the\\ntransformations.\\nKey benefits of using materialized lake views for medallion architecture include:\\nDeclarative pipelines: Define data transformations using SQL statements rather than\\nbuilding manual pipelines between layers.\\nAutomatic dependency management: Fabric automatically determines the correct\\nexecution order based on view dependencies.\\nData quality rules: Built-in support for defining and enforcing data quality constraints as\\ndata moves through layers.\\nOptimal refresh: The system automatically determines whether to perform incremental,\\nfull, or no refresh for each view.\\nUse materialized lake views for medallion architecture'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 30, 'page_label': '31'}, page_content='Visualization and monitoring: View lineage across all layers and track execution progress.\\nFor example, you can create a silver layer view that cleanses and joins data from bronze tables,\\nand then create gold layer views that aggregate the silver layer data for reporting. The system\\nhandles the refresh orchestration automatically.\\nFor more information, see Implement medallion architecture with materialized lake views.\\nThis section describes other guidance related to implementing a medallion lakehouse\\narchitecture in Fabric.\\nGenerally, a big data platform performs better when it has a few large files rather than many\\nsmall files. Performance degradation occurs when the compute engine has many metadata and\\nfile operations to manage. For better query performance, we recommend that you aim for data\\nfiles that are approximately 1 GB in size.\\nDelta Lake has a feature called predictive optimization. Predictive optimization automates\\nmaintenance operations for Delta tables. When this feature is enabled, Delta Lake identifies\\ntables that would benefit from maintenance operations and then optimizes their storage. While\\nthis feature should form part of your operational excellence and your data preparation work,\\nFabric can optimize data files during data write, too. For more information, see Predictive\\noptimization for Delta Lake.\\nBy default, Delta Lake maintains a history of all changes made, so the size of historical\\nmetadata grows over time. Based on your business requirements, keep historical data only for\\na certain period of time to reduce your storage costs. Consider retaining historical data for only\\nthe last month, or other appropriate period of time.\\nYou can remove older historical data from a Delta table by using the VACUUM command.\\nHowever, by default you can\\'t delete historical data within the last seven days. That restriction\\nmaintains the consistency in data. Configure the default number of days with the table\\nproperty delta.deletedFileRetentionDuration = \"interval <interval>\". That property\\ndetermines the period of time that a file must be deleted before it can be considered a\\ncandidate for a vacuum operation.\\nUnderstand Delta table data storage\\nFile size\\nHistorical retention'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 31, 'page_label': '32'}, page_content=\"When you store data in each layer, we recommended that you use a partitioned folder\\nstructure wherever applicable. This technique improves data manageability and query\\nperformance. Generally, partitioned data in a folder structure results in faster search for specific\\ndata entries because of partition pruning/elimination.\\nTypically, you append data to your target table as new data arrives. However, in some cases\\nyou might merge data because you need to update existing data at the same time. In that case,\\nyou can perform an upsert operation by using the MERGE command. When your target table is\\npartitioned, be sure to use a partition filter to speed up the operation. That way, the engine can\\neliminate partitions that don't require updating.\\nYou should plan and control who needs access to specific data in the lakehouse. You should\\nalso understand the various transaction patterns they're going to use while accessing this data.\\nYou can then define the right table partitioning scheme, and data collocation with Delta Lake\\nZ-order indexes.\\nFor more information about implementing medallion lakehouse architecture, see the following\\nresources.\\nTutorial: Lakehouse end-to-end scenario\\nTutorial: Implement medallion architecture with materialized lake views\\nLakehouse and Delta Lake tables\\nMicrosoft Fabric decision guide: choose a data store\\nThe need for optimize write on Apache Spark\\nQuestions? Try asking the Fabric community.\\nSuggestions? Contribute ideas to improve Fabric .\\n） Note: The author created this article with assistance from AI. Learn more\\nLast updated on 12/03/2025\\nTable partitions\\nData access\\nRelated content\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 32, 'page_label': '33'}, page_content=\"OneLake shortcuts\\nArticle• 05/19/2025\\nShortcuts in Microsoft OneLake allow you to unify your data across domains, clouds, and\\naccounts by creating a single virtual data lake for your entire enterprise. All Fabric experiences\\nand analytical engines can directly connect to your existing data sources such as Azure,\\nAmazon Web Services (AWS), and OneLake through a unified namespace. OneLake manages all\\npermissions and credentials, so you don't need to separately configure each Fabric workload to\\nconnect to each data source. Additionally, you can use shortcuts to eliminate edge copies of\\ndata and reduce process latency associated with data copies and staging.\\nShortcuts are objects in OneLake that point to other storage locations. The location can be\\ninternal or external to OneLake. The location that a shortcut points to is known as the target\\npath of the shortcut. The location where the shortcut appears is known as the shortcut path.\\nShortcuts appear as folders in OneLake and any workload or service that has access to OneLake\\ncan use them. Shortcuts behave like symbolic links. They're an independent object from the\\ntarget. If you delete a shortcut, the target remains unaffected. If you move, rename, or delete a\\ntarget path, the shortcut can break.\\nYou can create shortcuts in lakehouses and Kusto Query Language (KQL) databases.\\nFurthermore, the shortcuts you create within these items can point to other OneLake locations,\\nAzure Data Lake Storage (ADLS) Gen2, Amazon S3 storage accounts, or Dataverse. You can\\neven create shortcuts to on-premises or network-restricted locations with the use of the Fabric\\non-premises data gateway (OPDG).\\nWhat are shortcuts?\\n\\uf80a\\nWhere can I create shortcuts?\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 33, 'page_label': '34'}, page_content=\"You can use the Fabric UI to create shortcuts interactively, and you can use the REST API to\\ncreate shortcuts programmatically.\\nWhen creating shortcuts in a lakehouse, you must understand the folder structure of the item.\\nLakehouses are composed of two top-level folders: the Tables folder and the Files folder. The\\nTables folder represents the managed portion of the lakehouse for structured datasets. While\\nthe Files folder is the unmanaged portion of the lakehouse for unstructured or semi-structured\\ndata.\\nIn the Tables folder, you can only create shortcuts at the top level. Shortcuts aren't supported\\nin subdirectories of the Tables folder. Shortcuts in the tables section typically point to internal\\nsources within OneLake or link to other data assets that conform to the Delta table format. If\\nthe target of the shortcut contains data in the Delta\\\\Parquet format, the lakehouse\\nautomatically synchronizes the metadata and recognizes the folder as a table. Shortcuts in the\\ntables section can link to either a single table or a schema, which is a parent folder for multiple\\ntables.\\nIn the Files folder, there are no restrictions on where you can create shortcuts. You can create\\nthem at any level of the folder hierarchy. Table discovery doesn't happen in the Files folder.\\nShortcuts here can point to both internal (OneLake) and external storage systems with data in\\nany format.\\nLakehouse\\n７ Note\\nThe Delta format doesn't support tables with space characters in the name. Any shortcut\\ncontaining a space in the name won't be discovered as a Delta table in the lakehouse.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 34, 'page_label': '35'}, page_content='When you create a shortcut in a KQL database, it appears in the Shortcuts folder of the\\ndatabase. The KQL database treats shortcuts like external tables. To query the shortcut, use the\\nexternal_table function of the Kusto Query Language.\\nKQL database'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 35, 'page_label': '36'}, page_content='Any Fabric or non-Fabric service that can access data in OneLake can use shortcuts. Shortcuts\\nare transparent to any service accessing data through the OneLake API. Shortcuts just appear\\nas another folder in the lake. Apache Spark, SQL, Real-Time Intelligence, and Analysis Services\\ncan all use shortcuts when querying data.\\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake.\\nRelative file paths can be used to directly read data from shortcuts. Additionally, if you create a\\nshortcut in the Tables section of the lakehouse and it is in the Delta format, you can read it as a\\nmanaged table using Apache Spark SQL syntax.\\nPython\\nWhere can I access shortcuts?\\nApache Spark\\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")\\ndisplay(df)'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 36, 'page_label': '37'}, page_content='Python\\nYou can read shortcuts in the Tables section of a lakehouse through the SQL analytics endpoint\\nfor the lakehouse. You can access the SQL analytics endpoint through the mode selector of the\\nlakehouse or through SQL Server Management Studio (SSMS).\\nSQL\\nShortcuts in KQL databases are recognized as external tables. To query the shortcut, use the\\nexternal_table function of the Kusto Query Language.\\nKusto\\nYou can create semantic models for lakehouses containing shortcuts in the Tables section of\\nthe lakehouse. When the semantic model runs in Direct Lake mode, Analysis Services can read\\ndata directly from the shortcut.\\nApplications and services outside of Fabric can also access shortcuts through the OneLake API.\\nOneLake supports a subset of the ADLS Gen2 and Blob storage APIs. To learn more about the\\nOneLake API, see OneLake access with APIs.\\nHTTP\\ndf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000\")\\ndisplay(df)\\nSQL\\nSELECT TOP (100) *\\nFROM [MyLakehouse].[dbo].[MyShortcut]\\nReal-Time Intelligence\\nexternal_table(\\'MyShortcut\\')\\n| take 100\\nAnalysis Services\\nNon-Fabric\\nhttps://onelake.dfs.fabric.microsoft.com/MyWorkspace/MyLakhouse/Tables/MyShortcut/'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 37, 'page_label': '38'}, page_content=\"OneLake shortcuts support multiple filesystem data sources. These include internal OneLake\\nlocations, Azure Data Lake Storage (ADLS) Gen2, Amazon S3, S3 Compatible, Google Cloud\\nStorage(GCS) and Dataverse.\\nInternal OneLake shortcuts allow you to reference data within existing Fabric items, including:\\nKQL databases\\nLakehouses\\nMirrored Azure Databricks Catalogs\\nMirrored Databases\\nSemantic models\\nSQL databases\\nWarehouses\\nWarehouse snapshots\\nThe shortcut can point to a folder location within the same item, across items within the same\\nworkspace, or even across items in different workspaces. When you create a shortcut across\\nitems, the item types don't need to match. For instance, you can create a shortcut in a\\nlakehouse that points to data in a data warehouse.\\nWhen a user accesses data through a shortcut to another OneLake location, OneLake uses the\\nidentity of the calling user to authorize access to the data in the target path of the shortcut.\\nThis user must have permissions in the target location to read the data.\\nWhen you create shortcuts to Azure Data Lake Storage (ADLS) Gen2 storage accounts, the\\ntarget path can point to any folder within the hierarchical namespace. At a minimum, the target\\nMyFile.csv\\nTypes of shortcuts\\nInternal OneLake shortcuts\\n） Important\\nWhen users access shortcuts through Power BI semantic models or T-SQL, the calling\\nuser’s identity is not passed through to the shortcut target. The calling item owner’s\\nidentity is passed instead, delegating access to the calling user.\\nAzure Data Lake Storage shortcuts\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 38, 'page_label': '39'}, page_content=\"path must include a container name.\\nADLS shortcuts must point to the DFS endpoint for the storage account.\\nExample: https://accountname.dfs.core.windows.net/\\nIf your storage account is protected by a storage firewall, you can configure trusted service\\naccess. For more information, see Trusted workspace access\\nADLS shortcuts use a delegated authorization model. In this model, the shortcut creator\\nspecifies a credential for the ADLS shortcut and all access to that shortcut is authorized using\\nthat credential. ADLS shortcuts support the following delegated authorization types:\\nOrganizational account - must have Storage Blob Data Reader, Storage Blob Data\\nContributor, or Storage Blob Data Owner role on the storage account; or Delegator role\\non the storage account plus file or directory access granted within the storage account.\\nService principal - must have Storage Blob Data Reader, Storage Blob Data Contributor,\\nor Storage Blob Data Owner role on the storage account; or Delegator role on the storage\\naccount plus file or directory access granted within the storage account.\\nWorkspace identity - must have Storage Blob Data Reader, Storage Blob Data\\nContributor, or Storage Blob Data Owner role on the storage account; or Delegator role\\non the storage account plus file or directory access granted within the storage account.\\nShared Access Signature (SAS) - must include at least the following permissions: Read,\\nList, and Execute.\\nMicrosoft Entra ID delegated authorization types (organizational account, service principal, or\\nworkspace identity) require the Generate a user delegation key action at the storage account\\nlevel. This action is included as part of the Storage Blob Data Reader, Storage Blob Data\\nContributor, Storage Blob Data Owner, and Delegator roles. If you don't want to give a user\\nreader, contributor, or owner permissions for the whole storage account, assign them the\\nDelegator role instead. Then, define detailed data access rights using Access control lists (ACLs)\\nin Azure Data Lake Storage.\\n７ Note\\nYou must have hierarchical namespaces enabled on your ADLS Gen 2 storage account.\\nAccess\\nAuthorization\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 39, 'page_label': '40'}, page_content=\"Azure Blob Storage shortcut can point to the account name or URL for the Storage account.\\nExample: accountname or https://accountname.blob.core.windows.net/\\nBlob storage shortcuts use a delegated authorization model. In this model, the shortcut creator\\nspecifies a credential for the shortcut and all access to that shortcut is authorized using that\\ncredential. Blob shortcuts support the following delegated authorization types:\\nOrganizational account - must have Storage Blob Data Reader, Storage Blob Data\\nContributor, or Storage Blob Data Owner role on the storage account; or Delegator role\\non the storage account plus file or directory access granted within the storage account.\\nService principal - must have Storage Blob Data Reader, Storage Blob Data Contributor,\\nor Storage Blob Data Owner role on the storage account; or Delegator role on the storage\\naccount plus file or directory access granted within the storage account.\\nWorkspace identity - must have Storage Blob Data Reader, Storage Blob Data\\nContributor, or Storage Blob Data Owner role on the storage account; or Delegator role\\non the storage account plus file or directory access granted within the storage account.\\nShared Access Signature (SAS) - must include at least the following permissions: Read,\\nList, and Execute.\\nWhen you create shortcuts to Amazon S3 accounts, the target path must contain a bucket\\nname at a minimum. S3 doesn't natively support hierarchical namespaces but you can use\\nprefixes to mimic a directory structure. You can include prefixes in the shortcut path to further\\n） Important\\nCurrently, when workspace identity is used as the delegated authorization type for an\\nADLS shortcut, users can authenticate directly to the storage account without needing to\\ncreate a delegation key. However, this behavior will be restricted in the future. We\\nrecommend making sure that all users have the Generate a user delegation key action to\\nensure that your users' access isn't affected when this behavior changes.\\nAzure Blob Storage shortcuts\\nAccess\\nAuthorization\\nS3 shortcuts\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 40, 'page_label': '41'}, page_content='narrow the scope of data accessible through the shortcut. When you access data through an S3\\nshortcut, prefixes are represented as folders.\\nS3 shortcuts must point to the https endpoint for the S3 bucket.\\nExample: https://bucketname.s3.region.amazonaws.com/\\nS3 shortcuts use a delegated authorization model. In this model, the shortcut creator specifies\\na credential for the S3 shortcut and all access to that shortcut is authorized using that\\ncredential. The supported delegated credential is a key and secret for an IAM user.\\nThe IAM user must have the following permissions on the bucket that the shortcut is pointing\\nto:\\nS3:GetObject\\nS3:GetBucketLocation\\nS3:ListBucket\\nS3 shortcuts support S3 buckets that use S3 Bucket Keys for SSE-KMS encryption. To access\\ndata encrypted with SSE-KMS encryption, the user must have encrypt/decrypt permissions for\\nthe bucket key, otherwise they receive a \"Forbidden\" error (403). For more information, see\\nConfiguring your bucket to use an S3 Bucket Key with SSE-KMS for new objects.\\n７ Note\\nS3 shortcuts are read-only. They don\\'t support write operations regardless of the user\\'s\\npermissions.\\nAccess\\n７ Note\\nYou don\\'t need to disable the S3 Block Public Access setting for your S3 account for the S3\\nshortcut to function.\\nAccess to the S3 endpoint must not be blocked by a storage firewall or Virtual Private\\nCloud.\\nAuthorization'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 41, 'page_label': '42'}, page_content=\"Shortcuts can be created to Google Cloud Storage(GCS) using the XML API for GCS. When you\\ncreate shortcuts to Google Cloud Storage, the target path must contain a bucket name at a\\nminimum. You can also restrict the scope of the shortcut by further specifying the prefix/folder\\nyou want to point to within the storage hierarchy.\\nWhen configuring the connection for a GCS shortcut, you can either specify the global\\nendpoint for the storage service or use a bucket-specific endpoint.\\nGlobal endpoint example: https://storage.googleapis.com\\nBucket-specific endpoint example: https://<BucketName>.storage.googleapis.com\\nGCS shortcuts use a delegated authorization model. In this model, the shortcut creator\\nspecifies a credential for the GCS shortcut and all access to that shortcut is authorized using\\nthat credential. The supported delegated credential is an HMAC key and secret for a Service\\naccount or User account.\\nThe account must have permission to access the data within the GCS bucket. If the bucket-\\nspecific endpoint was used in the connection for the shortcut, the account must have the\\nfollowing permissions:\\nstorage.objects.get\\nstoage.objects.list\\nIf the global endpoint was used in the connection for the shortcut, the account must also have\\nthe following permission:\\nstorage.buckets.list\\nGoogle Cloud Storage shortcuts\\n７ Note\\nGCS shortcuts are read-only. They don't support write operations regardless of the user's\\npermissions.\\nAccess\\nAuthorization\\nDataverse shortcuts\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 42, 'page_label': '43'}, page_content=\"Dataverse direct integration with Microsoft Fabric enables organizations to extend their\\nDynamics 365 enterprise applications and business processes into Fabric. This integration is\\naccomplished through shortcuts, which can be created in two ways: through the PowerApps\\nmaker portal, or through Fabric directly.\\nAuthorized PowerApps users can access the PowerApps maker portal and use the Link to\\nMicrosoft Fabric feature. From this single action, a lakehouse is created in Fabric and shortcuts\\nare automatically generated for each table in the Dataverse environment.\\nFor more information, see Dataverse direct integration with Microsoft Fabric .\\nFabric users can also create shortcuts to Dataverse. When users create shortcuts, they can\\nselect Dataverse, supply their environment URL, and browse the available tables. This\\nexperience allows users to choose which tables to bring into Fabric rather than bringing in all\\ntables.\\nDataverse shortcuts use a delegated authorization model. In this model, the shortcut creator\\nspecifies a credential for the Dataverse shortcut, and all access to that shortcut is authorized\\nusing that credential. The supported delegated credential type is organizational account\\n(OAuth2). The organizational account must have the system administrator permission to access\\ndata in Dataverse Managed Lake.\\n７ Note\\nDataverse shortcuts are read-only. They don't support write operations regardless of the\\nuser's permissions.\\nCreating shortcuts through PowerApps maker portal\\nCreating shortcuts through Fabric\\n７ Note\\nDataverse tables must first be available in the Dataverse Managed Lake before they're\\nvisible in the Fabric create shortcuts UX. If your tables aren't visible from Fabric, use the\\nLink to Microsoft Fabric feature from the PowerApps maker portal.\\nAuthorization\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 43, 'page_label': '44'}, page_content=\"Shortcut caching can reduce egress costs associated with cross-cloud data access. As files are\\nread through an external shortcut, the files are stored in a cache for the Fabric workspace.\\nSubsequent read requests are served from cache rather than the remote storage provider. The\\nretention period for cached files can be set from 1-28 days. Each time the file is accessed, the\\nretention period is reset. If the file in remote storage provider is more recent than the file in the\\ncache, the request is served from remote storage provider and the updated file will then be\\nstored in cache. If a file hasn’t been accessed for more than the selected retention period, it's\\npurged from the cache. Individual files greater than 1 GB in size aren't cached.\\nTo enable caching for shortcuts, open the Workspace settings panel. Choose the OneLake tab.\\nToggle the cache setting to On and select the Retention Period.\\nThe cache can also be cleared at any time. From the same settings page, select the Reset cache\\nbutton. This action removes all files from the shortcut cache in this workspace.\\n７ Note\\nService principals added to the fabric workspace must have the admin role to authorize\\nthe Dataverse shortcut.\\nCaching\\n７ Note\\nShortcut caching is currently supported for GCS, S3, S3 compatible, and on-premises data\\ngateway shortcuts.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 44, 'page_label': '45'}, page_content=\"ADLS and S3 shortcut authorization is delegated by using cloud connections. When you create\\na new ADLS or S3 shortcut, you either create a new connection or select an existing connection\\nfor the data source. Setting a connection for a shortcut is a bind operation. Only users with\\npermission on the connection can perform the bind operation. If you don't have permissions\\non the connection, you can't create new shortcuts using that connection.\\nShortcuts require certain permissions to manage and use. OneLake shortcut security looks at\\nthe permissions required to create shortcuts and access data using them.\\n\\uf80a\\nHow shortcuts utilize cloud connections\\nShortcut security\\nHow do shortcuts handle deletions?\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 45, 'page_label': '46'}, page_content=\"Shortcuts don't perform cascading deletes. When you delete a shortcut, you only delete the\\nshortcut object. The data in the shortcut target remains unchanged. However, if you delete a\\nfile or folder within a shortcut, and you have permissions in the shortcut target to perform the\\ndelete operation, the files or folders are deleted in the target.\\nFor example, consider a lakehouse with the following path in it:\\nMyLakehouse\\\\Files\\\\MyShortcut\\\\Foo\\\\Bar. MyShortcut is a shortcut that points to an ADLS Gen2\\naccount that contains the Foo\\\\Bar directories.\\nYou can perform a delete operation on the following path: MyLakehouse\\\\Files\\\\MyShortcut. In\\nthis case, the MyShortcut shortcut is deleted from the lakehouse but the files and directories in\\nthe ADLS Gen2 account Foo\\\\Bar remain unaffected.\\nYou can also perform a delete operation on the following path:\\nMyLakehouse\\\\Files\\\\MyShortcut\\\\Foo\\\\Bar. In this case, if you write permissions in the ADLS Gen2\\naccount, the Bar directory is deleted from the ADLS Gen2 account.\\nWhen creating shortcuts between multiple Fabric items within a workspace, you can visualize\\nthe shortcut relationships through the workspace lineage view. Select the Lineage view button\\n(\\n  ) in the upper right corner of the Workspace explorer.\\nThe maximum number of shortcuts per Fabric item is 100,000. In this context, the term\\nitem refers to: apps, lakehouses, warehouses, reports, and more.\\nWorkspace lineage view\\n\\uf80a\\n７ Note\\nThe lineage view is scoped to a single workspace. Shortcuts to locations outside the\\nselected workspace don't appear.\\nLimitations and considerations\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 46, 'page_label': '47'}, page_content='The maximum number of shortcuts in a single OneLake path is 10.\\nThe maximum number of direct shortcuts to shortcut links is 5.\\nADLS and S3 shortcut target paths can\\'t contain any reserved characters from RFC 3986\\nsection 2.2. For allowed characters, see RFC 3968 section 2.3.\\nOneLake shortcut names, parent paths, and target paths can\\'t contain \"%\" or \"+\"\\ncharacters.\\nShortcuts don\\'t support non-Latin characters.\\nCopy Blob API isn\\'t supported for ADLS or S3 shortcuts.\\nCopy function doesn\\'t work on shortcuts that directly point to ADLS containers. It\\'s\\nrecommended to create ADLS shortcuts to a directory that is at least one level below a\\ncontainer.\\nMore shortcuts can\\'t be created inside ADLS or S3 shortcuts.\\nLineage for shortcuts to Data Warehouses and Semantic Models isn\\'t currently available.\\nA Fabric shortcut syncs with the source almost instantly, but propagation time might vary\\ndue to data source performance, cached views, or network connectivity issues.\\nIt might take up to a minute for the Table API to recognize new shortcuts.\\nOneLake shortcuts don\\'t support connections to ADLS Gen2 storage accounts that use\\nmanaged private endpoints. For more information, see managed private endpoints for\\nFabric.\\nCreate a OneLake shortcut\\nUse OneLake shortcuts REST APIs\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 47, 'page_label': '48'}, page_content=\"Shortcuts file transformations\\nShortcut transformations convert raw files (CSV, Parquet, and JSON) into Delta tables that stay always in sync with the source\\ndata. The transformation is executed by Fabric Spark compute, which copies the data referenced by a OneLake shortcut into a\\nmanaged Delta table so you don't have to build and orchestrate traditional extract, transform, load (ETL) pipelines yourself.\\nWith automatic schema handling, deep flattening capabilities, and support for multiple compression formats, shortcut\\ntransformations eliminate the complexity of building and maintaining ETL pipelines.\\nNo manual pipelines – Fabric automatically copies and converts the source files to Delta format; you don’t have to\\norchestrate incremental loads.\\nFrequent refresh – Fabric checks the shortcut every 2 minutes and synchronizes any changes almost immediately.\\nOpen & analytics-ready – Output is a Delta Lake table that any Apache Spark–compatible engine can query.\\nUnified governance – The shortcut inherits OneLake lineage, permissions, and Microsoft Purview policies.\\nSpark based – Transforms build for scale.\\nRequirement Details\\nMicrosoft Fabric SKU Capacity or Trial that supports Lakehouse workloads.\\nSource data A folder that contains homogeneous CSV, Parquet, or JSON files.\\nWorkspace role Contributor or higher.\\nAll data sources supported in OneLake are supported.\\nSource\\nfile\\nformat\\nDestinationSupported ExtensionsSupported Compression types Notes\\nCSV\\n(UTF-8,\\nUTF-\\n16)\\nDelta Lake\\ntable in the\\nLakehouse\\n/ Tables\\nfolder\\n.csv,.txt(delimiter),.tsv(tab-\\nseparated),.psv(pipe-\\nseparated),\\n.csv.gz,.csv.bz2 .csv.zip,.csv.snappy\\naren't supported\\nas of date\\nParquetDelta Lake\\ntable in the\\nLakehouse\\n/ Tables\\nfolder\\n.parquet .parquet.snappy,.parquet.gzip,.parquet.lz4,.parquet.brotli,.parquet.zstd\\n７ Note\\nShortcut transformations are currently in public preview and are subject to change.\\nWhy use shortcut transformations?\\nPrerequisites\\nﾉExpand table\\nSupported sources, formats and destinations\\nﾉExpand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 48, 'page_label': '49'}, page_content=\"Source\\nfile\\nformat\\nDestinationSupported ExtensionsSupported Compression types Notes\\nJSON Delta Lake\\ntable in the\\nLakehouse\\n/ Tables\\nfolder\\n.json,.jsonl,.ndjson.json.gz,.json.bz2,.jsonl.gz,.ndjson.gz,.jsonl.bz2,.ndjson.bz2.json.zip,\\n.json.snappy aren't\\nsupported as of\\ndate\\nExcel file support is part of roadmap\\nAI Transformations available to support unstructured file formats (.txt, .doc, .docx) with Text Analytics use case live with\\nmore enhancements upcoming\\n1. In your lakehouse, select New Table Shortcut in Tables section which is Shortcut transformation (preview) and choose\\nyour source (for example, Azure Data Lake, Azure Blob Storage, Dataverse, Amazon S3, GCP, SharePoint, OneDrive etc.).\\n2. Choose file, Configure transformation & create shortcut – Browse to an existing OneLake shortcut that points to the\\nfolder with your CSV files, configure parameters, and initiate creation.\\nDelimiter in CSV files – Select the character used to separate columns (comma, semicolon, pipe, tab, ampersand,\\nspace).\\nFirst row as headers – Indicate whether the first row contains column names.\\nTable Shortcut name – Provide a friendly name; Fabric creates it under /Tables.\\n3. Track refreshes and view logs for transparency in Manage Shortcut monitoring hub.\\nFabric Spark compute copies the data into a Delta table and shows progress in the Manage shortcut pane. Shortcut\\ntransformations are available in Lakehouse items. They create Delta Lake tables in the Lakehouse / Tables folder.\\nAfter the initial load, Fabric Spark compute:\\nPolls the shortcut target every 2 minutes.\\nDetects new or modified files and appends or overwrites rows accordingly.\\nDetects deleted files and removes corresponding rows.\\nShortcut transformations include monitoring and error handling to help you track ingestion status and diagnose issues.\\n1. Open the lakehouse and right-click the shortcut that feeds your transformation.\\nSet up a shortcut transformation\\nHow synchronization works\\nMonitor and troubleshoot\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 49, 'page_label': '50'}, page_content='2. Select Manage shortcut.\\n3. In the details pane, you can view:\\nStatus – Last scan result and current sync state.\\nRefresh history – Chronological list of sync operations with row counts and any error details.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 50, 'page_label': '51'}, page_content=\"4. View more details in logs to troubleshoot\\nCurrent limitations of shortcut transformations:\\nOnly CSV, Parquet, JSON file formats are supported.\\nFiles must share an identical schema; schema drift isn’t yet supported.\\nTransformations are read-optimized; MERGE INTO or DELETE statements directly on the table are blocked.\\nAvailable only in Lakehouse items (not Warehouses or KQL databases).\\nUnsupported datatypes for CSV: Mixed data type columns, Timestamp_Nanos, Complex logical types -\\nMAP/LIST/STRUCT, Raw binary\\nUnsupported datatype for Parquet: Timestamp_nanos, Decimal with INT32/INT64, INT96, Unassigned integer types -\\nUINT_8/UINT_16/UINT_64, Complex logical types - MAP/LIST/STRUCT)\\nUnsupported datatypes for JSON: Mixed data types in an array, Raw binary blobs inside JSON, Timestamp_Nanos\\nFlattening of Array data type in JSON: Array data type shall be retained in delta table and data accessible with Spark\\nSQL & Pyspark where for further transformations Fabric Materialized Lake Views could be used for silver layer\\nSource format: Only CSV, JSON, and Parquet files are supported as of date.\\nFlattening depth in JSON: Nested structures are flattened up to five levels deep. Deeper nesting requires preprocessing.\\nWrite operations: Transformations are read-optimized; direct MERGE INTO or DELETE statements on the transformation\\ntarget table aren't supported.\\nWorkspace availability: Available only in Lakehouse items (not Data Warehouses or KQL databases).\\nFile schema consistency: Files must share an identical schema.\\n７ Note\\nPause or Delete the transformation from this tab is an upcoming feature part of roadmap\\nLimitations\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 51, 'page_label': '52'}, page_content='To stop synchronization, delete the shortcut transformation from the lakehouse UI.\\nDeleting the transformation doesn’t remove the underlying files.\\n） Note: The author created this article with assistance from AI. Learn more\\nLast updated on 12/01/2025\\n７ Note\\nAdding support for some of the above and reducing limitations is part of our roadmap. Track our release\\ncommunications for further updates.\\nClean up'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 52, 'page_label': '53'}, page_content='AI-powered transforms in OneLake\\nshortcut transformations\\n07/17/2025\\nModern data lakes are brimming with raw, unstructured text, product reviews, support emails,\\nIoT device logs, and more. Turning that text into actionable insights typically requires custom\\ncode, orchestration pipelines, and constant maintenance. OneLake Shortcut Transformations\\nremove that overhead: you point to your files once, choose an AI transform, and Fabric does\\nthe rest.\\nBenefit What it means for you\\nAccelerate time-to-\\ninsight\\nGo from raw text to a queryable Delta table in minutes, no ETL required.\\nLower maintenance The transformation engine watches the source folder on a 2-minute schedule,\\nso outputs stay up to date automatically.\\nEnterprise-grade\\nsecurity\\nPII detection helps you comply with GDPR, HIPAA, and other regulations by\\nredacting sensitive data before it lands in analytics.\\nConsistent, repeatable\\nresults\\nBuilt-in AI models provide standardized sentiment scores, entity tags, and\\ntranslations, eliminating manual data-prep drift.\\nOneLake Shortcut Transformations in Microsoft Fabric include a set of built-in, AI-powered\\ntransforms that you can apply directly to .txt files referenced through shortcuts, without\\nwriting code or building pipelines. The engine automatically keeps the output Delta table in\\nsync, so your data is query-ready for Power BI, notebooks, pipelines, and other Fabric\\nexperiences.\\n） Important\\nAI transforms for OneLake shortcuts are currently Public Preview. Features and behavior\\nmay change before general availability.\\nWhy use AI-powered transforms?\\nﾉ Expand table\\nSupported AI transforms'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 53, 'page_label': '54'}, page_content='Transform Purpose\\nSummarization Generates concise summaries from long-form text.\\nTranslation Translates text between supported languages.\\nSentiment\\nanalysis\\nLabels text sentiment as positive, negative, or neutral.\\nPII detection Finds and redacts personally identifiable information (names, phone numbers,\\nemails).\\nName recognitionExtracts named entities such as people, organizations, or locations.\\nCustomer feedback stored in a data lake may contain sensitive details (names, emails, phone\\nnumbers). Apply the PII detection transform to automatically scan and redact this content and\\nproduce a privacy-compliant Delta table for analysis.\\n1. Create a shortcut\\nReference a folder of .txt files in Azure Data Lake, Amazon S3, or another OneLake\\nshortcuts source.\\n2. Select an AI transform\\nPick one of the supported transforms during shortcut creation.\\n3. Automatic sync\\nThe engine checks the source folder every 2 minutes. New, modified, or deleted files are\\nreflected in the Delta table.\\n4. Query-ready output\\nUse the resulting table immediately in reports, notebooks, or downstream pipelines.\\nﾉ Expand table\\n７ Note\\nAI transforms currently support .txt files only as input.\\nExample — PII detection in customer feedback\\nHow it works\\nRegional availability'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 54, 'page_label': '55'}, page_content='AI-powered transforms are currently available in these regions: Azure AI Language regional\\nsupport'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 55, 'page_label': '56'}, page_content=\"Create a OneLake shortcut\\nArticle• 03/31/2025\\nIn this article, you learn how to create a OneLake shortcut inside a Fabric lakehouse. You\\ncan use a lakehouse, a data warehouse, or a Kusto Query Language (KQL) database as\\nthe source for your shortcut.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts\\nprogrammatically, see OneLake shortcuts REST APIs.\\nA lakehouse in OneLake. If you don't have a lakehouse, create one by following these\\nsteps: Create a lakehouse with OneLake.\\n1. Open a lakehouse.\\n2. Right-click on a directory within the Explorer pane of the lakehouse.\\n3. Select New shortcut.\\nPrerequisite\\nCreate a shortcut\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 56, 'page_label': '57'}, page_content='1. Under Internal sources, select Microsoft OneLake.\\n2. Select the data source that you want to connect to, and then select Next.\\n3. Expand Files or Tables to view the available subfolders. Subfolders in the tables\\ndirectory that contain valid Delta or Iceberg tables are indicated with a table icon.\\nFiles or unidentified folders in the tables section are indicated with a folder icon.\\nSelect a source\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 57, 'page_label': '58'}, page_content='4. Select one or more subfolders to connect to, then select Next.\\nYou can select up to 50 subfolders when creating OneLake shortcuts.\\n5. Review your selected shortcut locations. Use the edit action to change the default\\nshortcut name. Use the delete action to remove any undesired selections. Select\\nCreate to generate shortcuts.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 58, 'page_label': '59'}, page_content='The lakehouse automatically refreshes. The shortcut appears under the selected\\ndirectory in the Explorer pane. You can differentiate a regular file or table from the\\nshortcut from its properties. The properties have a Shortcut Type parameter that\\nindicates the item is a shortcut.\\nEditing shortcuts requires write permission on the item being edited. The admin,\\nmember, and contributor roles grant write permissions.\\n1. To edit a shortcut, right-click on the shortcut and select Manage shortcut.\\n\\uf80a\\nEdit a shortcut'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 59, 'page_label': '60'}, page_content='2. In the Manage shortcut view, you can edit the following fields:\\nName\\nTarget connection\\nNot all shortcut types use the target connection feature.\\nTarget location and Target subpath\\nBoth of these fields are editable by selecting the Target location.\\nShortcut location\\nYou can also edit shortcuts by using the OneLake shortcuts REST APIs.\\nTo delete a shortcut, select the ... icon next to the shortcut file or table and select Delete.\\nTo delete shortcuts programmatically, see OneLake shortcuts REST APIs.\\nRemove a shortcut'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 60, 'page_label': '61'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nCreate an Azure Data Lake Storage Gen2 shortcut\\nCreate an Amazon S3 shortcut\\nUse OneLake shortcuts REST APIs\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 61, 'page_label': '62'}, page_content=\"Create an Azure Data Lake Storage Gen2\\nshortcut\\nArticle• 07/25/2024\\nIn this article, you learn how to create an Azure Data Lake Storage (ADLS) Gen2 shortcut\\ninside a Microsoft Fabric lakehouse.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts\\nprogrammatically, see OneLake shortcuts REST APIs.\\nIf you don't have a lakehouse, create one by following these steps: Create a\\nlakehouse with OneLake.\\nYou must have Hierarchical Namespaces enabled on your ADLS Gen 2 storage\\naccount.\\n1. Open a lakehouse.\\n2. Right-click on a directory within the Lake view of the lakehouse.\\n3. Select New shortcut.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 62, 'page_label': '63'}, page_content='1. Under External sources, select Azure Data Lake Storage Gen2.\\n2. Enter the Connection settings according to the following table:\\nSelect a source\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 63, 'page_label': '64'}, page_content='Field Description Value\\nURL The connection\\nstring for your\\ndelta container.\\nhttps://StorageAccountName.dfs.core.windows.net\\nConnection Previously\\ndefined\\nconnections for\\nthe specified\\nstorage location\\nappear in the\\ndrop-down. If no\\nconnections\\nexist, create a\\nnew connection.\\nCreate new connection.\\nConnection\\nname\\nThe Azure Data\\nLake Storage\\nGen2 connection\\nname.\\nA name for your connection.\\nAuthentication\\nkind\\nThe\\nauthorization\\nmodel. The\\nsupported\\nmodels are:\\nOrganizational\\naccount,\\nAccount key,\\nShared Access\\nSignature (SAS),\\nand Service\\nprincipal. For\\nDependent on the authorization model. Once you\\nselect an authentication kind, fill in the required\\ncredentials.\\n\\uf80a\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 64, 'page_label': '65'}, page_content='Field Description Value\\nmore\\ninformation, see\\nADLS shortcuts.\\n3. Select Next.\\n4. Browse to the target location for the shortcut.\\nIf you just used the storage account in the connection URL, all of your available\\ncontainers appear in the left navigation view. If you specified a container in\\nconnection URL, only the specified container and its contents appear in the\\nnavigation view.\\nNavigate the storage account by selecting a folder or clicking on the expansion\\narrow next to a folder.\\nIn this view, you can select one or more shortcut target locations. Choose target\\nlocations by clicking the checkbox next a folder in the left navigation view.\\n5. Select Next\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 65, 'page_label': '66'}, page_content='The review page allows you to verify all of your selections. Here you can see each\\nshortcut that will be created. In the action column, you can click the pencil icon to\\nedit the shortcut name. You can click the trash can icon to delete shortcut.\\n6. Select Create.\\n7. The lakehouse automatically refreshes. The shortcut appears in the left Explorer\\npane.\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 66, 'page_label': '67'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nCreate a OneLake shortcut\\nCreate an Amazon S3 shortcut\\nUse OneLake shortcuts REST APIs\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 67, 'page_label': '68'}, page_content=\"Create an Azure Blob Storage shortcut\\n(preview)\\nArticle• 05/19/2025\\nIn this article, you learn how to create an Azure Blob Storage shortcut inside a Microsoft Fabric\\nlakehouse.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts programmatically, see\\nOneLake shortcuts REST APIs.\\nA lakehouse in Microsoft Fabric. If you don't have a lakehouse, create one by following\\nthese steps: Create a lakehouse with OneLake.\\nAn Azure Storage account with data in a container.\\n1. Open a lakehouse in Fabric.\\n2. Right-click on a directory in the Explorer pane of the lakehouse.\\n3. Select New shortcut.\\n７ Note\\nAzure Blob Storage shortcuts are currently in public preview.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 68, 'page_label': '69'}, page_content='When you create a shortcut in a lakehouse, the New shortcut window opens to walk you\\nthrough the configuration details.\\n1. On the New shortcut window, under External sources, select Azure Blob Storage\\n(preview).\\n2. Select Existing connection or Create new connection, depending on whether this\\nStorage account is already connected in your OneLake.\\nFor an Existing connection, select the connection from the drop-down menu.\\nTo Create new connection, provide the following connection settings.\\nSelect a source'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 69, 'page_label': '70'}, page_content='Field Description\\nAccount name\\nor URL\\nThe name of your blob storage account.\\nConnection The default value, Create new connection.\\nConnection\\nname\\nA name for your Azure Blob Storage connection. The service generates a\\nsuggested connection name based on the storage account name, but you\\ncan overwrite with a preferred name.\\nAuthentication\\nkind\\nSelect the authorization model from the drop-down menu that you want\\nto use to connect to the Storage account. The supported models are:\\naccount key, organizational account, Shared Access Signature (SAS),\\nservice principal, and workspace identity. Once you select a model, fill in\\nthe required credentials. For more information, see Azure Blob Storage\\nshortcuts authorization.\\n3. Select Next.\\n4. Browse to the target location for the shortcut.\\nIf you provided the storage account name in the connection details, all of your available\\ncontainers appear in the navigation view. If you specified a container in connection URL,\\nonly the specified container and its contents appear in the navigation view.\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 70, 'page_label': '71'}, page_content='Navigate the storage account by selecting a folder or expanding a folder to view its child\\nitems.\\nChoose one or more target locations by selecting the checkbox next a folder in the\\nnavigation view. Then, select Next.\\n5. On the review page, verify your selections. Here you can see each shortcut to be created.\\nIn the Actions column, you can select the pencil icon to edit the shortcut name. You can\\nselect the trash can icon to delete the shortcut.\\n6. Select Create.\\n7. The lakehouse automatically refreshes. The shortcut or shortcuts appear in the Explorer\\npane.\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 71, 'page_label': '72'}, page_content='Create a OneLake shortcut\\nCreate an Amazon S3 shortcut\\nUse OneLake shortcuts REST APIs'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 72, 'page_label': '73'}, page_content=\"Create a OneDrive or SharePoint shortcut\\n(preview)\\nIn this article, you learn how to create a OneDrive or SharePoint shortcut inside a Microsoft\\nFabric lakehouse.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts programmatically, see\\nOneLake shortcuts REST APIs.\\nA lakehouse in Microsoft Fabric. If you don't have a lakehouse, create one by following\\nthese steps: Create a lakehouse with OneLake.\\nData in a OneDrive or SharePoint folder.\\n1. Open a lakehouse in Fabric.\\n2. Right-click on a directory in the Explorer pane of the lakehouse.\\n3. Select New shortcut.\\n７ Note\\nOneDrive and SharePoint shortcuts are currently in public preview.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 73, 'page_label': '74'}, page_content='When you create a shortcut in a lakehouse, the New shortcut window opens to walk you\\nthrough the configuration details.\\n1. On the New shortcut window, under External sources, select OneDrive (preview) or\\nSharePoint Folder (preview).\\n2. Select Existing connection or New connection, depending on whether this account is\\nalready connected in your OneLake.\\nFor an Existing connection, select the connection from the drop-down menu.\\nTo create a New connection, provide the following connection settings:\\nField Description\\nSite URL The root URL of your SharePoint account.\\nTo retrieve your URL, sign in to OneDrive. Select the settings gear icon, then\\nOneDrive settings > More settings. Copy the OneDrive web URL from the more\\nSelect a source\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 74, 'page_label': '75'}, page_content='Field Description\\nsettings page and remove anything after _onmicrosoft_com. For example,\\nhttps://mytenant-my.sharepoint.com/personal/user01_mytenant_onmicrosoft_com.\\nConnection The default value, Create new connection.\\nConnection\\nname\\nA name for your connection. The service generates a suggested connection name\\nbased on the storage account name, but you can overwrite with a preferred\\nname.\\nAuthentication\\nkind\\nThe supported authentication type for this shortcut is Organizational account.\\n3. Select Next.\\n4. Browse to the target location for the shortcut.\\nNavigate by selecting a folder or expanding a folder to view its child items.\\nChoose one or more target locations by selecting the checkbox next a folder in the\\nnavigation view. Then, select Next.\\n5. On the Transform page, select a transformation option if you want to transform the data\\nin your shortcut or select Skip.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 75, 'page_label': '76'}, page_content='For more information, see AI-powered transforms.\\n6. On the review page, verify your selections. Here you can see each shortcut to be created.\\nIn the Actions column, you can select the pencil icon to edit the shortcut name. You can\\nselect the trash can icon to delete the shortcut.\\n7. Select Create.\\n8. The lakehouse automatically refreshes. The shortcut or shortcuts appear in the Explorer\\npane.\\nCreate a OneLake shortcut\\nUse OneLake shortcuts REST APIs\\nLast updated on 11/18/2025\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 76, 'page_label': '77'}, page_content=\"Create an Amazon S3 shortcut\\nArticle• 03/31/2025\\nIn this article, you learn how to create an Amazon S3 shortcut inside a Fabric lakehouse.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts\\nprogrammatically, see OneLake shortcuts REST APIs.\\nYou can secure your S3 buckets using customer-managed KMS keys. As long as the IAM\\nuser has encrypt/decrypt permissions for the bucket key, OneLake can access the\\nencrypted data in the S3 bucket. For more information, see Configuring your bucket to\\nuse an S3 Bucket Key with SSE-KMS for new objects.\\nS3 shortcuts can take advantage of file caching to reduce egress costs associated with\\ncross-cloud data access. For more information, see OneLake shortcuts > Caching.\\nIf you don't have a lakehouse, create one by following these steps: Create a\\nlakehouse with OneLake.\\nEnsure your chosen S3 bucket and IAM user meet the access and authorization\\nrequirements for S3 shortcuts.\\n1. Open a lakehouse.\\n2. Right-click on the Tables directory within the lakehouse.\\n3. Select New shortcut.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 77, 'page_label': '78'}, page_content='1. Under External sources, select Amazon S3.\\n2. Enter the Connection settings according to the following table:\\nSelect a source\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 78, 'page_label': '79'}, page_content='Field Description Value\\nURL The connection\\nstring for your\\nAmazon S3\\nbucket.\\nhttps://BucketName.s3.RegionCode.amazonaws.com\\nConnection Previously\\ndefined\\nconnections for\\nthe specified\\nstorage location\\nappear in the\\ndrop-down. If no\\nconnections\\nexist, create a\\nnew connection.\\nCreate new connection\\nConnection\\nname\\nThe Amazon S3\\nconnection\\nname.\\nA name for your connection.\\nAuthentication\\nkind\\nThe Identity and\\nAccess\\nManagement\\n(IAM) policy. The\\npolicy must have\\nread and list\\npermissions. For\\nmore\\ninformation, see\\nIAM users.\\nDependent on the bucket policy.\\n\\uf80a\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 79, 'page_label': '80'}, page_content='Field Description Value\\nAccess Key ID The Identity and\\nAccess\\nManagement\\n(IAM) user key.\\nFor more\\ninformation, see\\nManage access\\nkeys for IAM\\nusers .\\nYour access key.\\nSecret Access\\nKey\\nThe Identity and\\nAccess\\nManagement\\n(IAM) secret key.\\nYour secret key.\\n3. Select Next.\\n4. Browse to the target location for the shortcut.\\nIf you used the global endpoint in the connection URL, all of your available buckets\\nappear in the left navigation view. If you used a bucket specific endpoint in the\\nconnection URL, only the specified bucket and its contents appear in the\\nnavigation view.\\nNavigate the storage account by selecting a folder or clicking on the expansion\\narrow next to a folder.\\nIn this view, you can select one or more shortcut target locations. Choose target\\nlocations by clicking the checkbox next a folder in the left navigation view.\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 80, 'page_label': '81'}, page_content='5. Select Next\\nThe review page allows you to verify all of your selections. Here you can see each\\nshortcut that will be created. In the action column, you can click the pencil icon to\\nedit the shortcut name. You can click the trash can icon to delete shortcut.\\n6. Select Create.\\nThe lakehouse automatically refreshes. The shortcut appears in the left Explorer pane\\nunder the Tables section.\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 81, 'page_label': '82'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nCreate a OneLake shortcut\\nCreate an Azure Data Lake Storage Gen2 shortcut\\nUse OneLake shortcuts REST APIs\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 82, 'page_label': '83'}, page_content=\"Create an Amazon S3 compatible shortcut\\n07/28/2025\\nIn this article, you learn how to create an S3 compatible shortcut inside a Fabric lakehouse. For\\nan overview of shortcuts, see OneLake shortcuts.\\nS3 compatible shortcuts can take advantage of file caching to reduce egress costs associated\\nwith cross-cloud data access. For more information, see OneLake shortcuts Caching. Currently\\nonly key or secret authentication is supported for S3-compatible sources. Entra-based OAuth,\\nService Principal, and RoleArn are not yet supported.\\nIf you don't have a lakehouse, create one by following these steps: Create a lakehouse\\nwith OneLake.\\nEnsure your chosen S3 compatible bucket and secret key credentials meet the access and\\nauthorization requirements for S3 shortcuts.\\n1. Open a lakehouse.\\n2. Right-click on a directory within the Lake view of the lakehouse.\\n3. Select New shortcut.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 83, 'page_label': '84'}, page_content='1. Under External sources, select Amazon S3 compatible.\\n2. Enter the Connection settings according to the following table:\\nSelect a source\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 84, 'page_label': '85'}, page_content='Field Description Value\\nURL The connection string for your S3 compatible endpoint.\\nFor this shortcut type, you must provide a non-bucket\\nspecific URL. This URL must allow path style bucket\\naddressing, not just virtual hosted style.\\nhttps://s3.contoso.com\\nConnectionPreviously defined connections for the specified storage\\nlocation appear in the drop-down. If no connections\\nexist, create a new connection.\\nCreate new connection\\nConnection\\nname\\nThe S3 compatible connection name. A name for your\\nconnection.\\nAccess Key\\nID\\nThe access key ID to be used when accessing the S3\\ncompatible endpoint.\\nYour access key.\\nSecret\\nAccess Key\\nThe secret key associated with the access key ID.Your secret key.\\n3. Select Next.\\n4. Enter a name for your shortcut.\\nOptionally, you can enter a sub path to select a specific folder in your S3 bucket.\\n\\uf80a\\nﾉ Expand table\\n７ Note'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 85, 'page_label': '86'}, page_content='5. Select Create.\\nThe lakehouse automatically refreshes. The shortcut appears under Files in the Explorer pane.\\nCreate a OneLake shortcut\\nCreate an Azure Data Lake Storage Gen2 shortcut\\nShortcut paths are case sensitive.\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 86, 'page_label': '87'}, page_content=\"Integrate Microsoft Entra with AWS S3\\nshortcuts using service principal\\nauthentication\\n07/10/2025\\nYou can integrate Microsoft Entra with AWS S3 using the Service Principal Name (SPN)\\napproach. This integration enables seamless, secure access to S3 buckets using Microsoft Entra\\ncredentials, simplifying identity management and enhancing security.\\nUnified identity management: Use Microsoft Entra credentials to access Amazon S3. No\\nneed to manage AWS IAM users.\\nOIDC-based authentication: Uses OpenID Connect for secure authentication with AWS\\nIAM roles.\\nAuditing support: Full traceability through AWS CloudTrail to monitor role assumptions.\\nSeamless integration: Designed to integrate with existing AWS deployments with minimal\\nconfiguration changes.\\nThe Entra-AWS integration is built on a federated identity model that uses OpenID Connect\\n(OIDC) to enable secure, temporary access to AWS resources. The architecture consists of the\\nfollowing three main components that work together to establish trust, authenticate users, and\\nauthorize access to Amazon S3 from Microsoft Fabric:\\n1. A Service Principal (SPN) registered in Microsoft Entra.\\n2. An OIDC trust relationship between AWS and Microsoft Entra.\\n3. A Fabric connection that uses temporary credentials from AWS Security Token Service\\n(STS).\\nIn the following sections you'll configure Microsoft Entra ID, AWS IAM, and Microsoft Fabric for\\nsecure access to Amazon S3 using the service principal-based integration. This setup\\nestablishes the necessary trust relationships and connection details required for the integration\\nto work.\\nKey benefits\\nArchitecture\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 87, 'page_label': '88'}, page_content=\"Sign in to Azure portal and navigate to Microsoft Entra ID.\\nFrom the left-hand menu, expand Manage > App registrations > New registration. Fill\\nout the following details:\\nName: Enter a name for your application such as S3AccessServicePrincipal.\\nRedirect URL: Leave it blank or set to https://localhost if necessary.\\nSelect Register to register your application.\\nOpen the Microsoft Entra application you created above.\\nFrom the left-hand menu, expand Manage > Certificates and secrets > New client secret\\nto add a new secret\\nNote down the generated secret and its expiration date\\nClient Secret: Get this value from the previous step\\n*Tenant ID: From the Azure portal, navigate to Microsoft Entra ID and open the Overview\\ntab and get the Tenant ID value.\\n７ Note\\nOnly key or secret authentication is supported for S3-compatible sources; Entra-based\\nOAuth, Service Principal, and RoleArn are not supported.\\nConfigure Microsoft Entra ID\\nStep1: Register a Microsoft Entra application\\n７ Note\\nIt's recommended to use a unique Service Principal per AWS role for enhanced security\\nStep2: Create a client secret\\nStep3: Get the application details\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 88, 'page_label': '89'}, page_content='From the Azure portal, navigate to Microsoft Entra ID. From the left-hand navigation,\\nexpand the Manage tab and open Enterprise applications. Search for the application you\\ncreated in the previous step. Copy the following values\\nApplication ID (also known as Client ID)\\nObject ID\\nThe following screenshot shows you how to get the application/client ID and object ID.\\nSign into the AWS IAM portal.\\nNavigate to AWS IAM → Identity providers → Add provider\\nSelect Provider type as Open ID Connect\\nProvider URL: https://sts.windows.net/<your-tenant-id>\\nAudience: https://analysis.windows.net/powerbi/connector/AmazonS3\\n７ Note\\nThese values are from Microsoft Entra ID > Enterprise applications tab and NOT from\\nMicrosoft Entra ID > App registrations tab.\\n\\uf80a\\nAWS IAM configuration\\nStep 1: Create OIDC identity provider'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 89, 'page_label': '90'}, page_content='Navigate to AWS IAM → Roles → Create Role\\nTrusted entity type: Web identity\\nIdentity provider: Select the Open ID Connect provider created in Step 1\\nAudience: https://analysis.windows.net/powerbi/connector/AmazonS3\\nAssign appropriate S3 access policies to the role\\nEnsure that the Trust policy has the service principal as one of the conditions\\nJSON\\n\\uf80a\\nStep 2: Create IAM roles\\n{\\n  \"Version\": \"2012-10-17\",\\n  \"Statement\": [\\n    {\\n      \"Effect\": \"Allow\",\\n      \"Principal\": {\\n        \"Federated\": \"arn:aws:iam::<aws-account>:oidc-\\nprovider/sts.windows.net/<tenant-id>/\" // (1)\\n      },\\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",// (2)\\n      \"Condition\": {\\n        \"StringEquals\": {\\n          \"sts.windows.net/<tenant-id>/:sub\": \"<Object ID of the SPN that \\nwill assume this role>\", // (3)\\n          \"sts.windows.net/<tenant-id>/:aud\": \\n\"https://analysis.windows.net/powerbi/connector/AmazonS3\" // (4)\\n        }\\n      }\\n    }'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 90, 'page_label': '91'}, page_content=\"Description of key fields:\\n1. Principal.Federated – Specifies the external identity provider(OIDC from Microsoft Entra\\nID).\\n2. Action – Grants permission to assume the role using a web identity token.\\n3. Condition > :sub – Limits which Microsoft Entra ID service can assume the role. This is the\\nObject ID that you noted in Step3: Get the application details\\n4. Condition > :aud – Ensures the request is from Power BI's S3 connector.\\nUse Microsoft Fabric OneLake's shortcut creation interface to create the shortcut as described\\nin the create an S3 shortcut article. Follow the same steps, but set RoleARN to the Amazon\\nResource Name (ARN) for the IAM role, and set the Authentication Kind to Service Principal\\nand fill in the following details:\\nTenant ID: Tenant ID of the Microsoft Entra application\\nService principal client ID: The Application ID you got in the previous step.\\nService principal key: The client secret of the Microsoft Entra application\\nUse a separate service principal per AWS role for better isolation and auditability\\n  ]\\n}\\n\\uf80a\\nCreate an S3 connection in Fabric\\nSecurity recommendations\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 91, 'page_label': '92'}, page_content=\"Rotate secrets periodically and store them securely\\nMonitor AWS CloudTrail for STS-related activity\\nThis feature currently supports only the service principal-based approach; OAuth and\\nWorkspace Identity aren't yet supported.\\nAccess to S3 buckets behind a firewall via on-premises data gateway isn't currently\\nsupported with service principal or OAuth.\\nCreate an S3 shortcut\\nCreate an Amazon S3 compatible shortcut\\nCurrent limitations\\nRelated content\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 92, 'page_label': '93'}, page_content=\"Create a Google Cloud Storage (GCS)\\nshortcut\\nArticle• 03/31/2025\\nIn this article, you learn how to create a Google Cloud Storage (GCS) shortcut inside a\\nFabric lakehouse.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts\\nprogrammatically, see OneLake shortcuts REST APIs.\\nGCS shortcuts can take advantage of file caching to reduce egress costs associated with\\ncross-cloud data access. For more information, see OneLake shortcuts > Caching.\\nIf you don't have a lakehouse, create one by following these steps: Creating a\\nlakehouse with OneLake.\\nEnsure your chosen GCS bucket and user meet the access and authorization\\nrequirements for GCS shortcuts.\\n1. Open a lakehouse.\\n2. Right-click on a directory within the Lake view of the lakehouse.\\n3. Select New shortcut.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 93, 'page_label': '94'}, page_content='1. Under External sources, select Google Cloud Storage.\\n2. Enter the Connection settings according to the following table:\\n\\uf80a\\nSelect a source\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 94, 'page_label': '95'}, page_content='Field Description Value\\nURL The connection string\\nfor your GCS bucket.\\nThe bucket name is\\noptional.\\nhttps://BucketName.storage.googleapis.com\\nhttps://storage.googleapis.com\\nConnection Previously defined\\nconnections for the\\nspecified storage\\nlocation appear in the\\ndrop-down. If no\\nconnections exist, create\\na new connection.\\nCreate new connection\\nConnection\\nname\\nThe user defined name\\nfor the connection.\\nA name for your connection.\\nAuthentication\\nkind\\nFabric uses Hash-based\\nMessage Authentication\\nCode (HMAC) keys to\\naccess Google Cloud\\nstorage. These keys are\\nassociated with a user\\nor service account. The\\naccount must have\\npermission to access the\\ndata within the GCS\\nbucket. If the bucket\\nspecific endpoint was\\nused in the connection\\nHMAC Key\\n\\uf80a\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 95, 'page_label': '96'}, page_content='Field Description Value\\nURL, the account must\\nhave the\\nstorage.objects.get\\nand\\nstorage.objects.list\\npermissions. If the\\nglobal endpoint was\\nused in the connection\\nURL, the account must\\nalso have the\\nstorage.buckets.list\\npermission.\\nAccess ID The access key\\nassociated with a user\\nor service account. For\\nmore on creating HMAC\\nkeys, see Manage\\nHMAC Keys .\\nYour access key.\\nSecret The secret for the access\\nkey.\\nYour secret key.\\n3. Select Next.\\n4. Browse to the target location for the shortcut.\\nIf you used the global endpoint in the connection URL, all of your available buckets\\nappear in the left navigation view. If you used a bucket specific endpoint in the\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 96, 'page_label': '97'}, page_content='connection URL, only the specified bucket and its contents appear in the\\nnavigation view.\\nNavigate the storage account by selecting a folder or clicking on the expansion\\narrow next to a folder.\\nIn this view, you can select one or more shortcut target locations. Choose target\\nlocations by clicking the checkbox next a folder in the left navigation view.\\n5. Select Next\\nThe review page allows you to verify all of your selections. Here you can see each\\nshortcut that will be created. In the action column, you can click the pencil icon to\\nedit the shortcut name. You can click the trash can icon to delete shortcut.\\n6. Select Create.\\nThe lakehouse automatically refreshes. The shortcut appears in the left Explorer pane.\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 97, 'page_label': '98'}, page_content='Feedback\\nWas this page helpful?\\nCreate a OneLake shortcut\\nCreate an Azure Data Lake Storage Gen2 shortcut\\nUse OneLake shortcuts REST APIs\\n\\uf80a\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 98, 'page_label': '99'}, page_content='Provide product feedback | Ask the community'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 99, 'page_label': '100'}, page_content='Create shortcuts to on-premises data\\nArticle• 03/31/2025\\nWith OneLake Shortcuts, you can create virtual references to bring together data from a\\nvariety sources across clouds, regions, systems, and domains – all with no data\\nmovement or duplication. By using a Fabric on-premises data gateway (OPDG), you can\\nalso create shortcuts to on-premises data sources, such as S3 compatible storage\\nhosted on-premises. With this feature, you can also create shortcuts to other network-\\nrestricted data sources, such as Amazon S3 or Google Cloud Storage buckets configured\\nbehind a firewall or Virtual Private Cloud (VPC).\\nOn-premises data gateways are software agents that you install on a Windows machine\\nand configure to connect to your data endpoints. By selecting an OPDG when creating a\\nshortcut, you can establish network connectivity between OneLake and your data\\nsource.\\nThis feature is available for the following shortcut types:\\nAmazon S3\\nS3 compatible\\nGoogle Cloud Storage\\nYou can use this feature in any Fabric-enabled workspace.\\nOn-premises data gateway shortcuts can take advantage of file caching to reduce\\negress costs associated with cross-cloud data access. For more information, see\\nOneLake shortcuts > Caching.\\nIn this document, we show you how to install and use these on-premises data gateways\\nto create shortcuts to on-premises or network-restricted data.\\nCreate or identify a Fabric lakehouse that will contain your shortcut(s).\\nIdentify the endpoint URL associated with your Amazon S3, Google Cloud Storage,\\nor S3 compatible location.\\n） Important\\nThis feature is in preview.\\nPrerequisites'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 100, 'page_label': '101'}, page_content='For S3 compatible, the endpoint is the URL for the service, not a specific bucket.\\nFor example:\\nhttps://mys3api.contoso.com\\nhttp://10.0.1.4:9000\\nFor Amazon S3, the endpoint is the URL for a specific bucket. For example:\\nhttps://BucketName.s3.us-east.amazonaws.com\\nFor Google Cloud Storage, the endpoint is either the URL for the bucket or the\\nservice. For example:\\nhttps://storage.googleapis.com\\nhttps://bucketname.storage.googleapis.com\\nIdentify the user or identity credentials that meet the necessary access and\\nauthorization requirements for your data source. Your credentials generally need to\\nbe able to list buckets, list objects, and read data.\\nIdentify a physical or virtual machine that:\\nHas network connectivity to your storage endpoint. This article explains how\\nyou can confirm this connectivity before creating your shortcut.\\nAllows you to install software.\\nFollow the instructions to install a standard on-premises data gateway on the\\nmachine you identified. Be sure to install the latest version.\\nIf your storage endpoint uses a self-signed certificate for HTTPS connections, be\\nsure to trust this certificate on the machine hosting your gateway.\\nBefore setting up your shortcut, follow these steps to confirm that your gateway can\\nconnect to your storage endpoint.\\n1. Log in to the machine hosting the gateway.\\n2. Install a client application that can query S3 compatible data sources, such as the\\nAmazon Web Services Command Line Interface, WinSCP, or another tool of choice.\\n3. Connect to your endpoint URL and provide the credentials you identified in the\\nprerequisite steps.\\n4. Ensure you can explore and read data from your storage location.\\nReview the instructions for creating an Amazon S3, Google Cloud Storage, or S3\\ncompatible shortcut.\\nCheck connectivity from gateway host\\nCreate a shortcut'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 101, 'page_label': '102'}, page_content=\"Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nDuring shortcut creation, select your on-premises data gateway (OPDG) in the Data\\ngateway dropdown field.\\nIf you encounter any connectivity issues during shortcut creation, try the following\\ntroubleshooting steps.\\nAs needed, ensure the machine hosting your gateway can connect to your storage\\nendpoint. Follow the steps to check connectivity.\\nIf you're using HTTPS and need to use a self-signed certificate, ensure the machine\\nhosting your gateway trusts the certificate. You may need to install the self-signed\\ncertificate on the machine.\\nCreate an Amazon S3 shortcut\\nCreate a Google Cloud Storage shortcut\\nCreate an S3 compatible shortcut\\n７ Note\\nIf you do not see your OPDG in the Data gateway dropdown field and someone\\nelse created the gateway, ask them to share the gateway with you from the\\nManage connections and gateways interface.\\nTroubleshooting\\nRelated content\\n\\ue8e1Yes \\ue8e0No\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 102, 'page_label': '103'}, page_content='Use Iceberg tables with OneLake\\n07/01/2025\\nIn Microsoft OneLake, you can seamlessly work with tables in both Delta Lake and Apache\\nIceberg formats.\\nThis flexibility is enabled through metadata virtualization, a feature that allows Iceberg tables\\nto be interpreted as Delta Lake tables, and vice versa. You can directly write Iceberg tables or\\ncreate shortcuts to them, making these tables accessible across various Fabric workloads.\\nSimilarly, Fabric tables written in the Delta Lake format can be read using Iceberg readers.\\nWhen you write or create a shortcut to an Iceberg table folder, OneLake automatically\\ngenerates virtual Delta Lake metadata (Delta log) for the table, enabling its use with Fabric\\nworkloads. Conversely, Delta Lake tables now include virtual Iceberg metadata, allowing\\ncompatibility with Iceberg readers.\\nWhile this article includes guidance for using Iceberg tables with Snowflake, this feature is\\nintended to work with any Iceberg tables with Parquet-formatted data files in storage.\\n） Important\\nThis feature is in preview.\\nVirtualize Delta Lake tables as Iceberg'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 103, 'page_label': '104'}, page_content='To set up the automatic conversion and virtualization of tables from Delta Lake format to\\nIceberg format, follow these steps.\\n1. Enable automatic table virtualization of Delta Lake tables to the Iceberg format by turning\\non the delegated OneLake setting named Enable Delta Lake to Apache Iceberg table\\nformat virtualization in your workspace settings.\\n2. Make sure your Delta Lake table, or a shortcut to it, is located in the Tables section of\\nyour data item. The data item may be a lakehouse or another Fabric data item.\\n3. Confirm that your Delta Lake table has converted successfully to the virtual Iceberg\\nformat. You can do this by examining the directory behind the table.\\nTo view the directory if your table is in a lakehouse, you can right-click the table in the\\nFabric UI and select View files.\\nIf your table is in another data item type, such as a warehouse, a database, or a mirrored\\ndatabase, you will need to use a client like Azure Storage Explorer or OneLake File\\nExplorer, rather than the Fabric UI, to view the files behind the table.\\n4. You should see a directory named metadata inside the table folder, and it should contain\\nmultiple files, including the conversion log file. Open the conversion log file to see more\\ninfo about the Delta Lake to Iceberg conversion, including the timestamp of the most\\nrecent conversion and any error details.\\n5. If the conversion log file shows that the table was successfully converted, read the Iceberg\\ntable using your service, app, or library of choice.\\nDepending on what Iceberg reader you use, you will need to know either the the path to\\nthe table directory or to the most recent .metadata.json file shown in the metadata\\n７ Note\\nThis setting controls a feature that is currently in preview. This setting will be\\nremoved in a future update when the feature is enabled for all users and is no longer\\nin preview.\\n\\uea80 Tip\\nIf your lakehouse is schema-enabled, then your table directory will be located\\ndirectly within a schema such as dbo. If your lakehouse is not schema-enabled, then\\nyour table directory will be directly within the Tables directory.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 104, 'page_label': '105'}, page_content=\"directory.\\nYou can see the HTTP path to the latest metadata file of your table by opening the\\nProperties view for the *.metadata.json file with the highest version number. Take note\\nof this path.\\nThe path to your data item's Tables folder may look like this:\\nWithin that folder, the relative path to the latest metadata file may look like\\ndbo/MyTable/metadata/321.metadata.json.\\nTo read your virtual Iceberg table using Snowflake, follow the steps in this guide.\\nIf you already have an Iceberg table in a storage location supported by OneLake shortcuts,\\nfollow these steps to create a shortcut and have your Iceberg table appear with the Delta Lake\\nformat.\\n1. Locate your Iceberg table. Find where your Iceberg table is stored, which could be in\\nAzure Data Lake Storage, OneLake, Amazon S3, Google Cloud Storage, or an S3\\ncompatible storage service.\\nhttps://onelake.dfs.fabric.microsoft.com/83896315-c5ba-4777-8d1c-\\ne4ab3a7016bc/a95f62fa-2826-49f8-b561-a163ba537828/Tables/\\nCreate a table shortcut to an Iceberg table\\n７ Note\\nIf you're using Snowflake and aren't sure where your Iceberg table is stored, you can\\nrun the following statement to see the storage location of your Iceberg table.\\nSELECT SYSTEM$GET_ICEBERG_TABLE_INFORMATION('<table_name>');\\nRunning this statement returns a path to the metadata file for the Iceberg table. This\\npath tells you which storage account contains the Iceberg table. For example, here's\\nthe relevant info to find the path of an Iceberg table stored in Azure Data Lake\\nStorage:\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 105, 'page_label': '106'}, page_content='Your Iceberg table folder needs to contain a metadata folder, which itself contains at least\\none file ending in .metadata.json.\\n2. In your Fabric lakehouse, create a new table shortcut in the Tables area of a lakehouse.\\n3. For the target path of your shortcut, select the Iceberg table folder. The Iceberg table\\nfolder contains the metadata and data folders.\\n4. Once your shortcut is created, you should automatically see this table reflected as a Delta\\nLake table in your lakehouse, ready for you to use throughout Fabric.\\n{\"metadataLocation\":\"azure://<storage_account_path>/<path_within_storage>/<tabl\\ne_name>/metadata/00001-389700a2-977f-47a2-9f5f-\\n7fd80a0d41b2.metadata.json\",\"status\":\"success\"}\\n\\uea80 Tip\\nIf you see schemas such as dbo under the Tables folder of your lakehouse, then the\\nlakehouse is schema-enabled. In this case, right-click on the schema and create a\\ntable shortcut under the schema.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 106, 'page_label': '107'}, page_content=\"If your new Iceberg table shortcut doesn't appear as a usable table, check the\\nTroubleshooting section.\\nThe following tips can help make sure your Iceberg tables are compatible with this feature:\\nOpen your Iceberg folder in your preferred storage explorer tool, and check the directory\\nlisting of your Iceberg folder in its original location. You should see a folder structure like the\\nfollowing example.\\nIf you don't see the metadata folder, or if you don't see files with the extensions shown in this\\nexample, then you might not have a properly generated Iceberg table.\\nTroubleshooting\\nCheck the folder structure of your Iceberg table\\n../\\n|-- MyIcebergTable123/\\n    |-- data/\\n        |-- A5WYPKGO_2o_APgwTeNOAxg_0_1_002.parquet\\n        |-- A5WYPKGO_2o_AAIBON_h9Rc_0_1_003.parquet\\n    |-- metadata/\\n        |-- 00000-1bdf7d4c-dc90-488e-9dd9-2e44de30a465.metadata.json\\n        |-- 00001-08bf3227-b5d2-40e2-a8c7-2934ea97e6da.metadata.json\\n        |-- 00002-0f6303de-382e-4ebc-b9ed-6195bd0fb0e7.metadata.json\\n        |-- 1730313479898000000-Kws8nlgCX2QxoDHYHm4uMQ.avro\\n        |-- 1730313479898000000-OdsKRrRogW_PVK9njHIqAA.avro\\n        |-- snap-1730313479898000000-9029d7a2-b3cc-46af-96c1-ac92356e93e9.avro\\n        |-- snap-1730313479898000000-913546ba-bb04-4c8e-81be-342b0cbc5b50.avro\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 107, 'page_label': '108'}, page_content=\"When an Iceberg table is virtualized as a Delta Lake table, a folder named _delta_log/ can be\\nfound inside the shortcut folder. This folder contains the Delta Lake format's metadata (the\\nDelta log) after successful conversion.\\nThis folder also includes the latest_conversion_log.txt file, which contains the latest\\nattempted conversion's success or failure details.\\nTo see the contents of this file after creating your shortcut, open the menu for the Iceberg table\\nshortcut under Tables area of your lakehouse and select View files.\\nYou should see a structure like the following example:\\nOpen the conversion log file to see the latest conversion time or failure details. If you don't see\\na conversion log file, conversion wasn't attempted.\\nIf you don't see a conversion log file, then the conversion wasn't attempted. Here are two\\ncommon reasons why conversion isn't attempted:\\nThe shortcut wasn't created in the right place.\\nCheck the conversion log\\nTables/\\n|-- MyIcebergTable123/\\n    |-- data/\\n        |-- <data files>\\n    |-- metadata/\\n        |-- <metadata files>\\n    |-- _delta_log/   <-- Virtual folder. This folder doesn't exist in the \\noriginal location.\\n        |-- 00000000000000000000.json\\n        |-- latest_conversion_log.txt   <-- Conversion log with latest \\nsuccess/failure details.\\nIf conversion wasn't attempted\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 108, 'page_label': '109'}, page_content='In order for a shortcut to an Iceberg table to get converted to the Delta Lake format, the\\nshortcut must be placed directly under the Tables folder of a non-schema-enabled\\nlakehouse. You shouldn\\'t place the shortcut in the Files section or under another folder if\\nyou want the table to be automatically virtualized as a Delta Lake table.\\nThe shortcut\\'s target path is not the Iceberg folder path.\\nWhen you create the shortcut, the folder path you select in the target storage location\\nmust only be the Iceberg table folder. This folder contains the metadata and data folders.\\nIf you are using Snowflake to write a new Iceberg table to OneLake, you might see the\\nfollowing error message:\\nFabric capacity region cannot be validated. Reason: \\'Invalid access token. This may be due\\nto authentication and scoping. Please verify delegated scopes.\\'\\nIf you see this error, have your Fabric tenant admin double-check that you\\'ve enabled both\\ntenant settings mentioned in the Write an Iceberg table to OneLake using Snowflake section:\\n\"Fabric capacity region cannot be validated\" error message in\\nSnowflake'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 109, 'page_label': '110'}, page_content=\"1. In the upper-right corner of the Fabric UI, open Settings, and select Admin portal.\\n2. Under Tenant settings, in the Developer settings section, enable the setting labeled\\nService principals can use Fabric APIs.\\n3. In the same area, in the OneLake settings section, enable the setting labeled Users can\\naccess data stored in OneLake with apps external to Fabric.\\nKeep in mind the following temporary limitations when you use this feature:\\nSupported data types\\nThe following Iceberg column data types map to their corresponding Delta Lake types\\nusing this feature.\\nIceberg\\ncolumn type\\nDelta Lake\\ncolumn type\\nComments\\nint integer\\nlong long See Type width issue.\\nfloat float\\ndouble double See Type width issue.\\ndecimal(P, S) decimal(P, S) See Type width issue.\\nboolean boolean\\ndate date\\ntimestamp timestamp_ntz The timestamp Iceberg data type doesn't contain time zone\\ninformation. The timestamp_ntz Delta Lake type isn't fully\\nsupported across Fabric workloads. We recommend the use of\\ntimestamps with time zones included.\\ntimestamptz timestamp In Snowflake, to use this type, specify timestamp_ltz as the\\ncolumn type during Iceberg table creation. More info on\\nIceberg data types supported in Snowflake can be found here.\\nstring string\\nbinary binary\\ntime N/A Not supported\\nLimitations and considerations\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 110, 'page_label': '111'}, page_content='Type width issue\\nIf you use Snowflake to write your Iceberg table and the table contains column types\\nINT64, double, or Decimal with precision >= 10, then the resulting virtual Delta Lake\\ntable may not be consumable by all Fabric engines. You may see errors such as:\\nWe\\'re working on a fix for this issue.\\nWorkaround: If you\\'re using the Lakehouse table preview UI and see this issue, you can\\nresolve this error by switching to the SQL Endpoint view (top right corner, select\\nLakehouse view, switch to SQL Endpoint) and previewing the table from there. If you then\\nswitch back to the Lakehouse view, the table preview should display properly.\\nIf you\\'re running a Spark notebook or job and encounter this issue, you can resolve this\\nerror by setting the spark.sql.parquet.enableVectorizedReader Spark configuration to\\nfalse. Here\\'s an example PySpark command to run in a Spark notebook:\\nIceberg table metadata storage isn\\'t portable\\nThe metadata files of an Iceberg table refer to each other using absolute path references.\\nIf you copy or move an Iceberg table\\'s folder contents to another location without\\nrewriting the Iceberg metadata files, the table becomes unreadable by Iceberg readers,\\nincluding this OneLake feature.\\nWorkaround:\\nIf you need to move your Iceberg table to another location to use this feature, use the\\ntool that originally wrote the Iceberg table to write a new Iceberg table in the desired\\nlocation.\\nIceberg table folders must contain only one set of metadata files\\nIf you drop and recreate an Iceberg table in Snowflake, the metadata files aren\\'t cleaned\\nup. This behavior is by design, in support of the UNDROP feature in Snowflake. However,\\nbecause your shortcut points directly to a folder and that folder now has multiple sets of\\nParquet column cannot be converted in file ... Column: [ColumnA], Expected: \\ndecimal(18,4), Found: INT32.\\nspark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 111, 'page_label': '112'}, page_content=\"metadata files within it, we can't convert the table until you remove the old table’s\\nmetadata files.\\nConversion will fail if more than one set of metadata files are found in the Iceberg table's\\nmetadata folder.\\nWorkaround:\\nTo ensure the converted table reflects the correct version of the table:\\nEnsure you aren’t storing more than one Iceberg table in the same folder.\\nClean up any contents of an Iceberg table folder after dropping it, before recreating\\nthe table.\\nMetadata changes not immediately reflected\\nIf you make metadata changes to your Iceberg table, such as adding a column, deleting a\\ncolumn, renaming a column, or changing a column type, the table may not be\\nreconverted until a data change is made, such as adding a row of data.\\nWe're working on a fix that picks up the correct latest metadata file that includes the\\nlatest metadata change.\\nWorkaround:\\nAfter making the schema change to your Iceberg table, add a row of data or make any\\nother change to the data. After that change, you should be able to refresh and see the\\nlatest view of your table in Fabric.\\nRegion availability limitation\\nThe feature isn't yet available in the following regions:\\nQatar Central\\nNorway West\\nWorkaround:\\nWorkspaces attached to Fabric capacities in other regions can use this feature. See the full\\nlist of regions where Microsoft Fabric is available.\\nPrivate links not supported\\nThis feature isn't currently supported for tenants or workspaces that have private links\\nenabled.\\nWe're working on an improvement to remove this limitation.\\nOneLake shortcuts must be same-region\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 112, 'page_label': '113'}, page_content=\"We have a temporary limitation on the use of this feature with shortcuts that point to\\nOneLake locations: the target location of the shortcut must be in the same region as the\\nshortcut itself.\\nWe're working on an improvement to remove this requirement.\\nWorkaround:\\nIf you have a OneLake shortcut to an Iceberg table in another lakehouse, be sure that the\\nother lakehouse is associated with a capacity in the same region.\\nCertain Iceberg partition transform types are not supported\\nCurrently, the Iceberg partition types bucket[N], truncate[W], and void are not\\nsupported.\\nIf the Iceberg table being converted contains these partition transform types,\\nvirtualization to the Delta Lake format will not succeed.\\nWe're working on an improvement to remove this limitation.\\nUse Snowflake to write or read Iceberg tables in OneLake.\\nLearn more about Fabric and OneLake security.\\nLearn more about OneLake shortcuts.\\nRelated content\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 113, 'page_label': '114'}, page_content=\"Use Snowflake with Iceberg tables in\\nOneLake\\nMicrosoft OneLake can be used with Snowflake for storage and access of Apache Iceberg\\ntables.\\nFollow this guide to use Snowflake on Azure to:\\nwrite Iceberg tables directly to OneLake\\nread virtual Iceberg tables converted from the Delta Lake format\\nBefore getting started, follow the pre-requisite steps shown below.\\nTo use Snowflake on Azure to write or read Iceberg tables with OneLake, your Snowflake\\naccount's identity in Entra ID needs to be able to communicate with Fabric. Enable the Fabric\\ntenant-level settings that allow service principals to call Fabric APIs and to call OneLake APIs.\\nIf you use Snowflake on Azure, you can write Iceberg tables to OneLake by following these\\nsteps:\\n1. Make sure your Fabric capacity is in the same Azure location as your Snowflake instance.\\nIdentify the location of the Fabric capacity associated with your Fabric lakehouse. Open\\nthe settings of the Fabric workspace that contains your lakehouse.\\n） Important\\nThis feature is in preview.\\nPrerequisite\\nWrite an Iceberg table to OneLake using Snowflake\\non Azure\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 114, 'page_label': '115'}, page_content='In the bottom-left corner of your Snowflake on Azure account interface, check the Azure\\nregion of the Snowflake account.\\nIf these regions are different, you need to use a different Fabric capacity in the same\\nregion as your Snowflake account.\\n2. Open the menu for the Files area of your lakehouse, select Properties, and copy the URL\\n(the HTTPS path) of that folder.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 115, 'page_label': '116'}, page_content='3. Identify your Fabric tenant ID. Select your user profile in the top-right corner of the Fabric\\nUI, and hover over the info bubble next to your Tenant Name. Copy the Tenant ID.\\n4. In Snowflake, set up your EXTERNAL VOLUME using the path to the Files folder in your\\nlakehouse. More info on setting up Snowflake external volumes can be found here.\\nSQL\\n７ Note\\nSnowflake requires the URL scheme to be azure://, so be sure to change the path\\nfrom https:// to azure://.\\nCREATE OR REPLACE EXTERNAL VOLUME onelake_write_exvol\\nSTORAGE_LOCATIONS =\\n('),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 116, 'page_label': '117'}, page_content=\"In this sample, any tables created using this external volume are stored in the Fabric\\nlakehouse, within the Files/icebergtables folder.\\n5. Now that your external volume is created, run the following command to retrieve the\\nconsent URL and name of the application that Snowflake uses to write to OneLake. This\\napplication is used by any other external volume in your Snowflake account.\\nSQL\\nThe output of this command returns the AZURE_CONSENT_URL and\\nAZURE_MULTI_TENANT_APP_NAME properties. Take note of both values. The Azure multitenant\\napp name looks like <name>_<number>, but you only need to capture the <name> portion.\\n6. Open the consent URL from the previous step in a new browser tab, if you haven't done\\nthis previously. If you would like to proceed, consent to the required application\\npermissions, if prompted. You may be redirected to the main Snowflake website.\\n7. Back in Fabric, open your workspace and select Manage access, then Add people or\\ngroups. Grant the application used by your Snowflake external volume the permissions\\nneeded to write data to lakehouses in your workspace. We recommend granting the\\nContributor role.\\n8. Back in Snowflake, use your new external volume to create an Iceberg table.\\nSQL\\nAfter running this statement, a new Iceberg table folder named Inventory has been\\ncreated within the folder path defined in the external volume.\\n    (\\n        NAME = 'onelake_write_exvol'\\n        STORAGE_PROVIDER = 'AZURE'\\n        STORAGE_BASE_URL = 'azure://<path_to_lakehouse>/Files/icebergtables'\\n        AZURE_TENANT_ID = '<Tenant_ID>'\\n    )\\n);\\nDESC EXTERNAL VOLUME onelake_write_exvol;\\nCREATE OR REPLACE ICEBERG TABLE MYDATABASE.PUBLIC.Inventory (\\n    InventoryId int,\\n    ItemName STRING\\n)\\nEXTERNAL_VOLUME = 'onelake_write_exvol'\\nCATALOG = 'SNOWFLAKE'\\nBASE_LOCATION = 'Inventory/';\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 117, 'page_label': '118'}, page_content=\"9. Add some data to your Iceberg table.\\nSQL\\n10. Finally, in the Tables area of the same lakehouse, you can create a OneLake shortcut to\\nyour Iceberg table. Through that shortcut, your Iceberg table appears as a Delta Lake\\ntable for consumption across Fabric workloads.\\nTo use Snowflake on Azure to read a virtual Iceberg table based on a Delta Lake table in Fabric,\\nfollow these steps.\\n1. Follow the guide to confirm your Delta Lake table has converted successfully to Iceberg,\\nand take note of the path to the data item containing your table, as well as your table's\\nmost recent *.metadata.json file.\\n2. Identify your Fabric tenant ID. Select your user profile in the top-right corner of the Fabric\\nUI, and hover over the info bubble next to your Tenant Name. Copy the Tenant ID.\\n3. In Snowflake, set up your EXTERNAL VOLUME using the path to the Tables folder of the data\\nitem that contains your table. More info on setting up Snowflake external volumes can be\\nfound here.\\nSQL\\nINSERT INTO MYDATABASE.PUBLIC.Inventory\\nVALUES\\n(123456,'Amatriciana');\\nRead a virtual Iceberg table from OneLake using\\nSnowflake on Azure\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 118, 'page_label': '119'}, page_content=\"4. Now that your external volume is created, run the following command to retrieve the\\nconsent URL and name of the application that Snowflake uses to write to OneLake. This\\napplication is used by any other external volume in your Snowflake account.\\nSQL\\nThe output of this command returns the AZURE_CONSENT_URL and\\nAZURE_MULTI_TENANT_APP_NAME properties. Take note of both values. The Azure multitenant\\napp name looks like <name>_<number>, but you only need to capture the <name> portion.\\n5. Open the consent URL from the previous step in a new browser tab, if you haven't done\\nthis previously. If you would like to proceed, consent to the required application\\npermissions, if prompted. You may be redirected to the main Snowflake website.\\n6. Back in Fabric, open your workspace and select Manage access, then Add people or\\ngroups. Grant the application used by your Snowflake external volume the permissions\\nneeded to read data from data items in your workspace.\\nCREATE OR REPLACE EXTERNAL VOLUME onelake_read_exvol\\nSTORAGE_LOCATIONS =\\n(\\n    (\\n        NAME = 'onelake_read_exvol'\\n        STORAGE_PROVIDER = 'AZURE'\\n        STORAGE_BASE_URL = 'azure://<path_to_data_item>/Tables/'\\n        AZURE_TENANT_ID = '<Tenant_ID>'\\n    )\\n)\\nALLOW_WRITES = false;\\n７ Note\\nSnowflake requires the URL scheme to be azure://, so be sure to change https://\\nto azure://.\\nReplace <path_to_data_item> with the path to your data item, such as\\nhttps://onelake.dfs.fabric.microsoft.com/83896315-c5ba-4777-8d1c-\\ne4ab3a7016bc/a95f62fa-2826-49f8-b561-a163ba537828.\\nDESC EXTERNAL VOLUME onelake_read_exvol;\\n\\uea80 Tip\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 119, 'page_label': '120'}, page_content=\"7. Create the CATALOG INTEGRATION object in Snowflake, if you haven't done this previously.\\nThis is required by Snowflake to reference existing Iceberg tables in storage.\\nSQL\\n8. Back in Snowflake, create an Iceberg table referencing the latest metadata file for the\\nvirtualized Iceberg table in OneLake.\\nSQL\\nAfter running this statement, you now have a reference to your virtualized Iceberg table\\nthat you can now query using Snowflake.\\n9. Query your virtualized Iceberg table by running the following statement.\\nSQL\\nSee the troubleshooting and limitations and considerations sections of our documentation of\\nOneLake table format virtualization and conversion between Delta Lake and Apache Iceberg\\ntable formats.\\nYou may instead choose to grant permissions at the data item level, if you wish.\\nLearn more about OneLake data access.\\nCREATE CATALOG INTEGRATION onelake_catalog_integration\\nCATALOG_SOURCE = OBJECT_STORE\\nTABLE_FORMAT = ICEBERG\\nENABLED = TRUE;\\nCREATE OR REPLACE ICEBERG TABLE MYDATABASE.PUBLIC.<TABLE_NAME>\\nEXTERNAL_VOLUME = 'onelake_read_exvol'\\nCATALOG = onelake_catalog_integration\\nMETADATA_FILE_PATH = '<metadata_file_path>';\\n７ Note\\nReplace <TABLE_NAME> with your table name, and <metadata_file_path> with your\\nIceberg table's metadata file path, such as dbo/MyTable/metadata/321.metadata.json.\\nSELECT TOP 10 * FROM MYDATABASE.PUBLIC.<TABLE_NAME>;\\nTroubleshooting\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 120, 'page_label': '121'}, page_content='Last updated on 07/01/2025'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 121, 'page_label': '122'}, page_content=\"OneLake shortcut security\\nOneLake shortcuts serve as pointers to data residing in various storage accounts, whether\\nwithin OneLake itself or in external systems like Azure Data Lake Storage (ADLS). This article\\nlooks at the permissions required to create shortcuts and access data using them.\\nTo ensure clarity around the components of a shortcut this document uses the following terms:\\nTarget path: The location that a shortcut points to.\\nShortcut path: The location where the shortcut appears.\\nTo create a shortcut a user needs to have Write permission on the Fabric Item where the\\nshortcut is being created. In addition, the user needs Read access to the data the shortcut is\\npointing to. Shortcuts to external sources might require certain permissions in the external\\nsystem. The What are shortcuts? article has the full list of shortcut types and required\\npermissions.\\nCapability Permission on shortcut path Permission on target path\\nCreate a shortcut Write ReadAll\\nDelete a shortcut Write N/A\\n If OneLake security is enabled, the user needs to be in a role that grants access to the target\\npath.  If OneLake data access roles is enabled, the user needs to be in a role that grants access\\nto the target path.\\nA combination of the permissions in the shortcut path and the target path governs the\\npermissions for shortcuts. When a user accesses a shortcut, the most restrictive permission of\\nthe two locations is applied. Therefore, a user that has read/write permissions in the lakehouse\\nbut only read permissions in the target path can't write to the target path. Likewise, a user that\\nonly has read permissions in the lakehouse but read/write in the target path also can't write to\\nthe target path.\\nThis table shows the permissions needed for each shortcut action.\\nCreate and delete shortcuts\\nﾉ Expand table\\n2 1\\n2\\n1\\n2\\nAccessing shortcuts\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 122, 'page_label': '123'}, page_content=\"Capability Permission on\\nshortcut path\\nPermission on target\\npath\\nRead file/folder content of shortcut ReadAll ReadAll\\nWrite to shortcut target location Write Write\\nRead data from shortcuts in table section of the\\nlakehouse via TDS endpoint\\nRead ReadAll\\n If OneLake security is enabled the user needs to be in a role that grants access to the target\\npath.\\n Alternatively, OneLake security with ReadWrite permission on the shortcut path.\\nOneLake security (preview) is a feature that enables you to apply role-based access control\\n(RBAC) to your data stored in OneLake. You can define security roles that grant read access to\\nspecific tables and folders within a Fabric item, and assign them to users or groups. The access\\npermissions determine what users will across all engines in Fabric, ensuring consistent access\\ncontrol.\\nﾉ Expand table\\n1 1\\n2 2\\n3\\n1\\n2\\n） Important\\n Exception to identity passthrough: While OneLake security typically passes through the\\ncalling user's identity to enforce permissions, certain query engines operate differently.\\nWhen accessing shortcut data through Power BI semantic models using DirectLake over\\nSQL or T-SQL engines configured for Delegated identity mode, these engines don't pass\\nthrough the calling user's identity to the shortcut target. Instead, they use the item\\nowner's identity to access the data, and then apply OneLake security roles to filter what\\nthe calling user can see.\\nThis means:\\nThe shortcut target is accessed using the item owner's permissions (not the end\\nuser's)\\nOneLake security roles still determine what data the end user can read\\nAny permissions configured directly at the shortcut target path for the end user are\\nbypassed\\n3\\nOneLake security\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 123, 'page_label': '124'}, page_content=\"Users in the Admin, Member, and Contributor roles have full access to read data from a\\nshortcut regardless of the OneLake data access roles defined. However they still need access\\non both the shortcut path and target path as mentioned in Workspace roles.\\nUsers in the Viewer role or that had a lakehouse shared with them directly have access\\nrestricted based on if the user has access through a OneLake data access role. For more\\ninformation on the access control model with shortcuts, see Data Access Control Model in\\nOneLake.\\nUsers in Viewer roles can create shortcuts if they have ReadWrite permissions on the path\\nwhere the shortcut is created.\\nThe following table illustrates the necessary permissions to perform shortcut operations.\\nShortcut operation Permission on shortcut path Permission on target path\\nCreate Fabric Read and OneLake security\\nReadWrite\\nOneLake security Read\\nRead (GET/LIST\\nshortcuts)\\nFabric Read and OneLake security Read N/A\\nUpdate Fabric Read and OneLake security\\nReadWrited\\nOneLake security Read (on the new\\ntarget)\\nDelete Fabric Read and OneLake security\\nReadWrite\\nN/A\\nShortcuts use two authentication models with OneLake security: passthrough and delegated.\\nIn the passthrough model, the shortcut accesses data in the target location by 'passing' the\\nuser’s identity to the target system. This ensures that any user accessing the shortcut is only\\nable to see whatever they have access to in the target.\\nWith OneLake to OneLake shortcuts, only passthrough mode is supported. This design ensures\\nthat the source system retains full control over its data. Organizations benefit from enhanced\\nsecurity because there’s no need to replicate or redefine access controls for the shortcut.\\nHowever, it’s important to understand that security for OneLake shortcuts can't be modified\\ndirectly from the downstream item. Any changes to access permissions must be made at the\\nsource location.\\nﾉ Expand table\\nShortcut auth models\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 124, 'page_label': '125'}, page_content=\"Delegated shortcuts access data by using some intermediate credential, such as another user\\nor an account key. These shortcuts allow for permission management to be separated or\\n'delegated' to another team or downstream user to manage. Delegated shortcuts always break\\nthe flow of security from one system to another. All delegated shortcuts in OneLake can have\\nOneLake security roles defined for them.\\nAll shortcuts from OneLake to external systems (multicloud shortcuts) like AWS S3 or Google\\nCloud Storage are delegated. This allows users to connect to the external system without being\\ngiven direct access. OneLake security can then be configured on the shortcut to limit what data\\nin the external system can be accessed\\nIn addition to OneLake security access to the target path, accessing external shortcuts via\\nSpark or direct API calls also require read permissions on the item containing the external\\nshortcut path.\\n\\uf80a\\n\\uf80a\\nOneLake security limitations\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 125, 'page_label': '126'}, page_content='What are shortcuts?\\nCreate a OneLake shortcut\\nUse OneLake shortcuts REST APIs\\nData Access Control Model in OneLake.\\nLast updated on 11/18/2025\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 126, 'page_label': '127'}, page_content='Manage connections for shortcuts\\nArticle• 04/25/2025\\nShortcuts in OneLake use shared cloud connections to access the cloud resources where your\\ndata is stored. These connections can be managed on a per-shortcut basis, but you can also\\nview and update connections in bulk to keep all of your shortcuts working efficiently.\\nYou can view and manage all existing cloud connections for shortcuts in a single lakehouse.\\n1. In the Microsoft Fabric portal, navigate to your lakehouse.\\n2. Select Settings.\\n3. Select Shortcut connections.\\nView shortcut connections'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 127, 'page_label': '128'}, page_content=\"4. On the Manage OneLake shortcut connections page, you can view all connections. The\\nAction required section highlights any broken connections that need attention. You can\\nalso see how many shortcuts share each connection.\\nThere are many reasons that you might have to replace a cloud connection. Maybe the\\nconnection is broken, maybe the user that created that connection left your organization and\\nyou can't access the connection anymore, or maybe you want to switch to a different\\nconnection that uses a different authentication method.\\n1. On the Manage OneLake shortcut connections page, select Replace for the connection\\nthat you want to update.\\n2. Select either Existing connection or Create new connection.\\n3. Provide the new connection information.\\nFor an existing connection, use the drop-down menu to select the connection, then\\nselect Save.\\nFor a new connection, provide the connection settings and credentials, then select\\nSave.\\nReplace shortcut connections\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 128, 'page_label': '129'}, page_content='Once the new cloud connection is established, all of the shortcuts that used the old connection\\nare updated to use the new connection.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 129, 'page_label': '130'}, page_content='Access Fabric OneLake shortcuts in an\\nApache Spark notebook\\nArticle• 06/05/2024\\nFor an overview of shortcuts, see OneLake shortcuts.\\nShortcuts appear as folders in OneLake, and Apache Spark can read from them just like\\nany other folder in OneLake.\\nTo access a shortcut as a folder:\\n1. From a lakehouse containing shortcuts, select Open notebook and then select\\nNew notebook.\\n2. Select a shortcut and right-click on a file from the shortcut.\\n3. In the right-click menu, select Load data and then select Spark.\\n4. Run the automatically generated code cell.\\nAccess shortcuts as folders in an Apache Spark\\nnotebook'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 130, 'page_label': '131'}, page_content='Microsoft Fabric automatically recognizes shortcuts in the Tables section of the\\nlakehouse that have data in the Delta\\\\Parquet format as tables. You can reference these\\ntables directly from a Spark notebook.\\nTo access a shortcut as a table:\\n1. From a Lakehouse containing shortcuts, select Open notebook and then select\\nNew notebook.\\n2. Select the Table view in the notebook.\\nAccess shortcuts as tables in a Spark notebook'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 131, 'page_label': '132'}, page_content='3. Right-click on the table, then select Load data and Spark.\\n4. Run the automatically generated code cell.\\nYou can also access shortcuts through the Azure Blob Filesystem (ABFS) driver or REST\\nendpoint directly. Copy these paths from the lakehouse.\\n1. Open a Lakehouse containing shortcuts.\\n2. Right-click on a shortcut and select Properties.\\n\\uf80a\\nAccess the HTTPS and ABFS paths of a shortcut'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 132, 'page_label': '133'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\n3. Select the copy icon next to the ABFS path or URL in the Properties screen.\\nOneLake access and APIs\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 133, 'page_label': '134'}, page_content='Assign variables to shortcuts (preview)\\n06/13/2025\\nFabric lifecycle management tools allow for the simple collaboration and continuous\\ndevelopment of analytical solutions across multiple environments like testing and production.\\nTo lean more about these tools and processes see: Introduction to CI/CD in Microsoft Fabric\\nWhen deploying solutions across environments, you may want to configure properties that are\\nunique to each environment so your testing environment points to test data and your\\nproduction environment points to production data. Workspace variables make this possible.\\nYou can use workspace variables within individual shortcut properties. This allows you to have\\nunique values for properties like connection ID or target location for each environment.\\nWorkspace variables and variable values sets can be defined within a variable library. See: Learn\\nhow to use Variable libraries .\\nOnce a variable is defined within a variable library, it can be assigned to a shortcut property\\nusing the manage shortcuts UX.\\n1. Open a lakehouse and select an existing shortcut\\n2. Right click on the shortcut and choose Manage Shortcut\\n） Important\\nThis feature is in preview.\\nAssign a variable through the UX'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 134, 'page_label': '135'}, page_content='3. Select Edit variables and choose the desired property to assign the variable to.\\n4. Assign a variable from the variable library\\n5. Once the variable is assigned, the variable name and variable value appear below the\\nshortcut property'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 135, 'page_label': '136'}, page_content='７ Note\\nOnly variables of type string are supported. Selecting a variable of any other type results\\nin an error.\\n７ Note\\nAssignment of workspace variables through the shortcuts REST API is not currently\\nsupported.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 136, 'page_label': '137'}, page_content='OneLake Shortcuts\\nService:Core\\nAPI Version:v1\\nCreate Shortcut Creates a new shortcut or updates an existing shortcut.\\nCreates Shortcuts In\\nBulk\\nCreates bulk shortcuts.\\nDelete Shortcut Deletes the shortcut but does not delete the destination storage folder.\\nGet Shortcut Returns shortcut properties.\\nList Shortcuts Returns a list of shortcuts for the item, including all the subfolders\\nexhaustively.\\nReset Shortcut CacheDeletes any cached files that were stored while reading from shortcuts.\\nOperations\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 137, 'page_label': '138'}, page_content=\"Connecting to Microsoft OneLake\\n09/19/2025\\nMicrosoft OneLake provides open access to all of your Fabric items through existing Azure\\nData Lake Storage (ADLS) and Blob APIs and SDKs. You can access your data in OneLake\\nthrough any API, SDK, or tool compatible with ADLS or Azure Blob Storage just by using a\\nOneLake URI instead. You can upload data to a lakehouse through Azure Storage Explorer, or\\nread a delta table through a shortcut from Azure Databricks.\\nAs OneLake is software as a service (SaaS), some operations, such as managing permissions or\\nupdating items, must be done through Fabric experiences, and can't be done via ADLS APIs.\\nFor a full list of changes to these APIs, see OneLake API parity.\\nBecause OneLake exists across your entire Microsoft Fabric tenant, you can refer to anything in\\nyour tenant by its workspace, item, and path:\\nHTTP\\nOneLake also supports referencing workspaces and items with globally unique identifiers\\n(GUIDs). OneLake assigns GUIDs and GUIDs don't change, even if the workspace or item name\\nchanges. You can find the associated GUID for your workspace or item in the URL on the Fabric\\nportal. You must use GUIDs for both the workspace and the item, and don't need the item type.\\nHTTP\\nWhen adopting a tool for use over OneLake instead of ADLS, use the following mapping:\\nURI syntax\\nhttps://onelake.dfs.fabric.microsoft.com/<workspace>/<item>.\\n<itemtype>/<path>/<fileName>\\n７ Note\\nBecause you can reuse item names across multiple item types, you must specify the item\\ntype in the extension. For example, .lakehouse for a lakehouse and .warehouse for a\\nwarehouse.\\nhttps://onelake.dfs.fabric.microsoft.com/<workspaceGUID>/<itemGUID>/<path>/<fileNa\\nme>\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 138, 'page_label': '139'}, page_content=\"The account name is always onelake.\\nThe container name is your workspace name.\\nThe data path starts at the item. For example: /mylakehouse.lakehouse/Files/.\\nOneLake also supports the Azure Blob Filesystem driver (ABFS) for more compatibility with\\nADLS and Azure Blob Storage. The ABFS driver uses its own scheme identifier abfs and a\\ndifferent URI format to address files and directories in ADLS accounts. To use this URI format\\nover OneLake, swap workspace for filesystem and include the item and item type.\\nHTTP\\nThe abfs driver URI doesn't allow special characters, such as spaces, in the workspace name. In\\nthese cases, you can reference workspaces and items with the globally unique identifiers\\n(GUIDs) as described earlier in this section.\\nYou can authenticate OneLake APIs using Microsoft Entra ID by passing through an\\nauthorization header. If a tool supports logging into your Azure account to enable token\\npassthrough, you can select any subscription. OneLake only requires your user token and\\ndoesn't care about your Azure subscription.\\nWhen calling OneLake via DFS APIs directly, you can authenticate with a bearer token for your\\nMicrosoft Entra account. To learn more about requesting and managing bearer tokens for your\\norganization, check out the Microsoft Authentication Library.\\nFor quick, ad-hoc testing of OneLake using direct API calls, here's a simple example using\\nPowerShell to sign in to your Azure account, retrieve a storage-scoped token, and copy it to\\nyour clipboard for easy use elsewhere. For more information about retrieving access tokens\\nusing PowerShell, see Get-AzAccessToken.\\nPowerShell\\nabfs[s]://<workspace>@onelake.dfs.fabric.microsoft.com/<item>.\\n<itemtype>/<path>/<fileName>\\nAuthorization\\n７ Note\\nOneLake only supports tokens in the Storage audience. In the following example, we set\\nthe audience through the ResourceTypeName parameter.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 139, 'page_label': '140'}, page_content='If you use the global endpoint (\\'https://onelake.dfs.fabric.microsoft.com`) to query data in a\\nregion different than your workspace\\'s region, there\\'s a possibility that data could leave your\\nregion during the endpoint resolution process. If you\\'re concerned about data residency, using\\nthe correct regional endpoint for your workspace ensures your data stays within its current\\nregion and doesn\\'t cross any regional boundaries. You can discover the correct regional\\nendpoint by checking the region of the capacity that the workspace is attached to.\\nOneLake regional endpoints all follow the same format: https://<region>-\\nonelake.dfs.fabric.microsoft.com. For example, a workspace attached to a capacity in the\\nWest US region would be accessible through the regional endpoint https://westus-\\nonelake.dfs.fabric.microsoft.com.\\nIf a tool or package compatible with ADLS isn\\'t working over OneLake, the most common issue\\nis URL validation. As OneLake uses a different endpoint (dfs.fabric.microsoft.com) than ADLS\\n(dfs.core.windows.net), some tools don\\'t recognize the OneLake endpoint and block it. Some\\ntools allow you to use custom endpoints (such as PowerShell). Otherwise, it\\'s often a simple fix\\nto add OneLake\\'s endpoint as a supported endpoint. If you find a URL validation issue or have\\nany other issues connecting to OneLake, let us know.\\nOneLake is accessible through the same APIs and SDKs as ADLS. To learn more about using\\nADLS APIs, please see the following pages:\\nADLS Gen2 API Reference\\nADLS Gen2 Filesystem SDKs\\n.NET\\nPython\\nJava\\nConnect-AzAccount\\n$testToken = Get-AzAccessToken -AsSecureString -ResourceTypeName Storage\\n# Retrieved token is of string type which you can validate with the \\n\"$testToken.Token.GetTypeCode()\" command.\\n$testToken.Token | Set-Clipboard\\nData residency\\nCommon issues\\nResources'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 140, 'page_label': '141'}, page_content='Create file\\nRequest PUT https://onelake.dfs.fabric.microsoft.com/{workspace}/{item}.\\n{itemtype}/Files/sample?resource=file\\nHeaders Authorization: Bearer <userAADToken>\\nResponseResponseCode: 201 Created\\nHeaders:\\nx-ms-version : 2021-06-08\\nx-ms-request-id : 272526c7-0995-4cc4-b04a-8ea3477bc67b\\nx-ms-content-crc64 : OAJ6r0dQWP0=\\nx-ms-request-server-encrypted : true\\nETag : 0x8DA58EE365\\nBody:\\nOneLake parity and integration\\nConnect to OneLake with Python\\nOneLake integration with Azure Synapse Analytics\\nSamples\\nﾉ Expand table\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 141, 'page_label': '142'}, page_content=\"OneLake and Azure Data Lake Storage\\n(ADLS) API parity\\nArticle• 02/14/2025\\nOneLake supports the same APIs as Azure Data Lake Storage (ADLS) and Azure Blob\\nStorage, enabling users to read, write, and manage their data in OneLake with the tools\\nthey already use today. Because OneLake is a managed, logical data lake, some features\\nare managed differently than in Azure Storage, and not all behaviors are supported over\\nOneLake. This page details these differences, including OneLake managed folders, API\\ndifferences, and open source compatibility.\\nThe workspaces and data items in your Fabric tenant define the structure of OneLake.\\nManaging workspaces and items is done through Fabric experiences - OneLake doesn't\\nsupport creating, updating, or deleting workspaces or items through the ADLS APIs.\\nOneLake only allows HEAD calls at the workspace (container) level and tenant (account)\\nlevel, as you must make changes to the tenant and workspaces in the Fabric\\nadministration portal.\\nOneLake also enforces a folder structure for Fabric items, protecting items and their\\nmanaged subfolders from creation, deletion, or renaming through ADLS and Blob APIs.\\nFabric-managed folders include the top-level folder in an item (for example,\\n/MyLakehouse.lakehouse) and the first level of folders within it (for example,\\n/MyLakehouse.lakehouse/Files and /MyLakehouse.lakehouse/Tables).\\nYou can perform CRUD operations on any folder or file created within these managed\\nfolders, and perform read-only operations on workspace and item folders.\\nEven in user-created files and folders, OneLake restricts some Fabric management\\noperations through ADLS APIs. You must use Fabric experiences to update permissions\\nor edit items and workspaces, and Fabric manages other options such as access tiers.\\nOneLake accepts almost all of the same headers as Storage, ignoring only some headers\\nthat relate to unpermitted actions on OneLake. Since these headers don't alter the\\nbehavior of the entire call, OneLake ignores the banned headers, returns them in a new\\n'x-ms-rejected-headers' response header, and permits the rest of the call. For example,\\nManaged OneLake folders\\nUnsupported request headers and parameters\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 142, 'page_label': '143'}, page_content=\"OneLake ignores the 'x-ms-owner' parameter in a PUT call since Fabric and OneLake\\ndon't have the same concept of owning users as Azure Storage.\\nOneLake rejects requests containing unallowed query parameters since query\\nparameters change the behavior of the entire call. For example, UPDATE calls with the\\n'setAccessControl' parameter are blocked since OneLake never supports setting access\\ncontrol via Azure Storage APIs.\\nOneLake doesn’t allow the following behaviors and their associated request headers and\\nURI parameters:\\nSet access control\\nURI Parameter:\\naction: setAccessControl (Request rejected)\\naction: setAccessControlRecursive (Request rejected)\\nRequest headers:\\nx-ms-owner (Header ignored)\\nx-ms-group (Header ignored)\\nx-ms-permissions (Header ignored)\\nx-ms-group (Header ignored)\\nx-ms-acls (Header ignored)\\nSet encryption scope\\nRequest headers:\\nx-ms-encryption-key (Header ignored)\\nx-ms-encryption-key (Header ignored)\\nx-ms-encryption-algorithm:AES256 (Header ignored)\\nSet access tier\\nRequest headers:\\nx-ms-access-tier (Header ignored)\\nSince OneLake uses a different permission model than ADLS, response headers related\\nto permissions are handled differently:\\n'x-ms-owner' and 'x-ms-group' always returns '$superuser' as OneLake doesn't\\nhave owning users or groups\\n'x-ms-permissions' always returns '---------' as OneLake doesn't have owning\\nusers, groups, or public access permissions\\n'x-ms-acl' returns the Fabric permissions for the calling user converted to a POSIX\\naccess control list (ACL), in the form 'rwx'\\nResponse header differences\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 143, 'page_label': '144'}, page_content=\"Since OneLake supports the same APIs as ADLS and Blob Storage, many open source\\nlibraries and packages compatible with ADLS and Blob Storage work seamlessly with\\nOneLake (for example, Azure Storage Explorer). Other libraries may require small\\nupdates to accommodate OneLake endpoints or other compatibility issues. The\\nfollowing libraries are confirmed to be compatible with OneLake due to recent changes.\\nThis list isn't exhaustive:\\nDelta-RS\\nRust Object Store\\nHTTP\\nHTTP\\nHTTP\\nHTTP\\nOpen Source Integration\\nExamples\\nList items within a workspace (ADLS)\\nGET https://onelake.dfs.fabric.microsoft.com/myWorkspace?\\nresource=filesystem&recursive=false\\nList items within a workspace (Blob)\\nGET  https://onelake.blob.fabric.microsoft.com/myWorkspace?\\nrestype=container&comp=list&delimiter=%2F\\nCreate a folder within a lakehouse (ADLS)\\nPUT \\nhttps://onelake.dfs.fabric.microsoft.com/myWorkspace/myLakehouse.Lakehouse/F\\niles/newFolder/?resource=directory\\nGet blob properties (Blob)\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 144, 'page_label': '145'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nConnect to OneLake using Python\\nUse Azure Storage Explorer to manage OneLake\\nHEAD  \\nhttps://onelake.blob.fabric.microsoft.com/myWorkspace/myLakehouse.Lakehouse/\\nFiles/file.txt\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 145, 'page_label': '146'}, page_content='Use Python to manage files and folders\\nin Microsoft OneLake\\nArticle• 11/21/2023\\nThis article shows how you can use the Azure Storage Python SDK to manage files and\\ndirectories in OneLake. This walkthrough covers the same content as Use Python to\\nmanage directories and files in ADLS Gen2 and highlights the differences when\\nconnecting to OneLake.\\nBefore starting your project, make sure you have the following prerequisites:\\nA workspace in your Fabric tenant with Contributor permissions.\\nA lakehouse in the workspace. Optionally, have data preloaded to read using\\nPython.\\nFrom your project directory, install packages for the Azure Data Lake Storage and Azure\\nIdentity client libraries. OneLake supports the same SDKs as Azure Data Lake Storage\\n(ADLS) Gen2 and supports Microsoft Entra ID authentication, which is provided by the\\nazure-identity package.\\nConsole\\nNext, add the necessary import statements to your code file:\\nPython\\nPrerequisites\\nSet up your project\\npip install azure-storage-file-datalake azure-identity\\nimport os\\nfrom azure.storage.filedatalake import (\\n    DataLakeServiceClient,\\n    DataLakeDirectoryClient,\\n    FileSystemClient\\n)\\nfrom azure.identity import DefaultAzureCredential'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 146, 'page_label': '147'}, page_content='The following example creates a service client connected to OneLake that you can use to\\ncreate filesystem clients for other operations. To authenticate to OneLake, this example\\nuses the DefaultAzureCredential to automatically detect credentials and obtain the\\ncorrect authentication token. Common methods of providing credentials for the Azure\\nSDK include using the \\'az login\\' command in the Azure Command Line Interface or the\\n\\'Connect-AzAccount\\' cmdlet from Azure PowerShell.\\nPython\\nTo learn more about using DefaultAzureCredential to authorize access to data, see\\nOverview: Authenticate Python apps to Azure using the Azure SDK.\\nTo work with a directory in OneLake, create a filesystem client and directory client. You\\ncan use this directory client to perform various operations, including renaming, moving,\\nor listing paths (as seen in the following example). You can also create a directory client\\nwhen creating a directory, using the FileSystemClient.create_directory method.\\nPython\\nAuthorize access to OneLake\\ndef get_service_client_token_credential(self, account_name) -> \\nDataLakeServiceClient:\\n    account_url = f\"https://{account_name}.dfs.fabric.microsoft.com\"\\n    token_credential = DefaultAzureCredential()\\n    service_client = DataLakeServiceClient(account_url, \\ncredential=token_credential)\\n    return service_client\\nWorking with directories\\ndef create_file_system_client(self, service_client, file_system_name: str) : \\nDataLakeServiceClient) -> FileSystemClient:\\n    file_system_client = service_client.get_file_system_client(file_system = \\nfile_system_name)\\n    return file_system_client\\ndef create_directory_client(self, file_system_client : FileSystemClient, \\npath: str) -> DataLakeDirectoryClient: directory_client \\n    directory_client = file_system_client.GetDirectoryClient(path)\\n    return directory_client\\ndef list_directory_contents(self, file_system_client: FileSystemClient, \\ndirectory_name: str):'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 147, 'page_label': '148'}, page_content='You can upload content to a new or existing file by using the\\nDataLakeFileClient.upload_data method.\\nPython\\nThe following code sample lists the directory contents of any folder in OneLake.\\nPython\\n    paths = file_system_client.get_paths(path=directory_name)\\n    for path in paths:\\n        print(path.name + \\'\\\\n\\')\\nUpload a file\\ndef upload_file_to_directory(self, directory_client: \\nDataLakeDirectoryClient, local_path: str, file_name: str):\\n    file_client = directory_client.get_file_client(file_name)\\n    with open(file=os.path.join(local_path, file_name), mode=\"rb\") as data:\\n        file_client.upload_data(dataW, overwrite=True)\\nSample\\n#Install the correct packages first in the same folder as this file. \\n#pip install azure-storage-file-datalake azure-identity\\nfrom azure.storage.filedatalake import (\\n    DataLakeServiceClient,\\n    DataLakeDirectoryClient,\\n    FileSystemClient\\n)\\nfrom azure.identity import DefaultAzureCredential\\n# Set your account, workspace, and item path here\\nACCOUNT_NAME = \"onelake\"\\nWORKSPACE_NAME = \"<myWorkspace>\"\\nDATA_PATH = \"<myLakehouse>.Lakehouse/Files/<path>\"\\ndef main():\\n    #Create a service client using the default Azure credential\\n    account_url = f\"https://{ACCOUNT_NAME}.dfs.fabric.microsoft.com\"\\n    token_credential = DefaultAzureCredential()\\n    service_client = DataLakeServiceClient(account_url, \\ncredential=token_credential)'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 148, 'page_label': '149'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nTo run this sample, save the preceding code into a file listOneLakeDirectory.py and run\\nthe following command in the same directory. Remember to replace the workspace and\\npath with your own values in the example.\\nterminal\\nUse Python to manage ADLS Gen2\\nOneLake parity and integration\\nSync OneLake with your Windows File Explorer\\n    #Create a file system client for the workspace\\n    file_system_client = \\nservice_client.get_file_system_client(WORKSPACE_NAME)\\n    \\n    #List a directory within the filesystem\\n    paths = file_system_client.get_paths(path=DATA_PATH)\\n    for path in paths:\\n        print(path.name + \\'\\\\n\\')\\nif __name__ == \"__main__\":\\n    main()\\npython listOneLakeDirectory.py\\nLearn more\\n\\ue8e1 Yes \\ue8e0 No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 149, 'page_label': '150'}, page_content=\"Use Blob and ADLS APIs to mirror data into\\nOneLake\\n10/10/2025\\nIf your application uses Azure Data Lake Storage (ADLS) or Blob Storage APIs and needs to\\nconnect to OneLake, you can continue using the existing APIs.\\nWe demonstrate how Blob and ADLS APIs are used with OneLake through a real-world\\nmirroring example and share developer insights from OneLake. We explore when and why you\\nmight choose one API over another, and how to get the most out of each. All the patterns we\\ncover apply to Azure Storage storage as well.\\nIn this scenario, we cover:\\nWhat is open mirroring\\nHow to use the .NET Azure Blob Storage and Distributed File System (DFS) clients to write\\ndata into the open mirror landing zone.\\nHow to combine the Blob Storage and DFS clients for uploading data and managing\\nfolders in OneLake, especially when performance matters\\nHow to handle scenarios that crop up with block blobs when writing parquet data to blob\\nstorage from .NET\\nHow to test everything locally using the Azurite emulator. (Yes—code you write against\\nOneLake works with the storage emulator too!)\\nIn this section, we demonstrate how to efficiently stream parquet data into OneLake,\\nparticularly the open mirroring landing zone. Open mirroring is a powerful way to bring data\\nfrom proprietary systems, where shortcuts shortcuts can't be used, into Microsoft Fabric. It\\nhandles the heavy lifting, converting raw data into Delta Lake format, managing upserts,\\ndelete vectors, optimize, vacuum, and more. All you need to do is upload your data into\\nthe landing zone, include a row marker, and mirroring takes it from there.\\nIt's common that teams write custom code to extract data from proprietary systems and output\\nit in an open format. While open mirroring ingests both CSV and Parquet into Delta tables, if\\nyou’re already writing code, you might as well go with Parquet, it’s more efficient to upload\\nand process.\\nParquet is a storage file format designed for analytics. Delta, on the other hand, is a table\\nprotocol built on top of Parquet. It adds transactional guarantees, schema enforcement, and\\nsupport for updates and deletes. When you upload Parquet files to the open mirroring landing\\nStreaming parquet into OneLake with Blob APIs\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 150, 'page_label': '151'}, page_content='zone, those files are ingested into Delta tables—bringing ACID semantics and query\\nperformance optimizations without requiring you to manage those complexities yourself. The\\nrow marker indicates how each record should be merged into the table, which enables the\\nmirroring process to know when to insert, update, or delete rows.\\nLet’s walk through a concrete (but fictional) example.\\nImagine you’ve got a .NET application that pulls UK house price data from an on-premises\\nInland Revenue system. (Just to be clear, this is a made-up scenario. The UK Inland Revenue\\nisn’t doing this to my knowledge, but the dataset is publicly available and makes for a good\\nexample.) This app runs monthly as an Azure Function, and to keep costs low, it needs to be\\nfast and avoid using local disk. So, the goal is to stream the data directly from the source into\\nthe open mirroring landing zone in Parquet format. The Price Paid dataset from the UK Inland\\nRevenue includes a RecordStatus  field, which must be mapped to the row marker required by\\nthe open mirroring format.\\nThis scenario aligns well with the Blob Storage API. It supports streaming writes, allowing you\\nto push data directly into OneLake without staging it locally. That makes it simple, efficient, and\\ncost-effective, especially for serverless workloads like Azure Functions. It’s also fast: the Blob\\nAPI supports parallel block uploads, which can significantly boost throughput when writing\\nlarge files—and isn’t supported when using the DFS endpoint.\\nFor this, we’re using the open source Parquet.NET library, a fully managed .NET assembly\\nthat makes it easy to write Parquet data on the fly. Also, working with an object store like Blob\\nStorage introduces a few nuances, especially around streaming and buffering, which gives us\\nan opportunity to explore some nuances when working with blob storage.\\nThis approach keeps your Azure Function lightweight, fast, and cost-efficient, no local disk, no\\nstaging, just stream, serialize, and upload.\\nThe open mirroring landing zone acts like an inbox for your mirrored tables, add files then\\nFabric takes care of the ingestion. But behind that simplicity is a clear protocol your application\\nneeds to follow to ensure data is correctly discovered and processed.\\nEach mirrored table has a dedicated path in OneLake:\\n<workspace>/mirrored-database/Files/LandingZone/<table-name>/\\nOpen mirroring landing zone summary\\nFolder structure'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 151, 'page_label': '152'}, page_content='This is where you’ll write both metadata and data files. You don’t need to explicitly create\\nfolders, just write blobs with the appropriate prefix, and the structure is inferred.\\nBefore writing any data, you must create a _metadata.json file in the table’s landing zone\\nfolder. This file defines the key columns used for upserts and deletes:\\nC#\\nThis metadata file tells Fabric how to uniquely identify rows. Without it, Fabric won’t ingest your\\ndata.\\nOnce the metadata is in place, you can start writing data. Files must be named sequentially\\nusing zero-padded numbers like 00000000000000000001.parquet,\\n00000000000000000002.parquet, etc. This ensures deterministic ordering and avoids collisions.\\nList APIs return blobs alphabetically, so our logic can quickly find the next sequence number by\\nprocessing the landing zone folder with a flat listing. Open mirroring moves processed files to\\nfolders prefixed with _, which are sorted below numeric values. Exiting the loop after seeing all\\nparquet files improves performance when you’re using the Azure Storage List Blob API – which\\nwould enumerate blobs in sub folders as it matches the prefix. If you’re using the ADLS Path\\nList API, you can choose to perform a recursive list, which allows control over whether or not to\\nlist the contents of sub folders – this is a clear advantage of the hierarchical namespace.\\nThe logic to determine the next file name looks like this:\\nC#\\nStep 1: Declare the table keys\\npublic async Task CreateTableAsync(OpenMirroredTableId table, params string[] \\nkeyColumns)\\n{\\n    await using var metadataFile = await OpenWriteAsync(table, \"_metadata.json\");\\n    var json = new { keyColumns };\\n    await JsonSerializer.SerializeAsync(metadataFile, json);\\n}\\nStep 2: Create data files with the correct name\\npublic async Task<MirrorDataFile> CreateNextTableDataFileAsync(OpenMirroredTableId \\ntable)\\n{\\n    var (containerClient, path) = GetTableLocation(table);\\n    var listBlobs = containerClient.GetBlobsAsync(prefix: path);\\n    var tableFound = false;'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 152, 'page_label': '153'}, page_content='You might be wondering what the MirrorDataFile class is for, we’ll come back to that shortly\\nwhen we cover how to work reliably with block blobs.\\nThe actual write is handled by opening a stream to the blob:\\nC#\\n    BlobItem? lastDataFile = null;\\n    // Parquet files will be first in the folder because other files and folders \\nall start with an underscore.\\n    // So we can just take the last Parquet file to get our sequence number.\\n    await foreach (var blob in listBlobs)\\n    {\\n        tableFound = true;\\n        if (blob.Name.EndsWith(\".parquet\") && blob.Properties.ContentLength > 0)\\n        {\\n            lastDataFile = blob;\\n        }\\n        else\\n        {\\n            break;\\n        }\\n    }\\n    if (!tableFound)\\n    {\\n        throw new ArgumentException($\"Table not found.\", nameof(table));\\n    }\\n    long lastFileNumber = 0;\\n    \\n    if (lastDataFile is not null)\\n    {\\n        var dataFileName = Path.GetFileName(lastDataFile.Name);\\n        lastFileNumber = long.Parse(dataFileName.Split(\\'.\\')[0]);\\n    }\\n    \\n    var stream = await OpenWriteAsync(table, $\"{++lastFileNumber:D20}.parquet\");\\n    return new MirrorDataFile(stream)\\n    {\\n        FileSequenceNumber = lastFileNumber\\n    };\\n}\\nStep 3: Upload the contents of the file\\nprivate async Task<Stream> OpenWriteAsync(OpenMirroredTableId table, string \\nfileName)\\n{\\n    var (containerClient, path) = GetTableLocation(table);\\n    path += fileName;\\n    var blobClient = containerClient.GetBlobClient(path);'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 153, 'page_label': '154'}, page_content='C#\\nThe business logic that reads Price Paid data from the Inland Revenue system converts each\\nrecord into Parquet format as it is streamed from the source. Each row is written directly into a\\nParquet file, which is simultaneously streamed byte-by-byte into Azure Storage using the\\nstream returned by CreateNextTableDataFileAsync. This approach avoids local staging and\\nsupports efficient, serverless ingestion.\\nHere’s how the code works:\\nC#\\n    var stream = await blobClient.OpenWriteAsync(true);\\n    return stream;\\n}\\n７ Note\\nMirrored databases support schemas, so the landing zone can contain an optional\\nschema folder to denote that the table is within a schema. Also, in OneLake, Fabric\\nworkspaces are mapped to storage Containers and ADLS Filesystems.\\npublic record OpenMirroredTableId(string WorkspaceName, string \\nMirroredDatabaseName, string TableName)\\n{\\n    public string? Schema { get; init; } = null;\\n    public string GetTablePath() => Schema == null\\n            ? $\"{MirroredDatabaseName}/Files/LandingZone/{TableName}/\"\\n            : $\"\\n{MirroredDatabaseName}/Files/LandingZone/{Schema}.schema/{TableName}/\";\\n}\\nprivate (BlobContainerClient ContainerClient, string TablePath) \\nGetTableLocation(OpenMirroredTableId table)\\n{\\n    var containerClient = \\nblobServiceClient.GetBlobContainerClient(table.WorkspaceName);\\n    var path = table.GetTablePath();\\n    \\n    return (containerClient, path);\\n}\\npublic async Task SeedMirrorAsync(OpenMirroredTableId tableId, CancellationToken \\ncancellationToken = default)\\n{\\n    await openMirroringWriter.CreateTableAsync(tableId,'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 154, 'page_label': '155'}, page_content='The WriteAsync method serializes the data into Parquet format, row group by row group:\\nC#\\nEach PricePaid record is transformed into a PricePaidMirroredDataFormat object, which\\nincludes the required __rowMarker__ field:\\nC#\\nPricePaidMirroredDataFormat.KeyColumns);\\n    var data = pricePaidDataReader.ReadCompleteData(cancellationToken);\\n    await using var mirrorDataFile = await \\nopenMirror.CreateNextTableDataFileAsync(tableId);\\n    await mirrorDataFile.WriteData(async stream => await data.WriteAsync(stream, \\nSettings.RowsPerRowGroup, cancellationToken));\\n}\\npublic static async Task WriteAsync(this IAsyncEnumerable<PricePaid> data, Stream \\nresultStream, int rowsPerRowGroup = 10000, CancellationToken cancellationToken = \\ndefault)\\n{\\n    await using var parquetWriter = await ParquetWriter.CreateAsync(\\n        PricePaidMirroredDataFormat.CreateSchema(), \\n        resultStream, \\n        cancellationToken: cancellationToken);\\n    await foreach (var chunk in data\\n        .Select(PricePaidMirroredDataFormat.Create)\\n        .ChunkAsync(rowsPerRowGroup, cancellationToken))\\n    {\\n        await ParquetSerializer.SerializeRowGroupAsync(parquetWriter, chunk, \\ncancellationToken);\\n    }\\n}\\npublic static PricePaidMirroredDataFormat Create(PricePaid pricePaid)\\n{\\n    var recordMarker = pricePaid.RecordStatus.Value switch\\n    {\\n        RecordStatus.Added => 0,\\n        RecordStatus.Changed => 1,\\n        RecordStatus.Deleted => 2,\\n        _ => throw new InvalidEnumArgumentException(\"Unexpected RecordStatus \\nvalue\")\\n    };\\n    return new PricePaidMirroredDataFormat\\n    {\\n        TransactionId = pricePaid.TransactionId,\\n        Price = pricePaid.Price,\\n        ...'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 155, 'page_label': '156'}, page_content='This protocol is simple and deterministic. It avoids unnecessary API calls, works seamlessly with\\nboth batch and streaming pipelines, and integrates smoothly with serverless environments like\\nAzure Functions. Uploading a blob to a prefix automatically creates the parent folders and\\nremains compatible with the storage emulator—more on that in the testing section.\\nOnce open mirroring has successfully processed your data, it moves the original files into\\nspecial folders, _ProcessedFiles and _FilesReadyToDelete, and adds a\\n_FilesReadyToDelete.json file. While Fabric will automatically delete these files after seven\\ndays, that retention window can lead to significant storage costs if you\\'re mirroring large\\nvolumes of data.\\nTo reduce costs, you can proactively delete these folders once you\\'re confident the data has\\nbeen ingested. This is a great use case for the ADLS API, which supports atomic directory\\ndeletion—far more efficient than enumerating and deleting individual blobs and updating the\\n_FilesReadyToDelete.json file.\\nHere’s how to do it:\\nC#\\n        __rowMarker__ = recordMarker\\n    };\\n}\\nStep 4: Clean up after yourself\\npublic async Task CleanUpTableAsync(OpenMirroredTableId tableId)\\n{\\n    var (fileSystemClient, tablePath) = GetTableLocation(tableId);\\n    var foldersToDelete = new []\\n    {\\n        \"_ProcessedFiles\",\\n        \"_FilesReadyToDelete\"\\n    };\\n    await Parallel.ForEachAsync(foldersToDelete, async (path, _) =>\\n    {\\n        var fullPath = $\"{tablePath}{path}/\";\\n        var directoryClient = fileSystemClient.GetDirectoryClient(fullPath);\\n        if (await directoryClient.ExistsAsync())\\n        {\\n            await directoryClient.DeleteAsync();\\n        }\\n    });\\n}\\nprivate (DataLakeFileSystemClient FileSystemClient, string TablePath) \\nGetTableLocation(OpenMirroredTableId table)\\n{'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 156, 'page_label': '157'}, page_content='I chose this example because it opens the door to talk about some nuances with block blobs.\\nThese aren\\'t OneLake specific, but because OneLake is built on Azure Storage, OneLake\\nexposes these nuances too.\\nOne such case: the .NET Storage SDK exposes an OpenWrite API that returns a Stream. Super\\nhandy. As shown in the example above, that stream fits nicely with the Parquet.NET APIs. It also\\nmakes testing a breeze—you can easily substitute the stream in unit tests without needing to\\nbuild extra abstractions just for testability.\\nC#\\n    var containerClient = \\ndataLakeServiceClient.GetFileSystemClient(table.WorkspaceName);\\n    var path = $\"\\n{table.MirroredDatabaseName}/Files/LandingZone/{table.TableName}/\";\\n    return (containerClient, path);\\n}\\nReliably working with block blobs\\n[Test]\\npublic async Task when_writing_price_paid_data_to_parquet()\\n{\\n    var row = new PricePaid\\n    {\\n        TransactionId = \"{34222872-B554-4D2B-E063-4704A8C07853}\",\\n        Price = 375000,\\n        DateOfTransfer = new DateTime(2004, 4, 27),\\n        Postcode = \"SW13 0NP\",\\n        PropertyType = PropertyType.Detached,\\n        IsNew = true,\\n        DurationType = DurationType.Freehold,\\n        PrimaryAddressableObjectName = \"10A\",\\n        SecondaryAddressableObjectName = string.Empty,\\n        Street = \"THE TERRACE\",\\n        Locality = string.Empty,\\n        TownCity = \"LONDON\",\\n        District = \"RICHMOND UPON THAMES\",\\n        County = \"GREATER LONDON\",\\n        CategoryType = CategoryType.AdditionalPricePaid,\\n        RecordStatus = RecordStatus.Added\\n    };\\n    using var memoryStream = new MemoryStream();\\n    await new[] { row }.ToAsyncEnumerable().WriteAsync(memoryStream);\\n    var readData = await \\nPricePaidMirroredDataFormat.Read(memoryStream).SingleAsync();\\n    Assert.Multiple(() =>'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 157, 'page_label': '158'}, page_content=\"But here’s the catch: Blob Storage isn’t the same as a local file system.\\nTo understand why the Flush() call in Parquet.NET matters, we need to take a quick detour\\ninto how Parquet files are structured—and how that interacts with the Blob Storage block blob\\nAPI.\\nParquet is a columnar storage format designed for efficient analytics. A Parquet file is made up\\nof:\\nRow groups: These are the core building blocks. Each row group contains a chunk of\\nrows, organized by column. Row groups are written sequentially and independently.\\nColumn chunks: Within each row group, data is stored column-by-column.\\nMetadata: At the end of the file, Parquet writes a footer that includes schema and offset\\ninformation for fast reads. In Parquet.NET, each time a row group is completed, the library\\ncalls Flush() on the output stream. This is where things get interesting when you're\\nwriting to Blob Storage.\\nBlob Storage, and therefore OneLake, uses a block blob model, which works like this:\\nYou upload blocks: Each block can be up to 4,000 MiB in size (default is 4 MiB). You can\\nupload blocks in parallel to maximize throughput—and the .NET Blob client does this for\\nyou automatically, which is great. This is one reason to use the Blob client (not the DFS\\nclient) for uploads. The DFS client doesn’t support parallel block uploads, which can be a\\nperformance bottleneck. That said, the DFS client can still read the resulting files just fine.\\nYou commit the blocks: Once all blocks are uploaded, you call CommitBlockList() to\\nfinalize the blob. You can upload up to 50,000 blocks per blob, which means—if you’re\\nusing 4,000-MiB blocks—you could theoretically write a single 190.7-TiB file. (Not that I’d\\n    {\\n        Assert.That(readData.TransactionId, Is.EqualTo(row.TransactionId));\\n        Assert.That(readData.Price, Is.EqualTo(row.Price));\\n        Assert.That(readData.DateOfTransfer, Is.EqualTo(row.DateOfTransfer));\\n        // more assertions\\n        Assert.That(readData.__rowMarker__, Is.EqualTo(0)); // RecordStatus.Added\\n    });\\n}\\nCalling flush – commits the blocks\\nParquet file basics\\nBlock blob model\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 158, 'page_label': '159'}, page_content=\"recommend it.) If you’d like to learn more, read this article Understanding block blobs,\\nappend blobs, and page blobs.\\nHere's the catch\\nWhen Parquet.NET calls Flush() after each row group, and you're writing to a stream backed\\nby Blob Storage, that flush triggers a BlockList commit. As the file grows, each new row group\\ncauses the library to recommit all previously uploaded blocks.\\nLet’s say you’re writing a 1-GB file with 100MB row groups, using 4MB blocks. That gives you\\n250 blocks in total. On the first flush, you commit 25 blocks. On the second, 50. By the final\\nflush, you’re committing all 250 blocks. Add that up across all 10 row groups, and you’ve\\ncommitted a total of 1,375 blocks—even though the final file only needs one commit.\\nThis is inefficient, and it introduces two key problems:\\nTimeouts and retries. Large BlockLists can lead to timeouts. Remember, Blob Storage is\\nbuilt on HTTP. It’s reliable—but not perfect. A timeout might succeed on the server, but\\nyour client doesn’t know that, so it retries. That retry can result in a 409 Conflict. Doing\\nsomething expensive 250 times instead of once increases the chance of this happening.\\nFewer commits = fewer retries = fewer headaches.\\nPremature blob visibility. One of the nice things about the Blob API is that the blocks\\ndon't become visible until you explicitly commit the BlockList. This aligns well for\\nscenarios like Parquet, where the file isn’t valid until the footer metadata is written. It\\nmeans downstream processes won’t accidentally pick up a half-written file. But here’s the\\ntwist: because Parquet.NET calls Flush() after each row group, and that flush triggers a\\ncommit, the blob becomes visible before the file is complete. So even though the Blob\\nAPI is designed to help you avoid this problem, the way Parquet.NET works with a Stream\\nintroduces an issue—unless you take steps to prevent it.\\nTo bridge the gap between Parquet.NET and the nuances of Blob Storage, don't implement the\\nFlush() call in the BlobFile.BlobStream implementation. That way, even though Parquet.NET\\ncalls Flush after each row group, the underlying stream doesn’t flush to storage.\\nThe storage clients (DFS and Blob) will create an empty file when calling OpenWrite. The open\\nmirroring processor logs an error when encountering a 0-byte file, which is possible if the\\nreplicator process interleaves with file creation. To avoid this write a file to another location and\\nmove it to the correct path, which the ADLS APIs supports through a rename operation, like so:\\nC#\\npublic async Task<BlobFile> CreateFileAsync(string filePath)\\n{\\n    if (filePath is null)\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 159, 'page_label': '160'}, page_content='Here’s another subtle but important behavior to be aware of: when you use the Azure Blob\\nstream, calling .Dispose() (or letting it be disposed implicitly) will commit all uploaded blocks.\\nThat’s usually what you want—but not always.\\nLet’s say your source system is streaming data using an IAsyncEnumerable, as it does in the\\nexample to illustrate the bug. If that source fails partway through, for example, the database\\nconnection times out or the network drops, you might only have a partially written Parquet file.\\nBut if the stream gets disposed (which it will, due to await using or a using block), those partial\\nblocks get committed.\\nTo avoid this, the example code explicitly commits once only when the entire operation\\ncompletes successfully. That way, if the producer fails mid-stream, you’re not left with a half-\\nwritten file.\\nThis is why the example returns a BlobFile object from CreateNextTableDataFileAsync, to\\nencapsulate the stream to prevent the zero-byte file, and issues described with Flush and\\nDispose committing partial parquet files.\\nC#\\n    {\\n        throw new ArgumentNullException(nameof(filePath), \"File path cannot be \\nnull.\");\\n    }\\n    var containerClient = \\nclient.blobServiceClient.GetBlobContainerClient(containerName);\\n    var temporaryPath = $\"_{filePath}.temp\"!;\\n    var blobClient = containerClient.GetBlobClient(Combine(temporaryPath));\\n    var blobStream = await blobClient.OpenWriteAsync(overwrite: true);\\n    return new BlobFile(blobStream, GetChildPath(temporaryPath), \\nCombine(filePath)!);\\n}\\nCalling dispose – commits the blocks and move the file\\npublic class BlobFile(Stream stream, IStoragePath temporaryFilePath, string \\nfinalFilePath) : IAsyncDisposable\\n{\\n    private readonly BlobStream stream = new(stream, temporaryFilePath, \\nfinalFilePath);\\n    \\n    public async Task WriteData(Func<Stream, Task> writeOperation)\\n    {\\n        try\\n        {\\n            await writeOperation(stream);\\n        }'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 160, 'page_label': '161'}, page_content='catch (Exception)\\n        {\\n            stream.Failed();\\n            throw;\\n        }\\n    }\\n    \\n    public async ValueTask DisposeAsync()\\n    {\\n        await stream.DisposeAsync();\\n    }\\n    \\n    private class BlobStream(Stream innerStream, IStoragePath temporaryFilePath, \\nstring finalFilePath) : Stream\\n    {\\n        private bool disposed = false;\\n        private bool success = true;\\n        public override bool CanRead => innerStream.CanRead;\\n        public override bool CanSeek => innerStream.CanSeek;\\n        public override bool CanWrite => innerStream.CanWrite;\\n        public override long Length => innerStream.Length;\\n        public override long Position\\n        {\\n            get => innerStream.Position;\\n            set => innerStream.Position = value;\\n        }\\n        public override void Flush()\\n        {\\n            // no-op\\n        }\\n        public override int Read(byte[] buffer, int offset, int count) => \\ninnerStream.Read(buffer, offset, count);\\n        public override long Seek(long offset, SeekOrigin origin) => \\ninnerStream.Seek(offset, origin);\\n        public override void SetLength(long value) => \\ninnerStream.SetLength(value);\\n        public override void Write(byte[] buffer, int offset, int count) => \\ninnerStream.Write(buffer, offset, count);\\n        \\n        public void Failed()\\n        {\\n            success = false;\\n        }\\n        public override async ValueTask DisposeAsync()\\n        {\\n            if (disposed)'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 161, 'page_label': '162'}, page_content=\"One of the great things about OneLake is that it’s built on Azure Storage—which means you\\ncan test your integration code locally using the Azurite emulator. This makes it easy to write\\nreliable tests, that emulate the behavior of OneLake / Azure Storage, without needing a live\\nFabric environment or cloud resources.\\nAzurite emulates the Blob Storage API, which is exactly what OneLake exposes. That means the\\nsame code you use in production can run unchanged in your test suite. You can spin up Azurite\\nas a local process or container, point your BlobServiceClient at it, and go.\\nThis is especially useful for unit and integration tests. You can:\\nValidate that your _metadata.json is written correctly.\\nCheck that your file naming logic produces the expected sequence.\\nSimulate partial writes resulting from calling Flush and Dispose on failure. This allows\\ntesting of the nuances described above and that the implementation handles them\\nappropriately.\\nAssert that your Parquet serialization round-trips cleanly.\\nAzurite doesn't support the ADFS APIs. This is why BlobFile above uses an IStoragePath\\ninterface to implement ADFS functionality using the blob APIs so they can work with the\\nemulator under testing. Here’s an example:\\nC#\\n            {\\n                return;\\n            }\\n            if (success)\\n            {\\n                await innerStream.DisposeAsync();\\n                await temporaryFilePath.RenameAsync(finalFilePath);\\n            }\\n            disposed = true;\\n        }\\n    }\\n}\\nWriting tests with OneLake using the Azurite\\nemulator\\npublic async Task RenameAsync(string newPath)\\n{\\n    async Task RenameDirectoryAsync(DataLakeServiceClient dataLakeServiceClient)\\n    {\\n        var fileSystemClient =\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 162, 'page_label': '163'}, page_content='Earlier, we talked about how Parquet.NET’s use of Flush() and Dispose() can lead to\\npremature or partial blob commits when writing to OneLake. These behaviors are subtle—but\\ntestable.\\nHere are a few tests to validate that the mirroring logic handles these scenarios correctly:\\nC#\\ndataLakeServiceClient.GetFileSystemClient(containerName);\\n        var directoryClient = fileSystemClient.GetDirectoryClient(path);\\n        await directoryClient.RenameAsync(newPath);\\n    }\\n    async Task CopyThenDeleteAsync(BlobServiceClient blobServiceClient)\\n    {\\n        var sourceBlob = \\nblobServiceClient.GetBlobContainerClient(containerName).GetBlobClient(path);\\n        var destinationBlob = \\nblobServiceClient.GetBlobContainerClient(containerName).GetBlobClient(newPath);\\n        \\n        await destinationBlob.StartCopyFromUriAsync(sourceBlob.Uri);\\n        await sourceBlob.DeleteIfExistsAsync();\\n    }\\n    StorageOperation operation = new()\\n    {\\n        WithFlatNamespace = CopyThenDeleteAsync,\\n        WithHierarchicalNamespace = RenameDirectoryAsync\\n    };\\n    await operation.Execute(client);\\n}\\nTesting corner cases: flush and dispose\\n[Test]\\npublic async Task it_should_write_data_to_table()\\n{\\n    await setup.FabricPricePaidMirror.SeedMirrorAsync(setup.TableId);\\n    var mirroredData = await GetMirroredBlobItem();\\n    var mirroredDataClient = \\nsetup.WorkspaceContainer.GetBlobClient(mirroredData!.Name);\\n    var mirroredDataContents = await mirroredDataClient.DownloadContentAsync();\\n    var readData = await \\nPricePaidMirroredDataFormat.Read(mirroredDataContents.Value.Content.ToStream()).Si\\nngleAsync();\\n    Assert.Multiple(() =>\\n    {\\n        Assert.That(mirroredData, Is.Not.Null);'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 163, 'page_label': '164'}, page_content='This test confirms that a successful write results in a valid, nonempty Parquet file that round-\\ntrips correctly.\\nNow for the failure cases:\\nC#\\nThis test simulates a mid-stream exception and verifies that no partial data is visible while the\\nwrite is in progress because of the no-op Flush() override.\\nAnd finally, the failure path:\\nC#\\n        Assert.That(mirroredData!.Properties.ContentLength, Is.GreaterThan(0));\\n        Assert.That(readData.TransactionId, \\nIs.EqualTo(setup.PricePaidReader.TransactionId));\\n        // ... more assertions ...\\n    });\\n}\\n[Test]\\npublic async Task it_should_not_commit_partially_written_data()\\n{\\n    long? lengthDuringWrite = null;\\n    setup.PricePaidReader.ActionBetweenRowGroups = async () =>\\n    {\\n        var mirroredData = await GetMirroredBlobItem();\\n        lengthDuringWrite = mirroredData!.Properties.ContentLength;\\n    };\\n    await setup.FabricPricePaidMirror.SeedMirrorAsync(setup.TableId);\\n    Assert.That(lengthDuringWrite, Is.EqualTo(0));\\n}\\n[Test]\\npublic async Task \\nafter_a_row_group_is_written_it_should_not_leave_a_partially_complete_blob()\\n{\\n    setup.PricePaidReader.ThrowsAfterFirstRowGroup = true;\\n    var previouslyMirroredFile = await GetMirroredBlobItem();\\n    var threw = true;\\n    try\\n    {\\n        await setup.FabricPricePaidMirror.SeedMirrorAsync(setup.TableId);\\n    }\\n    catch (Exception)\\n    {\\n        threw = true;'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 164, 'page_label': '165'}, page_content=\"This test confirms that even if the stream is disposed due to an exception a new mirrored file\\nisn't partially written.\\nThese tests provide confidence that the mirroring logic is robust, even under failure conditions.\\nThey demonstrate how the emulator can be used to simulate real-world behavior without\\nneeding a full Fabric (or Azure) environment.\\nAnd just to prove compatibility, tweaking the test setup to point at a Fabric workspace, here’s a\\ntable full of UK house price data.\\nC#\\n    }\\n    var mirroredData = await GetMirroredBlobItem();\\n    var mirroredTemporaryData = await GetMirroredBlobTemporaryItem();\\n    Assert.Multiple(() =>\\n    {\\n        Assert.That(threw, Is.True);\\n        if (previouslyMirroredFile == null)\\n        { \\n            Assert.That(mirroredData, Is.Null);\\n        }\\n        else\\n        {\\n            Assert.That(mirroredData!.Name, \\nIs.EqualTo(previouslyMirroredFile.Name));\\n        }\\n        Assert.That(mirroredTemporaryData, Is.Not.Null);\\n        Assert.That(mirroredTemporaryData!.Properties.ContentLength, \\nIs.EqualTo(0));\\n    });\\n}\\npublic class when_using_fabric\\n{\\n    public class in_success_cases : when_copying_to_mirror_successfully\\n    {\\n        [SetUp]\\n        public void UseFabric() => setup = TestSetup.UsingFabric();\\n    }\\n    public class in_failure_cases : when_copying_to_mirror_fails\\n    {\\n        [SetUp]\\n        public void UseFabric() => setup = TestSetup.UsingFabric();\\n    }\\n}\\npublic static TestSetup UsingFabric()\\n{\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 165, 'page_label': '166'}, page_content='If you’re building new pipelines on OneLake, especially for streaming or serverless workloads,\\nthe ADLS and Blob storage APIs work as expected. They\\'re fast, flexible, and work seamlessly\\nwith open mirroring. By following the landing zone protocol and handling block blob and file\\nsystem differences, you can build robust, testable integrations that work just as well in\\nproduction as they do in your local emulator. And best of all—you don’t need to rewrite your\\napp or fight the filesystem. Just stream, serialize, and upload to OneLake.\\n    var blobServiceClient = new BlobServiceClient(new \\nUri(\"https://onelake.blob.fabric.microsoft.com/\"), new DefaultAzureCredential());\\n    var pricePaidReader = new TestPricePaidReader();\\n    var tableId = new OpenMirroredTableId($\"TestWorkspace\", \\n\"HousePriceOpenMirror.MountedRelationalDatabase\", \"PricePaid\");\\n    var workspaceContainer = \\nblobServiceClient.GetBlobContainerClient(tableId.WorkspaceName);\\n    var fabricPricePaidMirror = new FabricPricePaidMirror(new \\nFabricOpenMirroringWriter(blobServiceClient), pricePaidReader)\\n    {\\n        Settings = new FabricPricePaidMirrorSettings { RowsPerRowGroup = 1 }\\n    };\\n    return new TestSetup\\n    {\\n        BlobServiceClient = blobServiceClient,\\n        PricePaidReader = pricePaidReader,\\n        TableId = tableId,\\n        WorkspaceContainer = workspaceContainer,\\n        FabricPricePaidMirror = fabricPricePaidMirror\\n    };\\n}\\n\\uf80a\\nWrapping up'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 166, 'page_label': '167'}, page_content='Integrate OneLake with Azure Synapse\\nAnalytics\\nArticle• 11/29/2023\\nAzure Synapse is a limitless analytics service that brings together enterprise data\\nwarehousing and Big Data analytics. This tutorial shows how to connect to OneLake\\nusing Azure Synapse Analytics.\\nFollow these steps to use Apache Spark to write sample data to OneLake from Azure\\nSynapse Analytics.\\n1. Open your Synapse workspace and create an Apache Spark pool with your\\npreferred parameters.\\n2. Create a new Apache Spark notebook.\\n3. Open the notebook, set the language to PySpark (Python), and connect it to your\\nnewly created Spark pool.\\n4. In a separate tab, navigate to your Microsoft Fabric lakehouse and find the top-\\nlevel Tables folder.\\n5. Right-click on the Tables folder and select Properties.\\nWrite data from Synapse using Apache Spark'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 167, 'page_label': '168'}, page_content=\"6. Copy the ABFS path from the properties pane.\\n7. Back in the Azure Synapse notebook, in the first new code cell, provide the\\nlakehouse path. This lakehouse is where your data is written later. Run the cell.\\nPython\\n8. In a new code cell, load data from an Azure open dataset into a dataframe. This\\ndataset is the one you load into your lakehouse. Run the cell.\\nPython\\n# Replace the path below with the ABFS path to your lakehouse Tables \\nfolder. \\noneLakePath = \\n'abfss://WorkspaceName@onelake.dfs.fabric.microsoft.com/LakehouseName.l\\nakehouse/Tables'\\nyellowTaxiDf = \\nspark.read.parquet('wasbs://nyctlc@azureopendatastorage.blob.core.windo\\nws.net/yellow/puYear=2018/puMonth=2/*.parquet')\\ndisplay(yellowTaxiDf.limit(10))\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 168, 'page_label': '169'}, page_content='9. In a new code cell, filter, transform, or prep your data. For this scenario, you can\\ntrim down your dataset for faster loading, join with other datasets, or filter down to\\nspecific results. Run the cell.\\nPython\\n10. In a new code cell, using your OneLake path, write your filtered dataframe to a new\\nDelta-Parquet table in your Fabric lakehouse. Run the cell.\\nPython\\n11. Finally, in a new code cell, test that your data was successfully written by reading\\nyour newly loaded file from OneLake. Run the cell.\\nPython\\nCongratulations. You can now read and write data in OneLake using Apache Spark in\\nAzure Synapse Analytics.\\nFollow these steps to use SQL serverless to read data from OneLake from Azure Synapse\\nAnalytics.\\n1. Open a Fabric lakehouse and identify a table that you\\'d like to query from Synapse.\\n2. Right-click on the table and select Properties.\\n3. Copy the ABFS path for the table.\\nfilteredTaxiDf = \\nyellowTaxiDf.where(yellowTaxiDf.tripDistance>2).where(yellowTaxiDf.pass\\nengerCount==1)\\ndisplay(filteredTaxiDf.limit(10))\\nfilteredTaxiDf.write.format(\"delta\").mode(\"overwrite\").save(oneLakePath \\n+ \\'/Taxi/\\')\\nlakehouseRead = spark.read.format(\\'delta\\').load(oneLakePath + \\'/Taxi/\\')\\ndisplay(lakehouseRead.limit(10))\\nRead data from Synapse using SQL'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 169, 'page_label': '170'}, page_content=\"Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\n4. Open your Synapse workspace in Synapse Studio .\\n5. Create a new SQL script.\\n6. In the SQL query editor, enter the following query, replacing ABFS_PATH_HERE with\\nthe path you copied earlier.\\nSQL\\n7. Run the query to view the top 10 rows of your table.\\nCongratulations. You can now read data from OneLake using SQL serverless in Azure\\nSynapse Analytics.\\nIntegrate OneLake with Azure Storage Explorer\\nSELECT TOP 10 *\\nFROM OPENROWSET(\\nBULK 'ABFS_PATH_HERE',\\nFORMAT = 'delta') as rows;\\nRelated content\\n\\ue8e1 Yes \\ue8e0 No\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 170, 'page_label': '171'}, page_content='Integrate OneLake with Azure Storage\\nExplorer\\nArticle• 11/29/2023\\nThis article demonstrates OneLake integration with Azure Storage Explorer. Azure\\nStorage Explorer allows you to view and manage your cloud storage account’s contents.\\nYou can upload, download, or move files from one location to another.\\n1. Install the latest version of Azure Storage Explorer from the product webpage.\\n2. Check to ensure the version installed is 1.29.0 or higher. (Check the version by\\nselecting Help > About.)\\n3. Select the Open connect dialog icon.\\n4. Azure Storage Explorer requires you to sign in to connect to Azure resources.\\nSelect Subscription and follow the instructions to sign in.\\nConnect and use Azure Storage Explorer\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 171, 'page_label': '172'}, page_content='5. Connect to OneLake by selecting the Open connect dialog icon again and select\\nADLS Gen2 container or directory.\\n6. Enter URL details of the workspace or item you would like to connect to, in this\\nformat: https://onelake.dfs.fabric.microsoft.com/{workspace-\\nName}/{itemName.itemType}/. You can find the workspace name and item name in\\nthe Properties pane of a file in the Microsoft Fabric portal.\\n\\uf80a \\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 172, 'page_label': '173'}, page_content='You can choose a Display name for convenience, then select Next.\\n7. Storage Explorer browses to the location of the OneLake you entered.\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 173, 'page_label': '174'}, page_content='8. To view the contents, select the OneLake folder you connected.\\n9. Select Upload. In the Select files to upload dialog box, select the files that you\\nwant to upload.\\n\\uf80a \\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 174, 'page_label': '175'}, page_content='10. To download, select the folders or files that you want to download and then select\\nDownload.\\n11. To copy data across locations, select the folders you want to copy and select Copy,\\nthen navigate to the destination location and select Paste.\\nIf a workspace name has capital letters, deletion of files or folders fails due to a\\nrestriction from the storage service. We recommend using your workspace name in\\nlowercase letters.\\n\\uf80a \\n\\uf80a \\nLimitations'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 175, 'page_label': '176'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nIntegrate OneLake with Azure Databricks\\nRelated content\\n\\ue8e1 Yes \\ue8e0 No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 176, 'page_label': '177'}, page_content=\"Integrate OneLake with Azure Databricks\\n10/10/2025\\nThis scenario shows how to connect to OneLake via Azure Databricks. After completing this\\ntutorial, you'll be able to read and write to a Microsoft Fabric lakehouse from your Azure\\nDatabricks workspace.\\nBefore you connect, you must have:\\nA Fabric workspace and lakehouse.\\nA premium Azure Databricks workspace. Only premium Azure Databricks workspaces\\nsupport Microsoft Entra credential passthrough, which you need for this scenario.\\n1. Open your Azure Databricks workspace and select Create > Cluster.\\n2. To authenticate to OneLake with your Microsoft Entra identity, you must enable Azure\\nData Lake Storage (ADLS) credential passthrough on your cluster in the Advanced\\nOptions.\\nPrerequisites\\nSet up your Databricks workspace\\n７  Note\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 177, 'page_label': '178'}, page_content='3. Create the cluster with your preferred parameters. For more information on creating a\\nDatabricks cluster, see Configure clusters - Azure Databricks.\\n4. Open a notebook and connect it to your newly created cluster.\\n1. Navigate to your Fabric lakehouse and copy the Azure Blob Filesystem (ABFS) path to\\nyour lakehouse. You can find it in the Properties pane.\\n2. Save the path to your lakehouse in your Databricks notebook. This lakehouse is where\\nyou write your processed data later:\\nPython\\n3. Load data from a Databricks public dataset into a dataframe. You can also read a file from\\nelsewhere in Fabric or choose a file from another ADLS Gen2 account you already own.\\nPython\\n4. Filter, transform, or prep your data. For this scenario, you can trim down your dataset for\\nfaster loading, join with other datasets, or filter down to specific results.\\nPython\\nYou can also connect Databricks to OneLake using a service principal. For more\\ninformation about authenticating Azure Databricks using a service principal, see\\nManage service principals.\\nAuthor your notebook\\n７  Note\\nAzure Databricks only supports the Azure Blob Filesystem (ABFS) driver when\\nreading and writing to ADLS Gen2 and OneLake:\\nabfss://myWorkspace@onelake.dfs.fabric.microsoft.com/.\\noneLakePath = \\n\\'abfss://myWorkspace@onelake.dfs.fabric.microsoft.com/myLakehouse.lakehouse/F\\niles/\\'\\nyellowTaxiDF = spark.read.format(\"csv\").option(\"header\", \\n\"true\").option(\"inferSchema\", \"true\").load(\"/databricks-\\ndatasets/nyctaxi/tripdata/yellow/yellow_tripdata_2019-12.csv.gz\")'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 178, 'page_label': '179'}, page_content='5. Write your filtered dataframe to your Fabric lakehouse using your OneLake path.\\nPython\\n6. Test that your data was successfully written by reading your newly loaded file.\\nPython\\nThis completes the setup and now you can now read and write data in Fabric using Azure\\nDatabricks.\\n[Databricks serverless compute]/azure/databricks/compute/serverless/) allows you to run\\nworkloads without provisioning a cluster. As per Databricks serverless limitations, to automate\\nthe configuration of Spark on serverless compute, Databricks doesn\\'t allow configuring Spark\\nproperties outside supported properties that are listed here.\\nIf you attempt to modify or set an unsupported Spark configuration in a notebook linked to\\nDatabricks serverless compute, the system returns a CONFIG_NOT_AVAILABLE error.\\nfilteredTaxiDF = \\nyellowTaxiDF.where(yellowTaxiDF.fare_amount<4).where(yellowTaxiDF.passenger_c\\nount==4)\\ndisplay(filteredTaxiDF)\\nfilteredTaxiDF.write.format(\"csv\").option(\"header\", \\n\"true\").mode(\"overwrite\").csv(oneLakePath)\\nlakehouseRead = spark.read.format(\\'csv\\').option(\"header\", \\n\"true\").load(oneLakePath)\\ndisplay(lakehouseRead.limit(10))\\nConnecting to OneLake using Databricks serverless\\ncompute\\n７  Note\\nThis limitation isn\\'t unique to Azure Databricks. Databricks Serverless implementations on\\nAmazon Web Services (AWS)  and Google Cloud exhibit the same behavior.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 179, 'page_label': '180'}, page_content=\"OneLake supports inbound connectivity from Databricks serverless compute. You can connect\\nto OneLake as provided you have successfully authenticated and there's network path between\\nDatabricks serverless compute and OneLake. With Databricks serverless, you must ensure that\\nyour code doesn't modify any unsupported Spark properties.\\nBefore you connect, you must have:\\nA Fabric workspace and lakehouse.\\nA premium Azure Databricks workspace.\\nA service principal with a minimum of Contributor workspace role assignment.\\nDatabase secrets or Azure Key Vault (AKV) to store and retrieve secrets. This example uses\\nDatabricks secrets.\\n1. Create a notebook in Databricks workspace and attach it to serverless compute.\\n2. Import Python modules - in this sample, you're using three modules:\\nmsal is Microsoft Authentication Library (MSAL) and it is designed to help\\ndevelopers integrate Microsoft identity platform authentication into their\\napplications.\\nrequests module is used to make HTTP requests using Python.\\ndelta lake is used to read and write Delta Lake tables using Python.\\nPython\\nPrerequisites\\nAuthor your notebook\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 180, 'page_label': '181'}, page_content='3. Declare variables for Microsoft Entra tenant including application ID. Use the tenant ID of\\nthe tenant where Microsoft Fabric is deployed.\\nPython\\n4. Declare Fabric workspace variables.\\nPython\\n5. Initialize client to acquire token.\\nPython\\nfrom msal import ConfidentialClientApplication\\nimport requests\\nfrom deltalake import DeltaTable\\n# Fetch from Databricks secrets.\\ntenant_id = dbutils.secrets.get(scope=\"<replace-scope-name>\",key=\"<replace \\nvalue with key value for tenant _id>\")\\nclient_id = dbutils.secrets.get(scope=\"<replace-scope-name>\",key=\"<replace \\nvalue with key value for client _id>\") \\nclient_secret = dbutils.secrets.get(scope=\"<replace-scope-name>\",key=\"\\n<replace value with key value for secret>\")\\nworkspace_id = \"<replace with workspace name>\"\\nlakehouse_id = \"<replace with lakehouse name>\"\\ntable_to_read = \"<name of lakehouse table to read>\"\\nstorage_account_name = workspace_id\\nonelake_uri = \\nf\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}.lake\\nhouse/Tables/{table_to_read}\"\\nauthority = f\"https://login.microsoftonline.com/{tenant_id}\"\\napp = ConfidentialClientApplication(\\n client_id,\\n authority=authority,\\n client_credential=client_secret\\n )\\n result = app.acquire_token_for_client(scopes=\\n[\"https://onelake.fabric.microsoft.com/.default\"])\\n if \"access_token\" in result:\\n   access_token = result[\"access_token\"]\\n   print(\"Access token acquired.\")\\n   token_val = result[\\'access_token\\']'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 181, 'page_label': '182'}, page_content='6. Read a delta table from OneLake\\nPython\\nThis completes the setup and you can now read data from OneLake using Databricks a\\nnotebook attached to serverless compute.\\nIntegrate OneLake with Azure HDInsight\\ndt = DeltaTable(onelake_uri, storage_options={\"bearer_token\": f\"{token_val}\", \\n\"use_fabric_endpoint\": \"true\"})\\ndf = dt.to_pandas()\\nprint(df.head())\\n７  Note\\nThe service principal has Contributor workspace role assignment and you can use it\\nto write data back to OneLake.\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 182, 'page_label': '183'}, page_content='Integrate Databricks Unity Catalog with\\nOneLake\\nArticle• 04/09/2024\\nThis scenario shows how to integrate Unity Catalog external Delta tables to OneLake\\nusing shortcuts. After completing this tutorial, you’ll be able to automatically sync your\\nUnity Catalog external Delta tables to a Microsoft Fabric lakehouse.\\nBefore you connect, you must have:\\nA Fabric workspace.\\nA Fabric lakehouse in your workspace.\\nExternal Unity Catalog Delta tables created within your Azure Databricks\\nworkspace.\\nFirst, examine which storage locations in Azure Data Lake Storage Gen2 (ADLS Gen2)\\nyour Unity Catalog tables are using. This Cloud storage connection is used by OneLake\\nshortcuts. To create a Cloud connection to the appropriate Unity Catalog storage\\nlocation:\\n1. Create a Cloud storage connection used by your Unity Catalog tables. See how to\\nset up a ADLS Gen2 connection.\\n2. Once you create the connection, obtain the connection ID by selecting Settings \\n> Manage connections and gateways > Connections > Settings.\\nPrerequisites\\nSet up your Cloud storage connection'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 183, 'page_label': '184'}, page_content='Once the Cloud connection ID is obtained, integrate Unity Catalog tables to Fabric\\nlakehouse as follows:\\n７ Note\\nGranting users direct storage level access to external location storage in ADLS Gen2\\ndoes not honor any permissions granted or audits maintained by Unity Catalog.\\nDirect access will bypass auditing, lineage, and other security/monitoring features\\nof Unity Catalog including access control and permissions. You are responsible for\\nmanaging direct storage access through ADLS Gen2 and ensuring that users have\\nthe appropriate permissions granted via Fabric. Avoid all scenarios granting direct\\nstorage level write access for buckets storing Databricks managed tables.\\nModifying, deleting, or evolving any objects directly through storage which were\\noriginally managed by Unity Catalog can result in data corruption.\\nRun the notebook'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 184, 'page_label': '185'}, page_content='1. Import sync notebook to your Fabric workspace. This notebook exports all Unity\\nCatalog tables metadata from a given catalog and schemas in your metastore.\\n2. Configure the parameters in the first cell of the notebook to integrate Unity\\nCatalog tables. The Databricks API, authenticated through PAT token, is utilized for\\nexporting Unity Catalog tables. The following snippet is used to configure the\\nsource (Unity Catalog) and destination (OneLake) parameters. Ensure to replace\\nthem with your own values.\\nPython\\n3. Run all cells of the notebook to start synchronizing Unity Catalog Delta tables to\\nOneLake using shortcuts. Once notebook is completed, shortcuts to Unity Catalog\\nDelta tables are available in the lakehouse, SQL endpoint, and semantic model.\\nIf you want to execute the notebook at regular intervals to integrate Unity Catalog Delta\\ntables into OneLake without manual resync / rerun, you can either schedule the\\nnotebook or utilize a notebook activity in a data pipeline within Fabric Data Factory.\\n# Databricks workspace\\ndbx_workspace = \"<databricks_workspace_url>\"\\ndbx_token = \"<pat_token>\"\\n# Unity Catalog\\ndbx_uc_catalog = \"catalog1\"\\ndbx_uc_schemas = \\'[\"schema1\", \"schema2\"]\\'\\n# Fabric\\nfab_workspace_id = \"<workspace_id>\"\\nfab_lakehouse_id = \"<lakehouse_id>\"\\nfab_shortcut_connection_id = \"<connection_id>\"\\n# If True, UC table renames and deletes will be considered\\nfab_consider_dbx_uc_table_changes = True\\nSchedule the notebook'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 185, 'page_label': '186'}, page_content='Feedback\\nIn the latter scenario, if you intend to pass parameters from the data pipeline, designate\\nthe first cell of the notebook as a toggle parameter cell and provide the appropriate\\nparameters in the pipeline.\\nFor production scenarios, we recommend using Databricks OAuth for\\nauthentication and Azure Key Vault to manage secrets. For instance, you can use\\nthe MSSparkUtils credentials utilities to access Key Vault secrets.\\nThe notebook works with Unity Catalog external Delta tables. If you’re using\\nmultiple Cloud storage locations for your Unity Catalog tables, i.e. more than one\\nADLS Gen2, the recommendation is to run the notebook separately by each Cloud\\nconnection.\\nUnity Catalog managed Delta tables, views, materialized views, streaming tables\\nand non-Delta tables are not supported.\\nChanges to Unity Catalog table schemas like add / delete columns are reflected\\nautomatically in the shortcuts. However, some updates like Unity Catalog table\\nrename and deletion require a notebook resync / rerun. This is considered by\\nfab_consider_dbx_uc_table_changes parameter.\\nFor writing scenarios, using the same storage layer across different compute\\nengines can result in unintended consequences. Be sure to grasp the implications\\nwhen using different Spark compute engines and runtime versions.\\nIntegrate OneLake with Azure Databricks\\nOneLake shortcuts\\n\\uf80a\\nOther considerations\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 186, 'page_label': '187'}, page_content='Was this page helpful?\\nProvide product feedback | Ask the community\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 187, 'page_label': '188'}, page_content='Write Iceberg tables from Snowflake to\\nOneLake (Preview)\\nYou can configure a new or existing Snowflake database to automatically store Iceberg tables\\nin Microsoft OneLake. This feature creates a new data item in Microsoft Fabric, and Iceberg\\ntables that you create in Snowflake are stored there by default. With this capability, both\\nMicrosoft Fabric and Snowflake can work with a single copy of Iceberg data without duplication\\nor movement.\\nThis article shows you how to:\\nAllow connectivity between Snowflake and Fabric\\nConfigure a new Snowflake database to write Iceberg tables to OneLake by default\\n1. Because this feature is in a Preview state, you first need to enable this setting at the\\ntenant or capacity level.\\nYour tenant admin can enable the setting tenant-wide using the \"Enable Snowflake\\ndatabase item (preview)\" setting seen in the Admin portal.\\nAlternatively, your capacity admin can enable this delegated tenant setting in the\\ncapacity settings area.\\n2. Select (or create) a Fabric workspace for the Snowflake database item.\\nTo keep things simple, use alphanumeric characters only for your workspace name.\\nIf workspace name has special characters, copy the workspace ID from the browser\\nURL seen when the workspace is open.\\nYou must have the Admin or Member role to proceed with this feature.\\n3. Find your Fabric tenant ID (a GUID) and your Snowflake account identifier.\\n） Important\\nThis feature is currently in Public Preview. Behavior may change before General\\nAvailability.\\nOverview\\nPrerequisites'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 188, 'page_label': '189'}, page_content=\"You can find your Fabric tenant ID by selecting your profile in the top-right corner of\\nthe Fabric UI and hovering over the tenant information.\\nYou can find your Snowflake account identifier in the lower-left corner of the\\nSnowflake UI and selecting Account details.\\n4. Identify the Snowflake warehouse that should be used when Fabric communicates with\\nSnowflake (for example, COMPUTE_WH).\\n5. Pick a strong password to assign to the new Snowflake user for Fabric to use when\\ncommunicating with Snowflake.\\nThis article assumes the connection to Snowflake will use a user account with a\\npassword. You may use the KeyPair authentication method alternatively.\\nIn your Snowflake account, log in with a user that has an administrative role.\\n1. Run the following SQL statements in Snowflake:\\nSQL\\n2. Go to Ingestion > Add data.\\n3. Select Microsoft OneLake.\\n4. Enter your Fabric tenant ID (a GUID).\\n5. If prompted, click Provide consent.\\nCreate least-privileged user and role in Snowflake\\n-- Use a privileged role\\nUSE ROLE ACCOUNTADMIN;\\n-- Create least-privilege role (if not exists)\\nCREATE ROLE IF NOT EXISTS R_ICEBERG_METADATA;\\n-- Create service user (adjust LOGIN_NAME / PASSWORD as needed)\\nCREATE USER IF NOT EXISTS SVC_FABRIC_ICEBERG_METADATA\\n  TYPE = LEGACY_SERVICE\\n  LOGIN_NAME = 'SVC_FABRIC_ICEBERG_METADATA'\\n  DISPLAY_NAME = 'Service - Fabric Iceberg Metadata'\\n  PASSWORD = '<STRONG_PASSWORD_HERE>'\\n  MUST_CHANGE_PASSWORD = FALSE\\n  DEFAULT_ROLE = R_ICEBERG_METADATA;\\n-- Grant role to user\\nGRANT ROLE R_ICEBERG_METADATA TO USER SVC_FABRIC_ICEBERG_METADATA;\\n-- Allow role to use an existing warehouse (adjust COMPUTE_WH as needed)\\nGRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE R_ICEBERG_METADATA;\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 189, 'page_label': '190'}, page_content='If prompted for permissions, review and accept (may require Entra admin). You may\\nclose the popup once complete.\\n6. Copy the multitenant app name displayed (used to write Iceberg tables to OneLake).\\n7. Keep this browser tab open.\\nIn a new browser tab:\\n1. Open the Fabric web UI.\\n2. Go to Settings (top right) > Manage connections and gateways.\\n3. Select + New.\\n4. Choose Cloud connection. Enter the following details:\\nConnection type: Snowflake\\nServer: https://<accountIDpart1>-<accountIDpart2>.snowflakecomputing.com\\nWarehouse: COMPUTE_WH (or your choice)\\nAuthentication method: Snowflake\\nUsername: SVC_FABRIC_ICEBERG_METADATA (unless customized)\\nPassword: Your chosen password\\n5. Create the connection. If it fails, recheck the information from the previous section.\\n6. Copy the connection ID (GUID) from the connection page.\\n7. Filter the list of connections to see your new connection.\\n8. On that connection, select Manage users.\\nPrepare connection in Fabric'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 190, 'page_label': '191'}, page_content='9. Grant access to the Snowflake multitenant app, then select Share.\\n10. Navigate to your Fabric workspace.\\n11. Select Manage access, Add people or groups.\\n12. Paste the multitenant app name. Grant at least Contributor role to ensure the app can be\\nused by Snowflake to create the data item and write data.\\n13. Keep this browser tab open.\\n７ Note\\nIf you granted consent in the previous section, it may take a few minutes for the\\nmultitenant app to appear in your search.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 191, 'page_label': '192'}, page_content='Continue the setup flow in Snowflake:\\n1. Enter the Fabric workspace name (if alphanumeric) or workspace ID.\\n2. Enter the Fabric connection ID that you copied in the previous section.\\n3. Provide a new Snowflake database name (for example, SnowflakeFabricIcebergDB).\\nIf you choose to have the item name in Fabric differ, keep track of that item name.\\n4. Confirm that you see the message \"Fabric item and database successfully created.\"\\n5. Continue, and acknowledge the configuration when prompted.\\nIf you enter a custom external volume name, keep track of that name.\\n6. Select Create volume, then View database.\\nConfirm you see the new database.\\n7. Run the following SQL statements:\\nReplace SnowflakeFabricIcebergDB with your database name.\\nSQL\\nEstablish connectivity in Snowflake\\n-- Grant Iceberg metadata role permissions on the new database\\nBEGIN\\n  LET db STRING := \\'SnowflakeFabricIcebergDB\\';\\n  EXECUTE IMMEDIATE \\'GRANT USAGE ON DATABASE \\' || db || \\' TO ROLE \\nR_ICEBERG_METADATA\\';\\n  EXECUTE IMMEDIATE \\'GRANT MONITOR ON DATABASE \\' || db || \\' TO ROLE \\nR_ICEBERG_METADATA\\';\\n  EXECUTE IMMEDIATE \\'GRANT USAGE ON ALL SCHEMAS IN DATABASE \\' || db || \\' TO \\nROLE R_ICEBERG_METADATA\\';\\n  EXECUTE IMMEDIATE \\'GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \\' || db || \\' TO \\nROLE R_ICEBERG_METADATA\\';\\n  EXECUTE IMMEDIATE \\'GRANT SELECT ON ALL ICEBERG TABLES IN DATABASE \\' || db || \\n\\' TO ROLE R_ICEBERG_METADATA\\';\\n  EXECUTE IMMEDIATE \\'GRANT SELECT ON FUTURE ICEBERG TABLES IN DATABASE \\' || db \\n|| \\' TO ROLE R_ICEBERG_METADATA\\';\\nEND;\\nGRANT USAGE ON EXTERNAL VOLUME SnowflakeFabricIcebergDB TO ROLE \\nR_ICEBERG_METADATA;\\nGRANT OWNERSHIP ON EXTERNAL VOLUME SnowflakeFabricIcebergDB TO ROLE \\nR_ICEBERG_METADATA COPY CURRENT GRANTS;'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 192, 'page_label': '193'}, page_content=\"You're done with setup! Proceed to create an Iceberg table and read it in Fabric.\\nIn Snowflake:\\n1. Run the following SQL statements to create an Iceberg table and populate it with data:\\nSQL\\n2. Run the following SQL statement to read the Iceberg table in Snowflake:\\nSQL\\nIn the Fabric web UI:\\n1. Navigate to your workspace.\\n2. Locate the Snowflake database item. Refresh if needed.\\n3. Open it to see the Iceberg table from Part 4.\\n4. Select SQL analytics endpoint (top right) to query this table using SQL:\\nSQL\\nCreate an Iceberg table in Snowflake\\n-- Create a sample Iceberg table\\nCREATE ICEBERG TABLE SnowflakeFabricIcebergDB.PUBLIC.SampleIcebergTable (\\n  id   INT,\\n  name STRING\\n)\\nCATALOG = 'SNOWFLAKE';\\n-- Insert sample rows\\nINSERT INTO SnowflakeFabricIcebergDB.PUBLIC.SampleIcebergTable VALUES\\n  (1, 'Alpha'),\\n  (2, 'Beta'),\\n  (3, 'Gamma');\\n-- Display all rows in the Iceberg table\\nSELECT * FROM SnowflakeFabricIcebergDB.PUBLIC.SampleIcebergTable;\\nRead the Iceberg table in Fabric\\n-- Display all rows in the Iceberg table\\nSELECT * FROM [SnowflakeFabricIcebergDB].[PUBLIC].[SampleIcebergTable];\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 193, 'page_label': '194'}, page_content='You should see the same data displayed in both Snowflake and Fabric.\\nThis data resides in OneLake, and both Fabric and Snowflake can work with the same copy of\\ndata, no data movement or duplication required!\\nLast updated on 11/18/2025'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 194, 'page_label': '195'}, page_content='Integrate OneLake with Azure HDInsight\\nArticle• 06/05/2024\\nAzure HDInsight is a managed cloud-based service for big data analytics that helps\\norganizations process large amounts data. This tutorial shows how to connect to\\nOneLake with a Jupyter notebook from an Azure HDInsight cluster.\\nTo connect to OneLake with a Jupyter notebook from an HDInsight cluster:\\n1. Create an HDInsight (HDI) Apache Spark cluster. Follow these instructions: Set up\\nclusters in HDInsight.\\na. While providing cluster information, remember your Cluster login Username\\nand Password, as you need them to access the cluster later.\\nb. Create a user assigned managed identity (UAMI): Create for Azure HDInsight -\\nUAMI and choose it as the identity in the Storage screen.\\n2. Give this UAMI access to the Fabric workspace that contains your items. For help\\ndeciding what role is best, see Workspace roles.\\nUsing Azure HDInsight\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 195, 'page_label': '196'}, page_content='3. Navigate to your lakehouse and find the name for your workspace and lakehouse.\\nYou can find them in the URL of your lakehouse or the Properties pane for a file.\\n4. In the Azure portal, look for your cluster and select the notebook.\\n5. Enter the credential information you provided while creating the cluster.\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 196, 'page_label': '197'}, page_content='Feedback\\n6. Create a new Apache Spark notebook.\\n7. Copy the workspace and lakehouse names into your notebook and build the\\nOneLake URL for your lakehouse. Now you can read any file from this file path.\\nPython\\n8. Try writing some data into the lakehouse.\\nPython\\n9. Test that your data was successfully written by checking your lakehouse or by\\nreading your newly loaded file.\\nYou can now read and write data in OneLake using your Jupyter notebook in an HDI\\nSpark cluster.\\nOneLake security\\n\\uf80a\\nfp = \\'abfss://\\' + \\'Workspace Name\\' + \\n\\'@onelake.dfs.fabric.microsoft.com/\\' + \\'Lakehouse Name\\' + \\'/Files/\\' \\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").load(fp + \\n\"test1.csv\") \\ndf.show()\\nwritecsvdf = df.write.format(\"csv\").save(fp + \"out.csv\") \\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 197, 'page_label': '198'}, page_content='Was this page helpful?\\nProvide product feedback | Ask the community\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 198, 'page_label': '199'}, page_content=\"Use OneLake files in Microsoft Foundry\\nUse Microsoft OneLake as a knowledge source for Microsoft Foundry. You can connect directly\\nand securely to OneLake from Foundry, index unstructured and semi structured files stored in\\nOneLake (including files that arrive through shortcuts), and then use that indexed content as a\\nknowledge source inside agents in Foundry.\\nWith this integration, you can ground your agents on the same enterprise data that already\\nlives in OneLake, instead of creating new copies of files in separate AI specific stores.\\nPermissions and governance are enforced through the same OneLake and Fabric controls that\\nyou use for analytics workloads.\\nA lakehouse in Fabric. If you don't have a lakehouse, follow the steps in Create a\\nlakehouse with OneLake.\\nFiles in the Files folder of the lakehouse.\\nA Foundry project. If you don't have one, follow the steps in Create a project.\\nAn Azure AI Search service at the Basic tier or higher. If you don't have one, follow the\\nsteps in Create an Azure AI Search service.\\nThe search service must be in the same tenant as your Fabric workspace.\\nIn this article, you create an assign a managed identity for the search service. To create\\na managed identity, you must be an Owner or User Access Administrator roles. To\\nassign roles, you must be an Owner, User Access Administrator, Role-based Access\\nControl Administrator, or a member of a custom role with\\nMicrosoft.Authorization/roleAssignments/write permissions.\\nUse Azure AI Search to configure a OneLake files indexer to make your lakehouse data\\nsearchable as a knowledge source.\\nReview the prerequisites in Index data from OneLake files and shortcuts > Prerequisites.\\nThen, follow the steps for system managed identity in Index data from OneLake files and\\nshortcuts > Grant permissions.\\nPrerequisites\\nIndex data from OneLake files\\nCreate a OneLake connection in Foundry\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 199, 'page_label': '200'}, page_content='1. Sign in to Microsoft Foundry.\\nMake sure the New Foundry toggle is On. The steps in this article refer to Microsoft\\nFoundry (new).\\n2. Open the project that you want to work in.\\n3. Select Build from the navigation menu, then select Knowledge from the left pane.\\n4. Select your AI Search resource.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 200, 'page_label': '201'}, page_content='5. Select Create a knowledge base.\\n6. Select Microsoft OneLake as the knowledge type. Select Connect.\\n7. Provide your Fabric workspace ID and lakehouse ID.\\nYou can retrieve both of these IDs from your lakehouse URL:\\nhttps://app.powerbi.com/groups/<WORKSPACE_ID>/lakehouses/<LAKEHOUSE_ID>.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 201, 'page_label': '202'}, page_content='8. Select Create.\\n9. Select Save knowledge base.\\nLast updated on 11/18/2025'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 202, 'page_label': '203'}, page_content=\"Manage OneLake with PowerShell\\nMicrosoft OneLake integrates with the Azure PowerShell module for data reading, writing, and\\nmanagement.\\nConnect to OneLake from PowerShell by following these steps:\\n1. Install the Azure Storage PowerShell module.\\nPowerShell\\n2. Sign in to your Azure account.\\nPowerShell\\n3. Create the storage account context.\\nStorage account name is onelake.\\nSet -UseConnectedAccount to passthrough your Azure credentials.\\nSet -endpoint as fabric.microsoft.com.\\n4. Run the same commands used for Azure Data Lake Storage (ADLS) Gen2. For more\\ninformation about ADLS Gen2 and the Azure Storage PowerShell module, see Use\\nPowerShell to manage ADLS Gen2.\\nPowerShell\\nConnect to OneLake with Azure PowerShell\\nInstall-Module Az.Storage -Repository PSGallery -Force\\nConnect-AzAccount\\nExample: Get the size of an item or directory\\nInstall-Module Az.Storage -Repository PSGallery -Force\\nConnect-AzAccount\\n$ctx = New-AzStorageContext -StorageAccountName 'onelake' -UseConnectedAccount -\\nendpoint 'fabric.microsoft.com' \\n# This example uses the workspace and item name. If the workspace name does not meet \\nAzure Storage naming criteria (no special characters), you can use GUIDs instead.\\n$workspaceName = 'myworkspace'\\n$itemPath = 'mylakehouse.lakehouse/Files'\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 203, 'page_label': '204'}, page_content='Integrate OneLake with Azure Synapse Analytics\\nLast updated on 11/18/2025\\n# Recursively get the length of all files within your lakehouse, sum, and convert to \\nGB.\\n$colitems = Get-AzDataLakeGen2ChildItem -Context $ctx -FileSystem $workspaceName -\\nPath $itemPath -Recurse -FetchProperty | Measure-Object -property Length -sum\\n\"Total file size: \" + ($colitems.sum / 1GB) + \" GB\"\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 204, 'page_label': '205'}, page_content='AzCopy\\n08/28/2025\\nAzCopy is a powerful command-line utility designed to facilitate the transfer of data between\\nAzure Storage accounts. Because Microsoft OneLake supports the same APIs, SDKs, and tools\\nas Azure Storage, you can also use AzCopy to load data to and from OneLake. This article helps\\nyou use AzCopy with OneLake, from copying data between artifacts to uploading or\\ndownloading data.\\nAzCopy is optimized for data plane operations at scale and large scale data movement. When\\nyou copy data between storage accounts (including OneLake), data moves directly from\\nstorage server to storage server, minimizing performance bottlenecks. AzCopy is also easy-to-\\nuse and reliable, with built-in mechanisms to handle network interruptions and retries. With\\nAzCopy, it\\'s easy to upload data to OneLake, or load data from existing sources directly into\\nyour items in Fabric!\\nTrusted workspace access lets you access firewall-enabled Azure Storage accounts securely by\\nconfiguring a resource instance rule on an Azure Storage account. This rule lets your specific\\nFabric workspace access the storage account\\'s firewall from select Fabric experiences, like\\nshortcuts, pipelines, and AzCopy. By configuring trusted workspace access, AzCopy can copy\\ndata from a firewall-enabled Azure Storage account into OneLake without affecting the firewall\\nprotections. Learn more at trusted workspace access.\\nIf you\\'re new to AzCopy, you can learn how to download and get started with AzCopy at Get\\nstarted with AzCopy.\\nWhen you use AzCopy with OneLake, there\\'s a few key points to remember:\\n1. Add \"fabric.microsoft.com\" as a trusted domain using the--trusted-microsoft-suffixes\\nparameter.\\n2. Select the subscription of your source Azure Storage account when logging in with your\\nMicrosoft Entra ID, as OneLake only cares about the tenant.\\n3. Use double quotes when using AzCopy in the command prompt, and single quotes when\\nin PowerShell.\\nWhy use AzCopy and OneLake?\\nTrusted workspace access and AzCopy\\nGetting Started'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 205, 'page_label': '206'}, page_content='The samples in this article also assume that your Microsoft Entra ID has appropriate\\npermissions to access both the source and destinations.\\nFinally, you need at least one source and destination for your data movement - the samples in\\nthis page use two Fabric lakehouses and one ADLS account.\\nUse this sample to copy a file from a lakehouse in one workspace to a different workspace by\\nusing the azcopy copy command. Remember to authenticate first by running azcopy login\\nfirst.\\nSyntax\\nAzCopy\\nThe copy operation is synchronous so when the command returns, all files are copied.\\nA shared access signature (SAS) provides short-term, delegated access to Azure Storage and\\nOneLake, and is a great option to provide tools or users temporary access to storage for one-\\ntime upload or downloads. A SAS is also a great option if the Azure Storage account is in a\\ndifferent tenant than your OneLake, as Entra authorization will not work if the tenants are\\ndifferent.\\nThis sample uses a unique SAS token to authenticate to both Azure Storage and OneLake. To\\nlearn more about generating and using SAS tokens with Azure Storage and OneLake, check out\\nthe following pages:\\nHow to create a OneLake shared access signature (SAS)\\nGrant limited access to Azure storage resources using shared access signatures (SAS)\\nSample: Copying data between Fabric workspaces\\nazcopy copy \"https://onelake.dfs.fabric.microsoft.com/<source-workspace-\\nname>/<source-item-name>/Files/<source-file-path>\" \\n\"https://onelake.dfs.fabric.microsoft.com/<destination-workspace-\\nname>/<destination-item-name>/Files/<destination-file-path>\" --trusted-microsoft-\\nsuffixes \"fabric.microsoft.com\" \\nSample: Copying data from ADLS to OneLake with\\na shared access signatures (SAS)'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 206, 'page_label': '207'}, page_content='AzCopy\\nSince OneLake is a managed data lake, some operations aren\\'t supported with AzCopy. For\\nexample, you can\\'t use AzCopy to move or copy entire items or workspaces. Instead, create the\\nnew item in your destination location using a Fabric experience (like the portal), and then use\\nAzCopy to move the contents of the existing item into the new item.\\nWhen attempting to perform operations directly between two Fabric tenants, you must use\\nexternal data sharing. This means you cannot currently use AzCopy to directly load data\\nbetween two Fabric tenants, as that results in a direct cross-tenant operation. Other methods\\nto load data, such as downloading the data locally or to a Spark cluster and then re-uploading\\nthe data to the new tenant, will function.\\nCopy blobs between Azure Storage accounts by using AzCopy\\n７  Note\\nWhen using a SAS token to authenticate to OneLake in AzCopy, you must set the \\'``-s2s-\\npreserve-access-tier\\' parameter to false.\\nazcopy copy \"https://<account-name>.blob.core.windows.net/<source-container-\\nname>/<source-file-path>?<blob-sas-token>\" \\n\"https://onelake.dfs.fabric.microsoft.com/<destination-workspace-\\nname>/<destination-item-name>/Files/<destination-file-path>?<onelake-sas-token>\" -\\n-trusted-microsoft-suffixes \"fabric.microsoft.com\" --s2s-preserve-access-\\ntier=false\\nLimitations\\nCross-tenant operations\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 207, 'page_label': '208'}, page_content='Overview of OneLake table APIs\\n10/27/2025\\nOneLake offers a REST API endpoint for interacting with tables in Microsoft Fabric. This\\nendpoint can be used with clients and libraries that are compatible with the Iceberg REST\\nCatalog (IRC) API open standard or the Unity Catalog API open standard..\\nUsing these APIs is straightforward once you identify a few pieces of information and select\\nyour preferred Microsoft Entra ID authentication flow.\\nTo use these APIs, you first need to gather the following pieces of information:\\nYour Fabric tenant ID.\\nThe tenant ID is a GUID, and it can be found in the Profile card or the **Help, About\\nFabric menu in Fabric.\\nThe workspace and data item ID of the data item (such as a lakehouse) with a top-level\\nTables directory.\\nThese IDs are GUIDs. They can be found within the OneLake URL of any table in OneLake.\\nThey can alternatively be found within the URL seen in your browser when you have a\\ndata item open in Fabric.\\nThe user or service principal identity in Microsoft Entra ID that has permissions to read\\ntables in your chosen data item.\\n1. Decide how you would like to authenticate with Microsoft Entra ID to obtain an access\\ntoken for your chosen Microsoft Entra identity.\\n） Important\\nThis feature is in preview.\\nPrerequisites\\nGathering basic information\\nPreparing for authentication'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 208, 'page_label': '209'}, page_content='You can check this guide to learn about the different ways to obtain an access token with\\nMicrosoft Entra ID. Microsoft offers convenient authentication libraries in several\\nlanguages.\\n2. If you are developing a new application that will either allow users to sign in or sign in as\\na standalone application, register your application with Microsoft Entra ID.\\n3. Grant API permission for the Azure Storage (https://storage.azure.com/) token audience,\\nto your Microsoft Entra ID application. Granting this permission ensures that your\\napplication can obtain tokens for use with the OneLake table endpoint.\\nLearn how to get started with the OneLake table API endpoint to interact with Iceberg tables in\\nOneLake. Initially, read-only metadata table operations are supported, and we plan to add\\nmore operations soon.\\nLearn how to get started with the OneLake table API endpoint to interact with Delta tables in\\nOneLake.\\n７ Note\\nThe OneLake table API endpoint accepts the same token audience as the OneLake\\nfilesystem endpoints.\\nIf you are developing an application, you might already know how to authenticate\\nwith Microsoft Entra ID to interact with OneLake filesystem REST APIs. If so, you can\\nuse the same approach to authenticate with the new OneLake table endpoint.\\nIceberg REST Catalog (IRC) API operations on\\nOneLake\\n７ Note\\nBefore using the Iceberg APIs, be sure you have Delta Lake to Iceberg metadata\\nconversion enabled for your tenant or workspace. Review the instructions to learn how to\\nenable automatic Delta Lake to Iceberg table format conversion.\\nDelta Lake REST API operations on OneLake\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 209, 'page_label': '210'}, page_content='Learn more about OneLake table APIs for Iceberg.\\nLearn more about OneLake table APIs for Delta.\\nSet up automatic Delta Lake to Iceberg format conversion.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 210, 'page_label': '211'}, page_content='OneLake table APIs for Iceberg\\n10/27/2025\\nOneLake offers a REST API endpoint for interacting with tables in Microsoft Fabric. This article\\ndescribes how to get started using this endpoint to interact with Apache Iceberg REST Catalog\\n(IRC) APIs available at this endpoint for metadata read operations.\\nFor overall OneLake table API guidance and prerequisite guidance, see the OneLake table API\\noverview.\\nFor detailed API documentation, see the Getting started guide.\\nThe OneLake table API endpoint is:\\nAt the OneLake table API endpoint, the Iceberg REST Catalog (IRC) APIs are available under the\\nfollowing <BaseUrl>. You can generally provide this path when initializing existing IRC clients or\\nlibraries.\\nExamples of IRC client configuration with the OneLake table endpoint are covered in the\\nGetting started guide.\\n） Important\\nThis feature is in preview.\\nIceberg table API endpoint\\nhttps://onelake.table.fabric.microsoft.com\\nhttps://onelake.table.fabric.microsoft.com/iceberg\\n７ Note\\nBefore using the Iceberg APIs, be sure you have Delta Lake to Iceberg metadata\\nconversion enabled for your tenant or workspace. See the instructions to learn how to\\nenable automatic Delta Lake to Iceberg metadata conversion.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 211, 'page_label': '212'}, page_content=\"The following IRC operations are currently supported at this endpoint. Detailed guidance for\\nthese operations is available in the Getting started guide.\\nGet configuration\\nGET <BaseUrl>/v1/config?warehouse=<Warehouse>\\nThis operation accepts the workspace ID and data item ID (or their equivalent friendly\\nnames if they don’t contain any special characters). <Warehouse> is typically\\n<WorkspaceID>/<dataItemID>.\\nThis operation returns the Prefix string that is used in subsequent requests.\\nList namespaces\\nGET <BaseUrl>/v1/<Prefix>/namespaces\\nThis operation returns the list of schemas within a data item. If the data item doesn't\\nsupport schemas, a fixed schema named dbo is returned.\\nGet namespace\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>\\nThis operation returns information about a schema within a data item, if the schema is\\nfound. If the data item doesn't support schemas, a fixed schema named dbo is supported\\nhere.\\nList tables\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>/tables\\nThis operation returns the list of tables found within a given schema.\\nGet table\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>/tables/<TableName>\\nThis operation returns metadata details for a table within a schema, if the table is found.\\nIceberg table API operations\\nCurrent limitations, considerations\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 212, 'page_label': '213'}, page_content=\"The use of the OneLake table APIs for Iceberg is subject to the following limitations and\\nconsiderations:\\nCertain data items may not support schemas\\nDepending on the type of data item you use, such as non-schema-enabled Fabric\\nlakehouses, there may not be schemas within the Tables directory. In such cases, for\\ncompatibility with API clients, the OneLake table APIs provide a default, fixed dbo schema\\n(or namespace) to contain all tables within a data item.\\nCurrent namespace scope\\nIn Fabric, data items contain a flat list of schemas, which each contains a flat list of tables.\\nToday, the top-level namespaces listed by the Iceberg APIs are schemas, so although the\\nIceberg REST Catalog (IRC) standard supports multi-level namespaces, the OneLake\\nimplementation offers one level, mapping to schemas.\\nBecause of this limitation, we don't yet support the parent query parameter for the list\\nnamespaces operation.\\nMetadata write operations, other operations\\nOnly the operations listed in Iceberg table API operations are supported today.\\nOperations that handle metadata write operations aren't yet supported by the OneLake\\ntable API endpoint. We plan to add support for more operations at a later time.\\nLearn more about OneLake table APIs.\\nSee detailed guidance and API details.\\nSet up automatic Delta Lake to Iceberg format conversion.\\nRelated content\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 213, 'page_label': '214'}, page_content='Getting started with OneLake table APIs for\\nIceberg\\n10/27/2025\\nOneLake offers a REST API endpoint for interacting with tables in Microsoft Fabric. This\\nendpoint supports read-only metadata operations for Apache Iceberg tables in Fabric. These\\noperations are compatible with the Iceberg REST Catalog (IRC) API open standard.\\nLearn more about OneLake table APIs for Iceberg and make sure to review the prerequisite\\ninformation.\\nReview these samples to learn how to set up existing Iceberg REST Catalog (IRC) clients or\\nlibraries for use with the new OneLake table endpoint.\\nUse the following sample Python code to configure PyIceberg to use the OneLake table API\\nendpoint. Then, list schemas and tables within a data item.\\nThis code assumes there is a default AzureCredential available for a currently signed-in user.\\nAlternatively, you can use the MSAL Python library to obtain a token.\\nPython\\n）  Important\\nThis feature is in preview.\\nPrerequisites\\nClient quickstart examples\\nPyIceberg\\nfrom pyiceberg.catalog import load_catalog\\nfrom azure.identity import DefaultAzureCredential\\n# Iceberg base URL at the OneLake table API endpoint\\ntable_api_url = \"https://onelake.table.fabric.microsoft.com/iceberg\"\\n# Entra ID token\\ncredential = DefaultAzureCredential()\\ntoken = credential.get_token(\"https://storage.azure.com/.default\").token'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 214, 'page_label': '215'}, page_content='Use the following sample code to create a new catalog-linked database in Snowflake. This\\ndatabase will automatically include any schemas and tables found within the connected Fabric\\ndata item. This involves the creation of a catalog integration, an external volume, and a\\ndatabase.\\nSQL\\n# Client configuration options\\nfabric_workspace_id = \"12345678-abcd-4fbd-9e50-3937d8eb1915\"\\nfabric_data_item_id = \"98765432-dcba-4209-8ac2-0821c7f8bd91\"\\nwarehouse = f\"{fabric_workspace_id}/{fabric_data_item_id}\"\\naccount_name = \"onelake\"\\naccount_host = f\"{account_name}.blob.fabric.microsoft.com\"\\n# Configure the catalog object for a specific data item\\ncatalog = load_catalog(\"onelake_catalog\", **{\\n    \"uri\": table_api_url,\\n    \"token\": token,\\n    \"warehouse\": warehouse,\\n    \"adls.account-name\": account_name,\\n    \"adls.account-host\": account_host,\\n    \"adls.credential\": credential,\\n})\\n# List schemas and tables within a data item\\nschemas = catalog.list_namespaces()\\nprint(schemas)\\nfor schema in schemas:\\n    tables = catalog.list_tables(schema)\\n    print(tables)\\nSnowflake\\n-- Create catalog integration object\\nCREATE OR REPLACE CATALOG INTEGRATION IRC_CATINT\\n    CATALOG_SOURCE = ICEBERG_REST\\n    TABLE_FORMAT = ICEBERG\\n    REST_CONFIG = (\\n        CATALOG_URI = \\'https://onelake.table.fabric.microsoft.com/iceberg\\' -- \\nIceberg base URL at the OneLake table endpoint\\n        CATALOG_NAME = \\'12345678-abcd-4fbd-9e50-3937d8eb1915/98765432-dcba-4209-\\n8ac2-0821c7f8bd91\\' -- Fabric data item scope, in the form `workspaceID/dataItemID`\\n    )\\n    REST_AUTHENTICATION = (\\n        TYPE = OAUTH -- Entra auth\\n        OAUTH_TOKEN_URI = \\'https://login.microsoftonline.com/11122233-1122-4138-\\n8485-a47dc5d60435/oauth2/v2.0/token\\' -- Entra tenant ID\\n        OAUTH_CLIENT_ID = \\'44332211-aabb-4d12-aef5-de09732c24b1\\' -- Entra \\napplication client ID\\n        OAUTH_CLIENT_SECRET = \\'[secret]\\' -- Entra application client secret value'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 215, 'page_label': '216'}, page_content='The response of DESC EXTERNAL VOLUME will return metadata about the external volume,\\nincluding:\\nAZURE_CONSENT_URL, which is the permissions request page that needs to be followed if it\\nhasn’t yet been done for your tenant.\\nAZURE_MULTI_TENANT_APP_NAME, which is the name of the Snowflake client application that\\nneeds access to the data item. Make sure to grant it access to the data item in order for\\nSnowflake to be able to read table contents.\\nSQL\\n        OAUTH_ALLOWED_SCOPES = (\\'https://storage.azure.com/.default\\') -- Storage \\ntoken audience\\n    )\\n    ENABLED = TRUE\\n;\\n-- Create external volume object\\nCREATE OR REPLACE EXTERNAL VOLUME IRC_EXVOL\\n    STORAGE_LOCATIONS =\\n    (\\n        (\\n            NAME = \\'IRC_EXVOL\\'\\n            STORAGE_PROVIDER = \\'AZURE\\'\\n            STORAGE_BASE_URL = \\'azure://onelake.dfs.fabric.microsoft.com/12345678-\\nabcd-4fbd-9e50-3937d8eb1915/98765432-dcba-4209-8ac2-0821c7f8bd91\\'\\n            AZURE_TENANT_ID=\\'81ba0d80-2361-40bb-9c1f-bf1c84027025\\' --tenant id\\n        )\\n    )\\n    ALLOW_WRITES = FALSE;\\n;\\n-- Describe the external volume\\nDESC EXTERNAL VOLUME IRC_EXVOL;\\n-- Create a Snowflake catalog linked database\\nCREATE OR REPLACE DATABASE IRC_CATALOG_LINKED\\n    LINKED_CATALOG = (\\n        CATALOG = \\'IRC_CATINT\\'\\n    )\\n    EXTERNAL_VOLUME = \\'IRC_EXVOL\\'\\n;\\nSELECT SYSTEM$CATALOG_LINK_STATUS(\\'IRC_CATALOG_LINKED\\');\\nSELECT * FROM IRC_CATALOG_LINKED.\"dbo\".\"sentiment\";\\nDuckDB'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 216, 'page_label': '217'}, page_content='Use the following sample Python code to configure DuckDB to list schemas and tables within\\na data item.\\nThis code assumes there is a default AzureCredential available for a currently signed-in user.\\nAlternatively, you can use the MSAL Python library to obtain a token.\\nPython\\nimport duckdb\\nfrom azure.identity import DefaultAzureCredential\\n# Iceberg API base URL at the OneLake table API endpoint\\ntable_api_url = \"https://onelake.table.fabric.microsoft.com/iceberg\"\\n# Entra ID token\\ncredential = DefaultAzureCredential()\\ntoken = credential.get_token(\"https://storage.azure.com/.default\").token\\n# Client configuration options\\nfabric_workspace_id = \"12345678-abcd-4fbd-9e50-3937d8eb1915\"\\nfabric_data_item_id = \"98765432-dcba-4209-8ac2-0821c7f8bd91\"\\nwarehouse = f\"{fabric_workspace_id}/{fabric_data_item_id}\"\\n# Connect to DuckDB\\ncon = duckdb.connect()\\n# Install & load extensions\\ncon.execute(\"INSTALL iceberg; LOAD iceberg;\")\\ncon.execute(\"INSTALL azure; LOAD azure;\")\\ncon.execute(\"INSTALL httpfs; LOAD httpfs;\")\\n# --- Auth & Catalog ---\\n# 1) Secret for the Iceberg REST Catalog (use existing bearer token)\\ncon.execute(\"\"\"\\nCREATE OR REPLACE SECRET onelake_catalog (\\nTYPE ICEBERG,\\nTOKEN ?\\n);\\n\"\"\", [token])\\n# 2) Secret for ADLS Gen2 / OneLake filesystem access via Azure extension\\n#    (access token audience must be https://storage.azure.com; account name is \\n\\'onelake\\')\\ncon.execute(\"\"\"\\nCREATE OR REPLACE SECRET onelake_storage (\\nTYPE AZURE,\\nPROVIDER ACCESS_TOKEN,\\nACCESS_TOKEN ?,\\nACCOUNT_NAME \\'onelake\\'\\n);\\n\"\"\", [token])\\n# 3) Attach the Iceberg REST catalog'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 217, 'page_label': '218'}, page_content='These example requests and responses illustrate the use of the Iceberg REST Catalog (IRC)\\noperations currently supported at the OneLake table API endpoint. For more information about\\nIRC, see the open standard specification.\\nFor each of these operations:\\n<BaseUrl> is https://onelake.table.fabric.microsoft.com/iceberg\\n<Warehouse> is <Workspace>/<DataItem>, which can be:\\n<WorkspaceID>/<DataItemID>, such as 12345678-abcd-4fbd-9e50-3937d8eb1915/98765432-\\ndcba-4209-8ac2-0821c7f8bd91\\n<WorkspaceName>/<DataItemName>.<DataItemType>, such as\\nMyWorkspace/MyItem.Lakehouse, as long as both names do not contain special\\ncharacters.\\n<Prefix> is returned by the Get configuration call, and its value is usually the same as\\n<Warehouse>.\\n<Token> is the access token value returned by Entra ID upon successful authentication.\\nList Iceberg catalog configuration settings.\\nRequest\\nResponse\\ncon.execute(f\"\"\"\\nATTACH \\'{warehouse}\\' AS onelake (\\nTYPE ICEBERG,\\nSECRET onelake_catalog,\\nENDPOINT \\'{table_api_url}\\'\\n);\\n\"\"\")\\n# --- Explore & Query ---\\ndisplay(con.execute(\"SHOW ALL TABLES\").fetchdf())\\nExample requests and responses\\nGet configuration\\nGET <BaseUrl>/v1/config?warehouse=<Warehouse>\\nAuthorization: Bearer <Token>'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 218, 'page_label': '219'}, page_content='JSON\\nList schemas\\nList schemas within a Fabric data item.\\nRequest\\nResponse\\nJSON\\nGet schema\\nGet schema details for a given schema.\\n200 OK\\n{\\n    \"defaults\": {},\\n    \"endpoints\": [\\n        \"GET /v1/{prefix}/namespaces\",\\n        \"GET /v1/{prefix}/namespaces/{namespace}\",\\n        \"HEAD /v1/{prefix}/namespaces/{namespace}\",\\n        \"GET /v1/{prefix}/namespaces/{namespace}/tables\",\\n        \"GET /v1/{prefix}/namespaces/{namespace}/tables/{table}\",\\n        \"HEAD /v1/{prefix}/namespaces/{namespace}/tables/{table}\"\\n    ],\\n    \"overrides\": {\\n        \"prefix\": \"<Prefix>\"\\n    }\\n}\\nGET <BaseUrl>/v1/<Prefix>/namespaces\\nAuthorization: Bearer <Token>\\n200 OK\\n{\\n    \"namespaces\": [\\n        [\\n            \"dbo\"\\n        ]\\n    ],\\n    \"next-page-token\": null\\n}'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 219, 'page_label': '220'}, page_content='Request\\nResponse\\nJSON\\nList tables\\nList tables within a given schema.\\nRequest\\nResponse\\nJSON\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>\\nAuthorization: Bearer <Token>\\n200 OK\\n{\\n    \"namespace\": [\\n        \"dbo\"\\n    ],\\n    \"properties\": {\\n        \"location\": \"d892007b-3216-424a-a339-f3dca61335aa/40ef140a-8542-\\n4f4c-baf2-0f8127fd59c8/Tables/dbo\"\\n    }\\n}\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>/tables\\nAuthorization: Bearer <Token>\\n200 OK\\n{\\n    \"identifiers\": [\\n        {\\n            \"namespace\": [\\n                \"dbo\"\\n            ],\\n            \"name\": \"DIM_TestTime\"\\n        },\\n        {\\n            \"namespace\": ['),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 220, 'page_label': '221'}, page_content='Get table\\nGet table details for a given table.\\nRequest\\nResponse\\nJSON\\n                \"dbo\"\\n            ],\\n            \"name\": \"DIM_TestTable\"\\n        }\\n    ],\\n    \"next-page-token\": null\\n}\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>/tables/<TableName>\\nAuthorization: Bearer <Token>\\n200 OK\\n{\\n    \"metadata-location\": \\n\"abfss://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime/metad\\nata/v3.metadata.json\",\\n    \"metadata\": {\\n        \"format-version\": 2,\\n        \"table-uuid\": \"...\",\\n        \"location\": \\n\"abfss://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime\",\\n        \"last-sequence-number\": 2,\\n        \"last-updated-ms\": ...,\\n        \"last-column-id\": 4,\\n        \"current-schema-id\": 0,\\n        \"schemas\": [\\n            {\\n                \"type\": \"struct\",\\n                \"schema-id\": 0,\\n                \"fields\": [\\n                    {\\n                        \"id\": 1,\\n                        \"name\": \"id\",\\n                        \"required\": false,\\n                        \"type\": \"int\"\\n                    },\\n                    {\\n                        \"id\": 2,'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 221, 'page_label': '222'}, page_content='\"name\": \"name\",\\n                        \"required\": false,\\n                        \"type\": \"string\"\\n                    },\\n                    {\\n                        \"id\": 3,\\n                        \"name\": \"age\",\\n                        \"required\": false,\\n                        \"type\": \"int\"\\n                    },\\n                    {\\n                        \"id\": 4,\\n                        \"name\": \"i\",\\n                        \"required\": false,\\n                        \"type\": \"boolean\"\\n                    }\\n                ]\\n            }\\n        ],\\n        \"default-spec-id\": 0,\\n        \"partition-specs\": [\\n            {\\n                \"spec-id\": 0,\\n                \"fields\": []\\n            }\\n        ],\\n        \"last-partition-id\": 999,\\n        \"default-sort-order-id\": 0,\\n        \"sort-orders\": [\\n            {\\n                \"order-id\": 0,\\n                \"fields\": []\\n            }\\n        ],\\n        \"properties\": {\\n            \"schema.name-mapping.default\": \"[ {\\\\n  \\\\\"field-id\\\\\" : 1,\\\\n  \\n\\\\\"names\\\\\" : [ \\\\\"id\\\\\" ]\\\\n}, {\\\\n  \\\\\"field-id\\\\\" : 2,\\\\n  \\\\\"names\\\\\" : [ \\\\\"name\\\\\" \\n]\\\\n}, {\\\\n  \\\\\"field-id\\\\\" : 3,\\\\n  \\\\\"names\\\\\" : [ \\\\\"age\\\\\" ]\\\\n}, {\\\\n  \\\\\"field-\\nid\\\\\" : 4,\\\\n  \\\\\"names\\\\\" : [ \\\\\"i\\\\\" ]\\\\n} ]\",\\n            \"write.metadata.delete-after-commit.enabled\": \"true\",\\n            \"write.data.path\": \\n\"abfs://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime\",\\n            \"XTABLE_METADATA\": \"\\n{\\\\\"lastInstantSynced\\\\\":\\\\\"...\\\\\",\\\\\"instantsToConsiderForNextSync\\\\\":\\n[],\\\\\"version\\\\\":0,\\\\\"sourceTableFormat\\\\\":\\\\\"DELTA\\\\\",\\\\\"sourceIdentifier\\\\\":\\\\\"3\\\\\"\\n}\",\\n            \"write.parquet.compression-codec\": \"zstd\"\\n        },\\n        \"current-snapshot-id\": ...,\\n        \"refs\": {\\n            \"main\": {\\n                \"snapshot-id\": ...,\\n                \"type\": \"branch\"\\n            }\\n        },'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 222, 'page_label': '223'}, page_content='\"snapshots\": [\\n            {\\n                \"sequence-number\": 2,\\n                \"snapshot-id\": ...,\\n                \"parent-snapshot-id\": ...,\\n                \"timestamp-ms\": ...,\\n                \"summary\": {\\n                    \"operation\": \"overwrite\",\\n                    \"XTABLE_METADATA\": \"\\n{\\\\\"lastInstantSynced\\\\\":\\\\\"...\\\\\",\\\\\"instantsToConsiderForNextSync\\\\\":\\n[],\\\\\"version\\\\\":0,\\\\\"sourceTableFormat\\\\\":\\\\\"DELTA\\\\\",\\\\\"sourceIdentifier\\\\\":\\\\\"3\\\\\"\\n}\",\\n                    \"added-data-files\": \"1\",\\n                    \"deleted-data-files\": \"1\",\\n                    \"added-records\": \"1\",\\n                    \"deleted-records\": \"1\",\\n                    \"added-files-size\": \"2073\",\\n                    \"removed-files-size\": \"2046\",\\n                    \"changed-partition-count\": \"1\",\\n                    \"total-records\": \"6\",\\n                    \"total-files-size\": \"4187\",\\n                    \"total-data-files\": \"2\",\\n                    \"total-delete-files\": \"0\",\\n                    \"total-position-deletes\": \"0\",\\n                    \"total-equality-deletes\": \"0\"\\n                },\\n                \"manifest-list\": \\n\"abfss://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime/metad\\nata/snap-....avro\",\\n                \"schema-id\": 0\\n            }\\n        ],\\n        \"statistics\": [],\\n        \"snapshot-log\": [\\n            {\\n                \"timestamp-ms\": ...,\\n                \"snapshot-id\": ...\\n            }\\n        ],\\n        \"metadata-log\": [\\n            {\\n                \"timestamp-ms\": ...,\\n                \"metadata-file\": \\n\"abfss://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime/metad\\nata/v1.metadata.json\"\\n            },\\n            {\\n                \"timestamp-ms\": ...,\\n                \"metadata-file\": \\n\"abfss://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime/metad\\nata/v2.metadata.json\"\\n            }\\n        ]\\n    }\\n}'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 223, 'page_label': '224'}, page_content='Learn more about OneLake table APIs.\\nLearn more about OneLake table APIs for Iceberg.\\nSet up automatic Delta Lake to Iceberg format conversion.\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 224, 'page_label': '225'}, page_content='OneLake table APIs for Delta\\n10/27/2025\\nOneLake offers a REST API endpoint for interacting with tables in Microsoft Fabric. This article\\ndescribes how to get started using this endpoint to interact with Delta APIs available at this\\nendpoint for metadata read operations. These operations are compatible with Unity Catalog\\nAPI open standard.\\nFor overall OneLake table API guidance and prerequisite guidance, see the OneLake table API\\noverview.\\nFor detailed API documentation, see the Getting started guide.\\nThe OneLake table API endpoint is:\\nAt the OneLake table API endpoint, the Delta APIs are available under the following <BaseUrl>.\\nThe following Delta API operations are currently supported at this endpoint. Detailed guidance\\nfor these operations is available in the Getting started guide.\\nList schemas\\nGET <BaseUrl>/<WorkspaceName or WorkspaceID>/<ItemName or ItemID>/api/2.1/unity-\\ncatalog/schemas?catalog_name=<ItemName or ItemID>\\n）  Important\\nThis feature is in preview.\\nDelta table API endpoint\\nhttps://onelake.table.fabric.microsoft.com\\nhttps://onelake.table.fabric.microsoft.com/delta\\nDelta table API operations'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 225, 'page_label': '226'}, page_content='This operation accepts the workspace ID and data item ID (or their equivalent friendly\\nnames if they don’t contain any special characters).\\nThis operation returns the list of schemas within a data item. If the data item does not\\nsupport schemas, a fixed schema named dbo is returned.\\nList tables\\nGET <BaseUrl>/<WorkspaceName or WorkspaceID>/<ItemName or ItemID>/api/2.1/unity-\\ncatalog/tables?catalog_name=<ItemName or ItemID>&schema_name=<SchemaName>\\nThis operation returns the list of tables found within a given schema.\\nGet table\\nGET <BaseUrl>/<WorkspaceName or WorkspaceID>/<ItemName or ItemID>/api/2.1/unity-\\ncatalog/tables/<TableName>\\nThis operation returns metadata details for a table within a schema, if the table is found.\\nSchema exists\\nHEAD <BaseUrl>/<WorkspaceName or WorkspaceID>/<ItemName or ItemID>/api/2.1/unity-\\ncatalog/schemas/<SchemaName>\\nThis operation checks for the existence of a schema within a data item and returns\\nsuccess if the schema is found.\\nTable exists\\nHEAD <BaseUrl>/<WorkspaceName or WorkspaceID>/<ItemName or ItemID>/api/2.1/unity-\\ncatalog/tables/<TableName>\\nThis operation checks for the existence of a table within a schema and returns success if\\nthe schema is found.\\nThe use of the OneLake table APIs for Delta is subject to the following limitations and\\nconsiderations:\\nCertain data items may not support schemas\\nDepending on the type of data item you use, such as non-schema-enabled Fabric\\nlakehouses, there may not be schemas within the Tables directory. In such cases, for\\nCurrent limitations, considerations'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 226, 'page_label': '227'}, page_content='compatibility with API clients, the OneLake table APIs provide a default, fixed dbo schema\\n(or namespace) to contain all tables within a data item.\\nAdditional query string parameters required if your schema name or table name\\ncontains dots\\nIf your schema or table name contains dots (.) and is included in the URL, you must also\\nprovide additional query parameters. For example, when the schema name includes dots,\\ninclude the catalog_name as an additional query parameter in the API call to check\\nwhether the schema exists.\\nMetadata write operations, other operations\\nOnly the operations listed in Delta table API operations are supported today. Operations\\nthat handle metadata write operations are not yet supported by the OneLake table Delta\\nAPI endpoint.\\nLearn more about OneLake table APIs.\\nSee detailed guidance and API details.\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 227, 'page_label': '228'}, page_content='Getting started with OneLake table APIs for\\nDelta\\n10/27/2025\\nOneLake offers a REST API endpoint for interacting with tables in Microsoft Fabric. This\\nendpoint supports read-only metadata operations for Delta tables in Fabric. These operations\\nare compatible with Unity Catalog API open standard.\\nThese example requests and responses illustrate the use of the Delta API operations currently\\nsupported at the OneLake table API endpoint.\\nFor each of these operations:\\n<BaseUrl> is https://onelake.table.fabric.microsoft.com/delta\\n<Workspace>/DataItem> can be:\\n<WorkspaceID>/<DataItemID>, such as 12345678-abcd-4fbd-9e50-3937d8eb1915/98765432-\\ndcba-4209-8ac2-0821c7f8bd91\\n<WorkspaceName>/<DataItemName>.<DataItemType>, such as\\nMyWorkspace/MyItem.Lakehouse, as long as both names do not contain special\\ncharacters.\\n<Token> is the access token value returned by Microsoft Entra ID upon successful\\nauthentication.\\nList schemas within a Fabric data item.\\nRequest\\nBash\\n）  Important\\nThis feature is in preview.\\nExample requests and responses\\nList schemas\\ncurl -X GET \\\\\\n  \"<BaseUrl>/<Workspace>/testlh.Lakehouse/api/2.1/unity-catalog/schemas?\\ncatalog_name=testlh.Lakehouse\" \\\\'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 228, 'page_label': '229'}, page_content='Response\\nJSON\\nList tables within a given schema.\\nRequest\\nBash\\nResponse\\nJSON\\n  -H \"Authorization: Bearer <Token>\" \\\\\\n  -H \"Content-Type: application/json\"\\n200 OK\\n{\\n\"schemas\": [\\n{\\n\"name\": \"dbo\",\\n\"catalog_name\": \"testlh.Lakehouse\",\\n        \"full_name\": \"testlh.Lakehouse.dbo\",\\n\"created_at\": 1759768029062,\\n\"updated_at\": 1759768029062,\\n\"comment\": null,\\n\"properties\": null,\\n\"owner\": null,\\n\"created_by\": null,\\n\"updated_by\": null,\\n\"schema_id\": null\\n}\\n],\\n\"next_page_token\": null\\n}\\nList tables\\ncurl -X GET \\\\\\n  \"<BaseUrl>/<Workspace>/testlh.Lakehouse/api/2.1/unity-catalog/tables?\\ncatalog_name=testlh.Lakehouse&schema_name=dbo\" \\\\\\n  -H \"Authorization: Bearer <Token>\" \\\\\\n  -H \"Content-Type: application/json\"\\n200 OK\\n{\\n\"tables\": [\\n    {'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 229, 'page_label': '230'}, page_content='Get table details for a given table.\\nRequest\\nBash\\nResponse\\nJSON\\n        \"name\": \"product_table\",\\n        \"catalog_name\": \"testlh.Lakehouse\",\\n        \"schema_name\": \"dbo\",\\n        \"table_type\": null,\\n        \"data_source_format\": \"DELTA\",\\n        \"columns\": null,\\n        \"storage_location\": \\n\"https://onelake.dfs.fabric.microsoft.com/.../.../Tables/product_table\",\\n        \"comment\": null,\\n        \"properties\": null,\\n        \"owner\": null,\\n        \"created_at\": null,\\n        \"created_by\": null,\\n        \"updated_at\": null,\\n        \"updated_by\": null,\\n        \"table_id\": null\\n    }\\n],\\n\"next_page_token\": null\\n}\\nGet table\\ncurl -X GET \\\\\\n  \"<BaseUrl>/<Workspace>/testlh.Lakehouse/api/2.1/unity-\\ncatalog/tables/testlh.Lakehouse.dbo.product_table\" \\\\\\n  -H \"Authorization: Bearer <Token>\" \\\\\\n  -H \"Content-Type: application/json\"\\n    200 OK\\n    {\\n\"name\": \"product_table\",\\n\"catalog_name\": \"testlh.Lakehouse\",\\n\"schema_name\": \"dbo\",\\n\"table_type\": null,\\n\"data_source_format\": \"DELTA\",\\n\"columns\": [\\n{\\n\"name\": \"product_id\",\\n\"type_text\": null,'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 230, 'page_label': '231'}, page_content='\"type_json\": null,\\n\"type_name\": \"string\",\\n\"type_precision\": 0,\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 0,\\n\"nullable\": true\\n},\\n{\\n\"name\": \"product_name\",\\n\"type_text\": null,\\n\"type_json\": null,\\n\"type_name\": \"string\",\\n\"type_precision\": 0,\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 1,\\n\"nullable\": true\\n},\\n{\\n\"name\": \"category\",\\n\"type_text\": null,\\n\"type_json\": null,\\n\"type_name\": \"string\",\\n\"type_precision\": 0,\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 2,\\n\"nullable\": true\\n},\\n{\\n\"name\": \"brand\",\\n\"type_text\": null,\\n\"type_json\": null,\\n\"type_name\": \"string\",\\n\"type_precision\": 0,\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 3,\\n\"nullable\": true\\n},\\n{\\n\"name\": \"price\",\\n\"type_text\": null,\\n\"type_json\": null,\\n\"type_name\": \"double\",\\n\"type_precision\": 0,'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 231, 'page_label': '232'}, page_content='Learn more about OneLake table APIs.\\nLearn more about OneLake table APIs for Delta.\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 4,\\n\"nullable\": true\\n},\\n{\\n\"name\": \"launch_date\",\\n\"type_text\": null,\\n\"type_json\": null,\\n\"type_name\": \"date\",\\n\"type_precision\": 0,\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 5,\\n\"nullable\": true\\n}\\n],\\n\"storage_location\": \\n\"https://onelake.dfs.fabric.microsoft.com/.../.../Tables/product_table\",\\n\"comment\": null,\\n\"properties\": null,\\n\"owner\": null,\\n\"created_at\": 1759703452000,\\n\"created_by\": null,\\n\"updated_at\": 1759703452000,\\n\"updated_by\": null,\\n\"table_id\": \"df2b3038-c21a-429d-90b8-f3bbf2d3db5d\"\\n    }\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 232, 'page_label': '233'}, page_content='OneLake security overview\\n09/08/2025\\nOneLake is a hierarchical data lake, like Azure Data Lake Storage (ADLS) Gen2 or the Windows\\nfile system. Security in OneLake is enforced at multiple levels, each corresponding to different\\naspects of access and control. Understanding the distinction between control plane and data\\nplane permissions is key to effectively securing your data:\\nControl plane permissions: Govern what actions users can perform within the\\nenvironment (e.g., creating, managing, or sharing items). Control plane permissions often\\nprovide data plane permissions by default.\\nData plane permissions: Govern what data users can access or view, regardless of their\\nability to manage resources.\\nYou can set security at each level within the data lake. However, some levels in the hierarchy\\nare given special treatment because they correlate with Fabric concepts. OneLake security\\ncontrols all access to OneLake data with different permissions inherited from the parent item or\\nworkspace permissions. You can set permissions at the following levels:\\nWorkspace: A collaborative environment for creating and managing items. Security is\\nmanaged through workspace roles at this level.\\nItem: A set of capabilities bundled together into a single component. A data item is a\\nsubtype of item that allows data to be stored within it using OneLake. Items inherit\\npermissions from the workspace roles, but can have additional permissions as well.\\nFolders: Folders within an item that are used for storing and managing data, such as\\nTables/ or Files/.\\nItems always live within workspaces and workspaces always live directly under the OneLake\\nnamespace. You can visualize this structure as follows:'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 233, 'page_label': '234'}, page_content='This section describes the security model based on generally available OneLake features.\\nWorkspace permissions define what actions users can take within a workspace and its items.\\nThese permissions are managed at the workspace level and are primarily control plane\\npermissions; they determine administrative and item management capabilities, not direct data\\naccess. However, workspace permissions will generally inherit down to the item and folder level\\nto grant data access by default. Workspace permissions allow for defining access to all items\\nwithin that workspace. There are four different workspace roles, each of which grants different\\ntypes of access. Below are the default behaviors of each workspace role.\\nRole Can add\\nadmins?\\nCan add\\nmembers?\\nCan edit\\nOneLake\\nsecurity?\\nCan write data\\nand create items?\\nCan read data in\\nOneLake?\\nAdmin Yes Yes Yes Yes Yes\\nMember No Yes Yes Yes Yes\\nContributorNo No No Yes Yes\\nViewer No No No No No*\\n\\uf80a\\nSecurity in OneLake\\nWorkspace permissions\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 234, 'page_label': '235'}, page_content=\"You can simplify the management of Fabric workspace roles by assigning them to security\\ngroups. This method lets you control access by adding or removing members from the security\\ngroup.\\nWith the sharing feature, you can give a user direct access to an item. The user can only see\\nthat item in the workspace and isn't a member of any workspace roles. Item permissions grant\\naccess to connect to that item and its endpoints the user is able to access.\\nPermission See the item metadata? See data in SQL? See data in OneLake?\\nRead Yes No No\\nReadData No Yes No\\nReadAll No No Yes*\\n*Not applicable to items with OneLake security or data access roles enabled. If the preview is\\nenabled, ReadAll only grants access if the DefaultReader role is in use. If the DefaultReader role\\nis edited or deleted, access is instead granted based on what data access roles the user is part\\nof.\\nAnother way to configure permissions is via an item's Manage permissions page. Using this\\npage, you can add or remove individual item permission for users or groups. The item type\\ndetermines which permissions are available.\\nOneLake security allows users to define granular role-based security to data stored in OneLake,\\nand enforce that security consistently across all compute engines in Fabric. OneLake security is\\nthe data plane security model for data in OneLake.\\n７ Note\\n*Viewers can be given access to data through OneLake security roles.\\nItem permissions\\nﾉ Expand table\\nOneLake security (preview)\\n７ Note\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 235, 'page_label': '236'}, page_content='Fabric users in the Admin or Member roles can create OneLake security roles to grant users\\naccess to data within an item. Each role has four components:\\nData: The tables or folders that users can access.\\nPermission: The permissions that users have on the data.\\nMembers: The users that are members of the role.\\nConstraints: The components of the data, if any, that are excluded from role access, such\\nas specific rows or columns.\\nOneLake security roles grant access to data for users in the Viewer workspace role or with\\nRead permission on the item. Admins, Members, and Contributors are not affected by OneLake\\nsecurity roles and can read and write all data in an item regardless of their role membership. A\\nDefaultReader role exists in all lakehouses that gives any user with the ReadAll permission\\naccess to data in the lakehouse. The DefaultReader role can be deleted or edited to remove\\nthat access.\\nLearn more about creating OneLake security roles for Tables and folders, Columns, and Rows.\\nLearn more about the access control model for OneLake security..\\nCompute permissions are a type of data plane permission that applies to a specific query\\nengine in Microsoft Fabric. The access granted applies only to queries run against that specific\\nengine, such as the SQL endpoint or a Power BI semantic model. However, users might see\\ndifferent results when they access data through a compute engine compared to when they\\naccess data directly in OneLake, depending on the compute permissions applied. OneLake\\nsecurity is the recommended approach to secure data in OneLake to ensure consistent\\nresults across all engines that a user might interact with.\\nCompute engines may have more advanced security features that are not yet available in\\nOneLake security, and in that case using the compute permissions may be required to solve\\nsome scenarios. When using compute permissions to secure access to data, make sure that\\nend users are given access only to the compute engine where the security is set. This prevents\\ndata from being accessed through a different engine without the necessary security features.\\nOneLake security replaces the existing OneLake data access roles (preview) feature that\\nwas released in April 2024. All data access roles users are seamlessly and automatically\\nupgraded to OneLake security roles when the feature moved to public preview.\\nCompute permissions\\nShortcut security'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 236, 'page_label': '237'}, page_content=\"Shortcuts in Microsoft Fabric allow for simplified data management. OneLake folder security\\napplies to OneLake shortcuts based on roles defined in the lakehouse where the data is stored.\\nFor more information on shortcut security considerations, see OneLake security access control\\nmodel.\\nFor information on access and authentication details for specific shortcuts, see types of\\nOneLake shortcuts.\\nOneLake uses Microsoft Entra ID for authentication; you can use it to give permissions to user\\nidentities and service principals. OneLake automatically extracts the user identity from tools,\\nwhich use Microsoft Entra authentication and maps it to the permissions you set in the Fabric\\nportal.\\nTo view your OneLake audit logs, follow the instructions in Track user activities in Microsoft\\nFabric. OneLake operation names correspond to ADLS APIs such as CreateFile or DeleteFile.\\nOneLake audit logs don't include read requests or requests made to OneLake via Fabric\\nworkloads.\\nData stored in OneLake is encrypted at rest by default using Microsoft-managed keys.\\nMicrosoft-managed keys are rotated appropriately. Data in OneLake is encrypted and\\ndecrypted transparently and is FIPS 140-2 compliant.\\nEncryption at rest using customer-managed keys currently isn't supported. You can submit a\\nrequest for this feature on Microsoft Fabric ideas.\\nAuthentication\\n７ Note\\nTo use service principals in a Fabric tenant, a tenant administrator must enable Service\\nPrincipal Names (SPNs) for the entire tenant or specific security groups. Learn more about\\nenabling Service Principals in Developer settings of the tenant admin portal.\\nAudit Logs\\nEncryption and networking\\nData at Rest\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 237, 'page_label': '238'}, page_content=\"Data in transit across the public internet between Microsoft services is always encrypted with at\\nleast TLS 1.2. Fabric negotiates to TLS 1.3 whenever possible. Traffic between Microsoft services\\nalways routes over the Microsoft global network.\\nInbound OneLake communication also enforces TLS 1.2 and negotiates to TLS 1.3 whenever\\npossible. Outbound Fabric communication to customer-owned infrastructure prefers secure\\nprotocols but might fall back to older, insecure protocols (including TLS 1.0) when newer\\nprotocols aren't supported.\\nTo configure private links in Fabric, see Set up and use private links.\\nYou can allow or restrict access to OneLake data from applications that are outside of the\\nFabric environment. Admins can find this setting in the OneLake section of the admin portal\\ntenant settings.\\nWhen you turn on this setting, users can access data from all sources. For example, turn this\\nsetting on if you have custom applications that use Azure Data Lake Storage (ADLS) APIs or\\nOneLake file explorer. When you turn off this setting, users can still access data from internal\\napps like Spark, Data Engineering, and Data Warehouse, but can't access data from\\napplications running outside of Fabric environments.\\nFabric and OneLake security overview\\nOneLake data access roles (preview)\\nWorkspace roles\\nOneLake file explorer\\nShare items\\nData in transit\\nPrivate links\\nAllow apps running outside of Fabric to access data\\nvia OneLake\\nRelated content\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 238, 'page_label': '239'}, page_content=\"Get started with OneLake security (preview)\\nOneLake security enables you to apply role-based access control (RBAC) to your data stored in\\nOneLake. You can define security roles that grant access to specific folders within a Fabric item,\\nthen assign these roles to users or groups. Roles can also contain row or column level security\\nto further limit access. The OneLake security permissions determine what data that user can see\\nacross all experiences in Fabric.\\nFabric users with Write and Reshare permissions (generally Admin and Member workspace\\nusers) can get started by creating OneLake security roles to grant access to only specific folders\\nor tables in a Fabric data item. To grant access to data in an item, add users to a data access\\nrole. Users that aren't part of a data access role see no data in that item.\\nTo configure OneLake security, you must be an Admin or Member in the workspace, or have\\nWrite and Reshare permissions. Role creation and membership assignment take effect as soon\\nas the role is saved, so make sure you want to grant access before adding someone to a role.\\nThe following table outlines which data items support OneLake security:\\nFabric item Status Supported permissions\\nLakehouse Preview Read, ReadWrite\\nAzure Databricks Mirrored Catalog Preview Read\\nOneLake security is currently in preview and as a result is disabled by default. The preview\\nfeature is configured on a per-item basis. The opt-in control allows for a single item to try the\\npreview without enabling it on any other Fabric items.\\nThe preview feature can't be turned off once enabled.\\n1. Navigate to a lakehouse and select Manage OneLake security (preview).\\n2. Review the confirmation dialog. The data access roles preview isn't compatible with the\\nExternal data sharing preview. If you're ok with the change, select Continue.\\nTo ensure a smooth opt-in experience, all users with read permission to data in the item\\ncontinue to have read access through a default data access role called DefaultReader. With\\nPrerequisites\\nﾉ Expand table\\nHow to opt in\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 239, 'page_label': '240'}, page_content='virtualized role memberships, all users that had the necessary permissions to view data in the\\nlakehouse (the ReadAll permission) are included as members of this default role. To start\\nrestricting access to those users, delete the DefaultReader role or remove the ReadAll\\npermission from the accessing users.\\nUse OneLake security roles to manage OneLake read access to any tables or folders in an item.\\nAccess to tables can be further restricted using row and/or column level security. Any security\\nset applies to access from all engines in Fabric. For more information, see the data access\\ncontrol model.\\nFor specific item types, ReadWrite access can also be configured. This permission gives users\\nthe ability to edit data in a lakehouse on specified tables or folders without giving them access\\nto create or manage Fabric items. ReadWrite access enables users to perform write operations\\nthrough Spark notebooks, the OneLake file explorer, or OneLake APIs. Write operations\\nthrough the Lakehouse UX for viewers is not supported.\\nUse the following steps to create a OneLake security role.\\n1. Open the Fabric item where you want to define security.\\n2. Select Manage OneLake security (preview) from the item menu.\\n3. On the OneLake security (preview) pane, select New.\\n4. Provide a name for the new role that meets the following guidelines:\\nThe role name can only contain alphanumeric characters.\\nThe role name must start with a letter.\\nNames are case insensitive and must be unique.\\nThe maximum name length is 128 characters.\\n5. Select Grant as the type of role.\\n） Important\\nMake sure that any users that are included in a data access role are removed from the\\nDefaultReader role. Otherwise they maintain full access to the data.\\nWhat types of data can be secured?\\nCreate a role'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 240, 'page_label': '241'}, page_content=\"6. Choose the permissions you want to grant. Read is selected at a minimum, and you can\\noptionally choose ReadWrite.\\n7. If you want this role to apply to all of the tables and files in this lakehouse, select the All\\ndata toggle.\\nThis selection also provides access to any folders that are added in the future.\\n8. If you want this role to apply only to a selected group of tables and folders, select the\\nSelected data toggle. Then, use the following steps to define the approved data for this\\nrole.\\na. Select Browse Lakehouse or the equivalent for the item that you're working with.\\nb. Expand the Tables and Files directories to view data in your lakehouse.\\nc. Check the boxes next to the tables and files that you want the role to apply to.\\nd. Select Add data to add the selected items to your role.\\n9. Use the Add members to your role textbox to manually enter the names or email\\naddresses of users that you want to include in the role. Or, select Advanced configuration\\nand follow the guidance in Assign virtual members.\\nTo add members manually:\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 241, 'page_label': '242'}, page_content='a. Enter the name or email address of a user.\\nb. Select the correct name from the suggested list.\\nc. Select the check icon to confirm your selection, or the X icon to clear the selection.\\n10. Review the Preview role summaries.\\na. To edit the data preview, select Browse Lakehouse and update the selected tables and\\nfolders.\\nb. To remove a user from the members preview, select more options (...) next to their\\nname, then Remove from role.\\n11. Select Create role and wait for the notification that the role was successfully published.\\nUse the following steps to edit an existing OneLake security role.\\n1. Open the item where you want to define security.\\n2. Select Manage OneLake security (preview) from the item menu.\\n3. On the OneLake security (preview) pane, select the role that you want to edit.\\nThis action opens the role details page, which includes two tabs: Data in role and\\nMembers in role.\\n4. Review the information in the Data in role tab:\\nThis tab shows all of the data that the members of the role can access.\\nThe role name tells you which role you are looking at. To edit the role name, select the\\nEdit dropdown in the upper right corner, select Update role name, enter a new name,\\nand then confirm with the check mark. You can discard your changes by selecting the X.\\nThe Permissions item at the top, tells you what permissions the role currently grants. To\\nchange the role permissions, select the Edit dropdown in the upper right corner, select\\nEdit role permissions, edit the selected permissions with the dropdown, and then confirm\\nwith the check mark. You can discard your changes by selecting the X.\\nThe Data column shows the name of the tables or folders that are part of the role access.\\nYou can expand and collapse schemas to view the items underneath. Hover over an entry\\nto view the full path of the table or folder. Hover over the ... to see options to configure\\nRow-level security or Column-level security. The row level security and column level\\nsecurity guides provide more information on how that works.\\nEdit a role'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 242, 'page_label': '243'}, page_content='The Type column tells you the type of item that was selected. The values are either:\\nSchema, Table, or Folder.\\nThe Data access column indicates whether any row or column level restrictions are\\napplied to the item. An icon with a lock and horizontal lines indicates row level security is\\napplied, while an icon with a lock and vertical lines indicates column level security is\\napplied.\\n5. To edit the data included in the role, select Add data.\\nThis action opens the table and folder selection dialog.\\n6. Check and uncheck tables or folders to add or remove them from the role.\\n7. Select Add data to confirm your selections.\\n8. Select the Members in role tab to view the members of the role.\\nThe Members column shows the profile picture and name of the member.\\nThe Type column indicates whether the member is a User or Group.\\nThe Added using column denotes whether a user was added via their Email as a member\\nof the role, or included as part of a lakehouse permissions group. For more information\\nabout adding users using item permissions, see Assign virtual members.\\n9. To edit the members of the role, select Add members.\\n10. To add members manually, enter a name or email in the Add members to your role\\ntextbox. Select the correct name from the suggested list. Then, select the check icon to\\nconfirm your selection, or select the X icon to clear the selection.\\n11. To remove users from the role, select more options (...) next to their name and select\\nRemove from role.\\nMaking any changes to role membership updates the role immediately. A notification notes\\nthe success or failure of any changes.\\nUse the following steps to delete a OneLake data access role.\\n1. Open the lakehouse where you want to define security.\\n2. Select Manage OneLake security (preview) from the Lakehouse menu.\\nDelete a role'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 243, 'page_label': '244'}, page_content=\"3. On the OneLake security (preview) pane, check the box next to the roles you want to\\ndelete.\\n4. Select Delete and wait for the notification that the roles are successfully deleted.\\nOneLake security role supports two methods of adding users to a role. The main method is by\\nadding users or groups directly to a role using the Add people or groups box on the Assign\\nrole page. The second is by creating virtual memberships with permission groups using the\\nAdvanced configuration control.\\nAdding users directly to a role adds the users as explicit members of the role. These users show\\nup with their name and picture shown in the Members list.\\nThe virtual members allow for the membership of the role to be dynamically adjusted based on\\nthe Fabric item permissions of the users. By selecting Advanced configuration and selecting a\\npermission, you add any user in the Fabric workspace who has all of the selected permissions\\nas an implicit member of the role. For example, if you chose ReadAll, Write then any user of\\nthe Fabric workspace that has ReadAll and Write permissions to the item would be included as\\na member of the role. You can see which users are being added by a permission group by\\nlooking at the Added using column in the Members in role tab. These members can't be\\nmanually removed directly. To remove a member that was added through a permission group,\\nremove the permission group from the role.\\nRegardless of which membership type you use, OneLake security roles support adding\\nindividual users, Microsoft Entra groups, and security principals.\\nThe permissions that can be used for virtual members are:\\nRead\\nWrite\\nReshare\\nExecute\\nReadAll\\nTo assign users with permission groups, use the following steps:\\n1. Select the name of the role you want to assign members to.\\n2. On the role details page, select the\\u202fMembers in role tab.\\nAssign a member or group\\nAssign virtual members\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 244, 'page_label': '245'}, page_content='3. Select Add members.\\n4. Select Advanced configuration.\\n5. In the Permission groups box, select the checkbox next to each permission that you want\\nto include users for.\\nEach permission group shows a count of how many users are included in that group.\\nSelecting multiple permission groups includes users with all of the selected required\\npermissions.\\n6. Select Add to include the groups and save the role.\\nFabric Security overview\\nFabric and OneLake security overview\\nData access control model\\nLast updated on 11/18/2025\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 245, 'page_label': '246'}, page_content=\"Best practices for OneLake security\\n09/08/2025\\nIn this article, we'll look at best practices around securing data in OneLake. For more\\ninformation on how to implement security for specific use cases, see the how-to guides.\\nLeast privilege access is a fundamental security principle in computer science that advocates\\nfor restricting users' permissions and access rights to only those permissions necessary to\\nperform their tasks. For OneLake, this means assigning permissions at the appropriate level to\\nensure that users aren't over-provisioned and reduce risk.\\nIf users only need access to a single lakehouse or data item, use the share feature to\\ngrant them access to only that item. Assigning a user to a workspace role should only be\\nused if that user needs to see ALL items in that workspace.\\nUse OneLake security to restrict access to folders and tables within a lakehouse. For\\nsensitive data, OneLake security row or column level security ensures that protected row\\nand columns remain hidden.\\nDifferent users need the ability to perform different actions in Fabric in order to do their jobs.\\nSome common use cases are identified in this section along with the necessary permissions\\nsetup in Fabric and OneLake.\\nManage workspace access The admin or member workspace roles are required. These roles\\ncan also manage OneLake security roles on an item.\\nCreate new items in Fabric Either Admin, Member, or Contributor roles can create or delete\\nnew items.\\nWrite data to OneLake Either Admin, Member, or Contributor roles can write data to OneLake\\nthrough Spark or through uploads. They can also write data to a warehouse. Users with only\\nread access on a warehouse can be given permissions to write data through SQL permissions.\\nRead data from OneLake A user needs to be a workspace Viewer, or have the Read permission\\nand the ReadAll permission to read data from OneLake. For lakehouses with the OneLake\\nsecurity (preview) feature enabled, access to data is controlled by the user's OneLake security\\nrole permissions.\\nLeast privilege\\nSecure by use case\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 246, 'page_label': '247'}, page_content='Subscribe to OneLake events A user needs SubscribeOneLakeEvents to be able to subscribe to\\nevents from a Fabric item. Admin, Member, and Contributor roles have this permission by\\ndefault. You can add this permission for a user with Viewer role.\\nFabric Security overview\\nFabric and OneLake security overview\\nData Access Control Model\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 247, 'page_label': '248'}, page_content='OneLake security access control model\\n(preview)\\nThis document provides a detailed guide to how the OneLake security access control model\\nworks. It contains details on how the roles are structured, how they apply to data, and what the\\nintegration is with other structures within Microsoft Fabric.\\nOneLake security uses a role based access control (RBAC) model for managing access to data\\nin OneLake. Each role is made up of several key components.\\nType: Whether the role gives access (GRANT) or removes access (DENY). Only GRANT\\ntype roles are supported.\\nPermission: The specific action or actions that are being granted or denied.\\nScope: The OneLake objects that have the permission. Objects are tables, folders, or\\nschemas.\\nMembers: Any Microsoft Entra identity that is assigned to the role, such as users, groups,\\nor nonuser identities. The role is granted to all members of a Microsoft Entra group.\\nBy assigning a member to a role, that user is then subject to the associated permissions on the\\nscope of that role. Because OneLake security uses a deny-by-default model, all users start with\\nno access to data unless explicitly granted by a OneLake security role.\\nOneLake security roles support the following permission:\\nRead: Grants the user the ability to read data from a table and view the associated table\\nand column metadata. In SQL terms, this permission is equivalent to both\\nVIEW_DEFINITION and SELECT. For more information, see the Metadata security.\\nReadWrite: Grants the user the ability to read and write data from a table or folder and\\nview the associated table and column metadata. In SQL terms, this permission is\\nequivalent to ALTER, DROP, UPDATE, and INSERT. For more information see ReadWrite\\npermission.\\nOneLake security enables users to define data access roles for the following Fabric items only.\\nOneLake security roles\\nPermissions and supported items\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 248, 'page_label': '249'}, page_content='Fabric item Status Supported permissions\\nLakehouse Public Preview Read, ReadWrite\\nAzure Databricks Mirrored Catalog Public Preview Read\\nWorkspace permissions are the first security boundary for data within OneLake. Each\\nworkspace represents a single domain or project area where teams can collaborate on data.\\nYou manage security in the workspace through Fabric workspace roles. Learn more about\\nFabric role-based access control (RBAC): Workspace roles\\nFabric workspace roles give permissions that apply to all items in the workspace. The following\\ntable outlines the basic permissions allowed by workspace roles.\\nPermission Admin Member Contributor Viewer\\nView files in OneLakeAlways*\\nYes\\nAlways*\\nYes\\nAlways* Yes No by default. Use OneLake security\\nto grant the access.\\nWrite files in OneLakeAlways*\\nYes\\nAlways*\\nYes\\nAlways* Yes No\\nCan edit OneLake\\nsecurity roles\\nAlways*\\nYes\\nAlways*\\nYes\\nNo No\\n*Since Workspace Admin, Member and Contributor roles automatically grant Write permissions\\nto OneLake, they override any OneLake security Read permissions.\\nWorkspace roles manage the control plane data access, meaning interactions with creating and\\nmanaging Fabric artifacts and permissions. In addition, workspace roles also provide default\\naccess levels to data items by using OneLake security default roles. (Note that default roles\\nonly apply to Viewers, since Admin, Member, and Contributor have elevated access through\\nthe Write permission) A default role is a normal OneLake security role that is created\\nautomatically with every new item. It gives users with certain workspace or item permissions a\\ndefault level of access to data in that item. For example, Lakehouse items have a DefaultReader\\nrole that lets users with the ReadAll permission see data in the Lakehouse. This ensures that\\nusers accessing a newly created item have a basic level of access. All default roles use a\\nmember virtualization feature, so that the members of the role are any user in that workspace\\nwith the required permission. For example, all users with ReadAll permission on the Lakehouse.\\nOneLake security and workspace permissions\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 249, 'page_label': '250'}, page_content=\"The following table shows what the standard default roles are. Items may have specialized\\ndefault roles that apply only to that item type.\\nFabric\\nitem\\nRole name PermissionFolders included Assigned members\\nLakehouseDefaultReader Read All folders under Tables/\\nand Files/\\nAll users with ReadAll\\npermission\\nLakehouseDefaultReadWriter Read All folders All users with Write\\npermission\\nWithin a workspace, Fabric items can have permissions configured separately from the\\nworkspace roles. You can configure permissions either through sharing an item or by managing\\nthe permissions of an item. The following permissions determine a user's ability to perform\\nactions on data in OneLake. For more information on item sharing, see How Lakehouse sharing\\nworks\\nPermission Can view files in OneLake? Can write files in\\nOneLake?\\nCan read data through\\nSQL analytics\\nendpoint?\\nRead No by default. Use OneLake\\nsecurity to grant access.\\nNo No\\nReadAll Yes through the DefaultReader\\nrole. Use OneLake security to\\nrestrict access.\\nNo No*\\nWrite Yes Yes Yes\\nExecute, Reshare,\\nViewOutput,\\nN/A - can't be granted on its ownN/A - can't be\\ngranted on its\\nN/A - can't be granted\\non its own\\nﾉ Expand table\\n７ Note\\nTo restrict the access to specific users or specific folders, either modify the default role or\\nremove it and create a new custom role.\\nOneLake security and item permissions\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 250, 'page_label': '251'}, page_content='Permission Can view files in OneLake? Can write files in\\nOneLake?\\nCan read data through\\nSQL analytics\\nendpoint?\\nViewLogs own\\n*Depends on the SQL analytics endpoint mode.\\nYou can define and manage OneLake security roles through your lakehouse data access\\nsettings.\\nLearn more in Get started with data access roles.\\nData access to OneLake occurs in one of two ways:\\nThrough a Fabric query engine or\\nThrough user access (Queries from non-Fabric engines are considered user access)\\nOneLake security ensures that data is always kept secure. Because certain OneLake security\\nfeatures like row and column level security aren\\'t supported by storage level operations, not all\\ntypes of access to row or column level secured data can be permitted. This guarantees that\\nusers can\\'t see rows or columns they aren\\'t permitted to. Microsoft Fabric engines are enabled\\nto apply row and column level security filtering to data queries. This means when a user\\nqueries data in a lakehouse or other item with OneLake security RLS or CLS on it, the results\\nthe user sees have the hidden rows and columns removed. For user access to data in OneLake\\nwith RLS or CLS on it, the query is blocked if the user requesting access isn\\'t permitted to see\\nall the rows or columns in that table.\\nThe table below outlines which Microsoft Fabric engines support RLS and CLS filtering.\\nEngine RLS/CLS filtering Status\\nLakehouse Yes Public preview\\nSpark notebooks Yes Public preview\\nSQL Analytics Endpoint in \"user\\'s identity mode\" Yes Public preview\\nCreate roles\\nEngine and user access to data\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 251, 'page_label': '252'}, page_content=\"Engine RLS/CLS filtering Status\\nSemantic models using DirectLake on OneLake modeYes Public preview\\nEventhouse No Planned\\nData warehouse external tables No Planned\\nThis section provides details on how OneLake security roles grant access to specific scopes,\\nhow that access operates, and how access is resolved across multiple roles and access types.\\nAll OneLake tables are represented by folders in the lake, but not all folders in the lake are\\ntables from the perspective of OneLake security and query engines in Fabric. To be considered\\na valid table, the following conditions must be met:\\nThe folder exists in the Tables/ directory of an item.\\nThe folder contains a _delta_log folder with corresponding JSON files for the table\\nmetadata.\\nThe folder does not contain any child shortcuts.\\nAny tables that do not meet those criteria will have access denied if table level security is\\nconfigured on them.\\nOneLake security's Read access to data grants full access to the data and metadata in a table.\\nFor users with no access to a table, the data is never exposed and generally the metadata isn't\\nvisible. This also applies to column level security and a user's ability to see or not see a column\\nin that table. However, OneLake security doesn't guarantee that the metadata for a table won't\\nbe accessible, specifically in the following cases:\\nSQL Endpoint queries: SQL Analytics Endpoint uses the same metadata security behavior\\nas SQL Server. This means that if a user doesn't have access to a table or column, the\\nerror message for that query will explicitly state the table or column names the user\\ndoesn't have access to.\\nSemantic models: Giving a user Build permission on a semantic model allows them access\\nto see the table names included in the model, regardless of whether the user has access\\nOneLake security access control model details\\nTable level security\\nMetadata security\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 252, 'page_label': '253'}, page_content=\"to them or not. In addition, report visuals that contain hidden columns show the column\\nname in the error message.\\nFor any given folder, OneLake security permissions always inherit to the entire hierarchy of the\\nfolder's files and subfolders.\\nFor example, consider the following hierarchy of a lakehouse in OneLake:\\nBash\\nYou create two roles for this lakehouse. Role1 grants read permission to folder1, and Role2\\ngrants read permission to folder2.\\nFor the given hierarchy, OneLake security permissions for Role1 and Role2 inherit in the\\nfollowing way:\\nRole1: Read folder1\\nBash\\nRole2: Read folder2\\nBash\\nPermission inheritance\\nTables/\\n──── (empty folder)\\nFiles/\\n────folder1\\n│   │   file11.txt\\n│   │\\n│   └───subfolder11\\n│       │   file1111.txt\\n|       │\\n│       └───subfolder111\\n|            │   file1111.txt\\n│   \\n└───folder2\\n    │   file21.txt\\n│   │   file11.txt\\n│   │\\n│   └───subfolder11\\n│       │   file1111.txt\\n|       │\\n│       └───subfolder111\\n|            │   file1111.txt\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 253, 'page_label': '254'}, page_content=\"OneLake security provides automatic traversal of parent items to ensure that data is easy to\\ndiscover. Granting a user Read permissions to subfolder11 grants the user the ability to list and\\ntraverse the parent directory folder1. This functionality is similar to Windows folder permissions\\nwhere giving access to a subfolder provides discovery and traversal for the parent directories.\\nThe list and traversal granted to the parent doesn't extend to other items outside of the direct\\nparents, ensuring other folders are kept secure.\\nFor example, consider the following hierarchy of a lakehouse in OneLake.\\nBash\\nFor the given hierarchy, OneLake security permissions for 'Role1' provides the following access.\\nAccess to file11.txt isn't visible as it isn't a parent of subfolder11. Likewise for Role2, file111.txt\\nisn't visible either.\\nRole1: Read subfolder11\\nBash\\n    │   file21.txt\\nTraversal and listing in OneLake security\\nTables/\\n──── (empty folder)\\nFiles/\\n────folder1\\n│   │   file11.txt\\n│   │\\n│   └───subfolder11\\n│       │   file111.txt\\n|       │\\n│       └───subfolder111\\n|            │   file1111.txt\\n│   \\n└───folder2\\n    │   file21.txt\\nFiles/\\n────folder1\\n│   │\\n│   └───subfolder11\\n│       │   file111.txt\\n|       │\\n│       └───subfolder111\\n|            │   file1111.txt\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 254, 'page_label': '255'}, page_content=\"Role2: Read subfolder111\\nBash\\nFor shortcuts, the listing behavior is slightly different. Shortcuts to external data sources behave\\nthe same as folders do, however shortcuts to other OneLake locations have specialized\\nbehavior. The target permissions of the shortcut determine access to a OneLake shortcut.\\nWhen listing shortcuts, no call is made to check the target access. As a result, when listing a\\ndirectory all internal shortcuts are returned regardless of a user's access to the target. When a\\nuser tries to open the shortcut, the access check evaluates and a user only sees data that they\\nhave the required permissions to see. For more information on shortcuts, see the shortcuts\\nsecurity section.\\nConsider the following folder hierarchy that contains shortcuts.\\nBash\\nRole1: Read folder1\\nBash\\nRole2: No permissions defined\\nBash\\nFiles/\\n────folder1\\n│   │\\n│   └───subfolder11\\n|       │\\n│       └───subfolder111\\n|            │   file1111.txt\\nFiles/\\n────folder1\\n│   \\n└───shortcut2\\n|\\n└───shortcut3\\nFiles/\\n────folder1\\n│   \\n└───shortcut2\\n|\\n└───shortcut3\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 255, 'page_label': '256'}, page_content=\"OneLake security allows users to specify row level security by writing SQL predicates to limit\\nwhat data is shown to a user. RLS operates by showing rows where the predicate evaluates to\\ntrue. For more information, see the row level security.\\nRow level security evaluates string data as case insensitive, using the following collation for\\nsorting and comparisons: Latin1_General_100_CI_AS_KS_WS_SC_UTF8\\nWhen using row level security, ensure that the RLS statements are clean and easy to\\nunderstand. Use integer columns for sorting and greater than or less than operations. Avoid\\nstring equivalencies if you don't know the format of the input data, especially in relation to\\nunicode characters or accent sensitivity.\\nOneLake security supports limiting access to columns by removing (hiding) a user's access to a\\ncolumn. A hidden column is treated as having no permissions assigned to it, resulting in the\\ndefault policy of no access. Hidden columns won't be visible to users, and queries on data\\ncontaining hidden columns return no data for that column. As noted in metadata security there\\nare certain case where the metadata of a column might still be visible in some error messages.\\nColumn level security also follows a more strict behavior in SQL Endpoint by operating through\\na deny semantic. Deny on a column in SQL Endpoint ensures that all access to the column is\\nblocked, even if multiple roles would combine to give access to it. As a result, CLS in SQL\\nEndpoint operates using an intersection between all roles a user is part of instead of the union\\nbehavior in place for all other permission types. See the Evaluating multiple OneLake security\\nroles section for more information on how roles combine.\\nThe ReadWrite permission gives read-only users the ability to perform write operations to\\nspecific items. ReadWrite permission is only applicable for Viewers or users with the Read\\npermission on an item. Assigning ReadWrite access to an Admin, Member, or Contributor has\\nno effect as those roles already have that permission implicitly.\\nFiles/\\n│   \\n└───shortcut2\\n|\\n└───shortcut3\\nRow level security\\nColumn level security\\nReadWrite permission\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 256, 'page_label': '257'}, page_content=\"ReadWrite access enables users to perform write operations through Spark notebooks, the\\nOneLake file explorer, or OneLake APIs. Write operations through the Lakehouse UX for viewers\\nis not supported.\\nThe ReadWrite permission operates in the following ways:\\nThe ReadWrite permission includes all privileges granted by the Read permission.\\nUsers with ReadWrite permissions on an object can perform write operations on that\\nobject, inclusive. That is, any operations can also be performed on the object itself.\\nReadWrite allows the following actions:\\nCreate a new folder or table\\nDelete a folder or table\\nRename a folder or table\\nUpload or edit a file\\nCreate a shortcut\\nDelete a shortcut\\nRename a shortcut\\nOneLake security roles with ReadWrite access cannot contain RLS or CLS constraints.\\nBecause Fabric only supports single engine writes to data, users with ReadWrite\\npermission on an object can only Write to that data through OneLake. However, the Read\\noperations will be enforced consistently through all querying engines.\\nOneLake security integrates with shortcuts in OneLake to ensure data inside and outside of\\nOneLake can be easily secured. There are two main authentication modes for shortcuts:\\nPassthrough shortcuts (SSO): The credential of the querying user is evaluated against the\\nshortcut target to determine what data is allowed to be seen.\\nDelegated shortcuts: The shortcut uses a fixed credential to access the target and the\\nquerying user is evaluated against OneLake security prior to checking the delegated\\ncredential's access to the source.\\nIn addition, OneLake security permissions are evaluated when creating any shortcuts in\\nOneLake. Read about shortcut permissions in the shortcut security document.\\nShortcuts\\nShortcuts overview\\nOneLake security in passthrough shortcuts\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 257, 'page_label': '258'}, page_content=\"Security set on a OneLake folder always flows across any internal shortcuts to restrict access to\\nthe shortcut source path. When a user accesses data through a shortcut to another OneLake\\nlocation, the identity of the calling user is used to authorize access to the data in the target\\npath of the shortcut. As a result, this user must have OneLake security permissions in the target\\nlocation to read the data.\\nDefining OneLake security permissions for the internal shortcut isn't allowed and must be\\ndefined on the target folder located in the target item. The target item must be an item type\\nthat supports OneLake security roles. If the target item doesn't support OneLake security, the\\nuser's access is evaluated based on whether they have the Fabric ReadAll permission on the\\ntarget item. Users don't need Fabric Read permission on an item in order to access it through a\\nshortcut.\\nOneLake supports defining permissions for shortcuts such as ADLS, S3, and Dataverse\\nshortcuts. In this case, the permissions are applied on top of the delegated authorization\\nmodel enabled for this type of shortcut.\\nSuppose user1 creates an S3 shortcut in a lakehouse pointing to a folder in an AWS S3 bucket.\\nThen user2 is attempting to access data in this shortcut.\\nDoes S3 connection authorize\\naccess for the delegated user1?\\nDoes OneLake security authorize\\naccess for the requesting user2?\\nResult: Can user2 access\\ndata in S3 Shortcut?\\nYes Yes Yes\\nNo No No\\nNo Yes No\\nYes No No\\n） Important\\nWhen accessing shortcuts through Power BI semantic models using DirectLake over SQL\\nor T-SQL engines in Delegated identity mode, the calling user's identity isn't passed\\nthrough to the shortcut target. The calling item owner's identity is passed instead,\\ndelegating access to the calling user. To resolve this, use Power BI semantic models in\\nDirectLake over OneLake mode or T-SQL in User's identity mode.\\nOneLake security in delegated shortcuts\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 258, 'page_label': '259'}, page_content='The OneLake security permissions can be defined either for the entire scope of the shortcut or\\nfor selected subfolders. Permissions set on a folder inherit recursively to all subfolders, even if\\nthe subfolder is within the shortcut. Security set on an external shortcut can be scoped to grant\\naccess either to the entire shortcut, or any subpath inside the shortcut. Another internal\\nshortcut pointing to an external shortcut still requires the user to have access to the original\\nexternal shortcut.\\nUnlike other types of access in OneLake security, a user accessing an external shortcut requires\\nFabric Read permission on the data item where the external shortcut resides. This is necessary\\nfor securely resolving the connection to the external system.\\nLearn more about S3, ADLS, and Dataverse shortcuts in OneLake shortcuts.\\nUsers can be members of multiple different OneLake security roles, each one providing its own\\naccess to data. The combination of these roles together is called the \"effective role\" and is what\\na user will see when accessing data in OneLake. Roles combine in OneLake security using a\\nUNION or least-restrictive model. This means if Role1 gives access to TableA, and Role2 gives\\naccess to TableB, then the user will be able to see both TableA and TableB.\\nOneLake security roles also contain row and column level security, which limits access to the\\nrows and columns of a table. Each RLS and CLS policy exists within a role and limits access to\\ndata for all users within that single role. For example, if Role1 gives access to Table1, but has\\nRLS on Table1 and only shows some columns of Table1 then the effective role for Role1 is\\ngoing to be the RLS and CLS subsets of Table1. This can be expressed as (R1ols n R1cls n R1rls)\\nwhere n is the INTERSECTION of each component in the role.\\nWhen dealing with multiple roles, RLS and CLS combine with a UNION semantic on the\\nrespective tables. CLS is a direct set UNION of the tables visible in each role. RLS is combined\\nacross predicates using an OR operator. For example, WHERE city = \\'Redmond\\' OR city = \\'New\\nYork\\'.\\nTo evaluate multiple roles each with RLS or CLS, each role is first resolved based on the access\\ngiven by the role itself. This means evaluating the INTERSECTION of all object, row, and column\\nlevel security. Each evaluated role is then combined with all other roles a user is a member of\\nvia the UNION operation. The output is the effective role for that user. This can be expressed\\nas:\\n( (R1ols n R1cls n R1rls) u (R2ols n R2cls n R2rls) )\\nLastly, each shortcut in a lakehouse generates a set of inferred roles that are used to propagate\\nthe shortcut target\\'s permissions to the item being queried. Inferred roles operate in a similar\\nEvaluating multiple OneLake security roles'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 259, 'page_label': '260'}, page_content=\"way to noninferred roles except they're resolved first in place on the shortcut target before\\nbeing combined with roles in the shortcut lakehouse. This ensures that any inheritance of\\npermissions on the shortcut lakehouse is broken and the inferred roles are evaluated correctly.\\nThe full combination logic can then be expressed as:\\n( (R1ols n R1cls n R1rls) u (R2ols n R2cls n R2rls) ) n ( (R1'ols n R1'cls n R1'rls) u\\n(R2'ols n R2'cls n R2'rls)) )\\nWhere R1' and R2' are the inferred roles and R1 and R2 are the shortcut lakehouse roles.\\nIf you assign a OneLake security role to a B2B guest user, you must configure your\\nexternal collaboration settings for B2B in Microsoft Entra External ID. The Guest user\\naccess setting must be set to Guest users have the same access as members (most\\ninclusive).\\nOneLake security doesn't support cross-region shortcuts. Any attempts to access shortcut\\nto data across different capacity regions result in 404 errors.\\nIf you add a distribution list to a role in OneLake security, the SQL endpoint can't resolve\\nthe members of the list to enforce access. The result is that users appear not to be\\nmembers of the role when they access the SQL endpoint. DirectLake on SQL semantic\\nmodels are subject to this limitation too.\\nTo query data from a Spark notebook using Spark SQL, the user must have at least Viewer\\naccess in the workspace they're querying.\\nMixed-mode queries are not supported. Single queries that access both OneLake security\\nenabled and non-OneLake security enabled data will fail with query errors.\\nSpark notebooks require that the environment be 3.5 or higher and using Fabric runtime\\n1.3.\\nOneLake security doesn't work with private link protection.\\nThe external data sharing preview feature isn't compatible with the data access roles\\npreview. When you enable the data access roles preview on a lakehouse, any existing\\n） Important\\nIf two roles combine such that the columns and rows aren't aligned across the queries,\\naccess is blocked to ensure that no data is leaked to the end user.\\nOneLake security limitations\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 260, 'page_label': '261'}, page_content=\"external data shares might stop working.\\nAzure Mirrored Databricks Catalog does not support Manage Catalog functionality if\\nOneLake security is enabled on that item. This functionality is coming in November, 2025.\\nThe following table provides the limitations of OneLake data access roles.\\nScenario Limit\\nMaximum number of OneLake security roles per Fabric Item 250 roles per lakehouse\\nMaximum number of members per OneLake security role500 users or user groups per role\\nMaximum number of permissions per OneLake security role500 permissions per role\\nChanges to role definitions take about 5 minutes to apply.\\nChanges to a user group in a OneLake security role take about an hour for OneLake to\\napply the role's permissions on the updated user group.\\nSome Fabric engines have their own caching layer, so might require an extra hour to\\nupdate access in all systems.\\nLast updated on 11/18/2025\\nﾉ Expand table\\nLatencies in OneLake security\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 261, 'page_label': '262'}, page_content=\"Table and folder security in OneLake\\n(preview)\\n10/16/2025\\nTable-level and folder-level security, or object level security (OLS), is a feature of OneLake\\nsecurity (preview) that lets you grant access to specific tables or folders in a data item. Using\\nOLS you create permissions for both structured and unstructured data at the folder level.\\nAn item in Fabric with OneLake security turned on. For more information, see Get started\\nwith OneLake security.\\nSwitch the SQL analytics endpoint on the lakehouse to User's identity mode through the\\nSecurity tab.\\nFor creating semantic models, use the steps to create a DirectLake model.\\nFor a full list of limitations, see the known limitations section.\\nYou can define object-level security on any folder within a data item. Because delta-parquet\\ntables in OneLake are represented as folders, security can also be configured on tables.\\nLikewise, schemas are also folders and can be secured similarly.\\nUse the following steps to define security roles for tables or folders.\\n1. Navigate to your Lakehouse and select Manage OneLake security (preview).\\n2. Select an existing role that you want to define table or folder security for, or select New\\nto create a new role.\\n3. On the role details page, select Add data. This action opens the data browsing\\nexperience.\\nPrerequisites\\nDefine security rules\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 262, 'page_label': '263'}, page_content='4. Expand the Tables or Files directories to browse to the items you want to include in the\\nrole.\\nFor tables, you can expand schemas to choose individual tables.\\nFor files, you can expand any number of folders to identify the right items.\\n5. Select the checkbox next to the items you want to grant access to. You can select up to\\n500 items per role.\\n6. Once you have made your selection, select Add data to save your changes and return to\\nthe data in role page'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 263, 'page_label': '264'}, page_content='Your changes to the role are saved automatically.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 264, 'page_label': '265'}, page_content=\"Column-level security in OneLake (preview)\\n09/08/2025\\nColumn-level security (CLS) is a feature of OneLake security (preview) that allows you to have\\naccess to selected columns in a table instead of full access to the table. CLS lets you specify a\\nsubset of tables that users can access. Any columns that are removed from the list aren't visible\\nto users.\\nAn item in OneLake with OneLake security turned on. For more information, see Get\\nstarted with OneLake data access roles.\\nSwitch the SQL analytics endpoint on the lakehouse to User's identity mode through the\\nSecurity tab.\\nFor creating semantic models, use the steps to create a DirectLake model.\\nFor a full list of limitations, see the known limitations section.\\nOneLake security CLS gets enforced in one of the following two ways:\\nFiltered tables in Fabric engines: Queries to the Fabric engines, like Spark notebooks,\\nresult in the user seeing only the columns they're allowed to see per the CLS rules.\\nBlocked access to tables: Tables with CLS rules applied to them can't be read outside of\\nsupported Fabric engines.\\nFor filtered tables, the following behaviors apply:\\nCLS rules don't restrict access to users with the Admin, Member, and Contributor roles.\\nIf the CLS rule has a mismatch with the table it's defined on, the query fails and no\\ncolumns are returned. For example, if CLS is defined for a column that isn't part of the\\ntable.\\nQueries of CLS tables fail with an error if a user is part of two different roles and one of\\nthe roles has row-level security (RLS).\\nCLS rules can only be enforced for Delta parquet table objects.\\nCLS rules that are applied to non-Delta table objects block access to the entire table\\nfor members of the role.\\nIf a user runs a select * query on a table where they only have access to some of the\\ncolumns, CLS rules behave differently depending on the Fabric engine.\\nSpark notebooks: The query succeeds and only shows the allowed columns.\\nPrerequisites\\nEnforce column-level security\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 265, 'page_label': '266'}, page_content=\"SQL analytics Endpoint: Column access is blocked for the columns the user can't\\naccess.\\nSemantic models: Column access is blocked for the columns the user can't access.\\nYou can define column-level security as part of a OneLake security role for any Delta-parquet\\ntable in the Tables section of an item. CLS is always specified for a table and is either enabled\\nor disabled. By default, CLS is disabled and users have access to all columns. Users can enable\\nCLS and remove columns from the list to revoke access.\\nUse the following steps to define column-level security:\\n1. Navigate to your data item and select Manage OneLake security (preview).\\n2. Select an existing role that you want to define table or folder security for, or select New\\nto create a new role.\\n3. On the role details page, select more options (...) next to the table you want to define CLS\\nfor, then select Column security (preview).\\nDefine column-level security rules\\n） Important\\nRemoving access to a column doesn't deny access to that column if another role grants\\naccess to it.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 266, 'page_label': '267'}, page_content=\"4. By default, CLS for a table is disabled. Select Enable CLS or create a New rule to enable it.\\nThe UI populates with a list of columns for that table that the users are allowed to see. By\\ndefault, it shows all of the columns.\\n5. To restrict access to a column, select the checkbox next to the column name, then select\\nRemove. At least one column must remain in the list of allowed columns.\\n6. Select Save to update the role.\\n7. If you want to add a removed column, select New rule. This action adds a new CLS rule\\nentry to the end of the list. Then, use the dropdown to choose the column you want to\\ninclude in the access.\\n8. Once you complete your changes, select Save.\\nBefore you can use OneLake security with SQL analytics endpoint, you must enable its User's\\nidentity mode. Newly created SQL analytics endpoints will default to user's identity mode, so\\nthese steps must be followed for existing SQL analytics endpoints.\\nEnable OneLake security for SQL analytics endpoint\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 267, 'page_label': '268'}, page_content=\"1. Navigate to SQL analytics endpoint.\\n2. In the SQL analytics endpoint experience, select the Security tab in the top ribbon.\\n3. Select User's identity under OneLake access mode.\\n4. In the prompt, select Yes, use the user's identity.\\nNow the SQL analytics endpoint is ready to use with OneLake security.\\nRow-level and column-level security can be used together to restrict user access to a table.\\nHowever, the two policies have to be applied using a single OneLake security role. In this\\nscenario, access to data is restricted according to the rules that are set in the one role.\\nOneLake security doesn't support the combination of two or more roles where one contains\\nRLS rules and another contains CLS rules. Users that try to access tables that are part of an\\n７ Note\\nSwitching to User's identity mode only needs to be done once per SQL analytics endpoint.\\nEndpoints that are not switched to user's identity mode will continue to use a delegated\\nidentity to evaluate permissions.\\nCombine row-level and column-level security\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 268, 'page_label': '269'}, page_content='unsupported role combination receive query errors.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 269, 'page_label': '270'}, page_content='Row-level security in OneLake (preview)\\n09/08/2025\\nRow-level security (RLS) is a feature of OneLake security (preview) that allows for defining row-\\nlevel data restrictions for tabular data stored in OneLake. Users can define roles in OneLake\\nthat contain rules for filtering rows of data for members of that role. When a member of an RLS\\nrole goes to query that data, the RLS rules are evaluated and only allowed rows are returned.\\nAn item in OneLake with OneLake security turned on. For more information, see Get\\nstarted with OneLake data access roles.\\nSwitch the SQL Analytics Endpoint on the lakehouse to \"User\\'s identity\" mode through\\nthe Security tab.\\nFor creating semantic models, use the steps to create a DirectLake model.\\nFor a full list of limitations, see the known limitations section.\\nOneLake security RLS gets enforced in one of two ways:\\nFiltered tables in Fabric engines: Queries to the list of supported Fabric engines, like\\nSpark notebooks, result in the user seeing only the rows they\\'re allowed to see per the\\nRLS rules.\\nBlocked access to tables: Tables with RLS rules applied to them can\\'t be read outside of\\nsupported Fabric engines.\\nFor filtered tables, the following behaviors apply:\\nRLS rules don\\'t restrict access for users in the Admin, Member, and Contributor roles.\\nIf the RLS rule has a mismatch with the table it\\'s defined on, the query fails and no rows\\nare returned. For example, if the RLS rule references a column that isn\\'t part of the table.\\nQueries of RLS tables fail with an error if a user is part of two different roles and one of\\nthe roles has column-level security (CLS).\\nRLS rules can only be enforced for objects that are Delta parquet tables.\\nRLS rules that are applied to non-Delta table objects instead block access to the entire\\ntable for members of the role.\\nAccess to a table might be blocked if the RLS statement contains syntax errors that\\nprevent it from being evaluated.\\nPrerequisites\\nEnforce row-level security'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 270, 'page_label': '271'}, page_content='You can define row-level security rules as part of any OneLake security role that grants access\\nto table data in Delta parquet format. Rows are a concept only relevant to tabular data, so RLS\\ndefinitions aren\\'t allowed for non-table folders or unstructured data.\\nRLS rules use SQL syntax to specify the rows that a user can see. This syntax takes the form of a\\nSQL SELECT statement with the RLS rules defined in the WHERE clause. RLS rules only support a\\nsubset of the SQL language as defined in Syntax rules. Queries with invalid RLS syntax or RLS\\nsyntax that doesn\\'t match the underlying table result in no rows being shown to users, or query\\nerrors in the SQL analytics endpoint.\\nAs a best practice, avoid using vague or overly complex RLS expressions. Strongly-typed\\nexpressions with integer or string lookups with \"=\" will be the most secure and easy to\\nunderstand.\\nUse the following steps to define RLS rules:\\n1. Navigate to your Lakehouse and select Manage OneLake security (preview).\\n2. Select an existing role that you want to define table or folder security for, or select New\\nto create a new role.\\n3. On the role details page, select more options (...) next to the table you want to define RLS\\nfor, then select Row security (preview).\\nDefine row-level security rules'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 271, 'page_label': '272'}, page_content=\"4. Type the SQL statement for defining which rows you want users to see in the code editor.\\nUse the Syntax rules section for guidance.\\n5. Select Save to confirm the row security rules.\\nBefore you can use OneLake security with SQL analytics endpoint, you must enable its User's\\nidentity mode. Newly created SQL analytics endpoints will default to user's identity mode, so\\nthese steps must be followed for existing SQL analytics endpoints.\\n1. Navigate to SQL analytics endpoint.\\n2. In the SQL analytics endpoint experience, select the Security tab in the top ribbon.\\nEnable OneLake security for SQL analytics endpoint\\n７ Note\\nSwitching to User's identity mode only needs to be done once per SQL analytics endpoint.\\nEndpoints that are not switched to user's identity mode will continue to use a delegated\\nidentity to evaluate permissions.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 272, 'page_label': '273'}, page_content=\"3. Select User's identity under OneLake access mode.\\n4. In the prompt, select Yes, use the user's identity.\\nNow the SQL analytics endpoint is ready to use with OneLake security.\\nAll row-level security rules take the following form:\\nSELECT * FROM {schema_name}.{table_name} WHERE {column_level_boolean_1}\\n{column_level_boolean_2}...{column_level_boolean_N}\\nFor example:\\nSELECT * FROM Sales WHERE Amount>'50000' AND State='CA'\\nThe maximum number of characters in a row-level security rule is 1000.\\nPlaceholder Description\\n{schema_name} The name of the schema where {table_name} is located. If the artifact supports\\nschemas, then {schema_name} is required.\\nSyntax rules\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 273, 'page_label': '274'}, page_content=\"Placeholder Description\\n{table_name} The name of the table that the RLS predicate gets applied to. This value must\\nbe an exact match with the name of the table, or the RLS results in no rows\\nbeing shown.\\n{column_level_boolean}A Boolean statement containing the following components:\\n* Column name: The name of a column in {table_name} as specified in the Delta\\nlog schema. Column names can be formatted either as {column_name} or\\n{table_name}.{column_name}.\\n* Operator: One of the Supported operators that evaluates the column name\\nand value to a Boolean value.\\n* Value: A static value or set of values to be evaluated against.\\nYou can have one or more Boolean statements separated by AND or OR.\\nRow-level security rules support the following list of operators and keywords:\\nOperator Description\\n= (equals) Evaluates to true if the two values are the same data type and exact matches.\\n<> (not equals)Evaluates to true if the two values aren't the same data type and not exact matches.\\n> (greater than)Evaluates to true if the column value is greater than the evaluation value. For string\\nvalues, this operator uses bitwise comparison to determine if one string is greater\\nthan the other.\\n>= (greater than\\nor equal to)\\nEvaluates to true if the column value is greater than or equal to the evaluation value.\\nFor string values, this operator uses bitwise comparison to determine if one string is\\ngreater than or equal to the other.\\n< (less than) Evaluates to true if the column value is less than the evaluation value. For string\\nvalues, this operator uses bitwise comparison to determine if one string is less than\\nthe other.\\n<= (less than or\\nequal to)\\nEvaluates to true if the column value is less than or equal to the evaluation value. For\\nstring values, this operator uses bitwise comparison to determine if one string is less\\nthan or equal to the other.\\nIN Evaluates to true if any of the evaluation values are the same data type and exactly\\nmatch the column value.\\nSupported operators\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 274, 'page_label': '275'}, page_content=\"Operator Description\\nNOT Evaluates to true if any of the evaluation values aren't the same data type or not an\\nexact match of the column value.\\nAND Combines the previous statement and the subsequent statement using a Boolean\\nAND operation. Both statements must be true for the entire predicate to be true.\\nOR Combines the previous statement and the subsequent statement using a Boolean OR\\noperation. One of the statements must be true for the entire predicate to be true.\\nTRUE The Boolean expression for true.\\nFALSE The Boolean expression for false.\\nBLANK The blank data type, which can be used with the IS operator. For example, row IS\\nBLANK.\\nNULL The null data type, which can be used with the IS operator. For example, row IS NULL.\\nRow-level and column-level security can be used together to restrict user access to a table.\\nHowever, the two policies have to be applied using a single OneLake security role. In this\\nscenario, access to data is restricted according to the rules that are set in the one role.\\nOneLake security doesn't support the combination of two or more roles where one contains\\nRLS rules and another contains CLS rules. Users that try to access tables that are part of an\\nunsupported role combination receive query errors.\\nCombine row-level and column-level security\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 275, 'page_label': '276'}, page_content=\"Customer-managed keys for Fabric\\nworkspaces\\nMicrosoft Fabric encrypts all data-at-rest using Microsoft managed keys. With customer-\\nmanaged keys for Fabric workspaces, you can use your Azure Key Vault keys to add another\\nlayer of protection to the data in your Microsoft Fabric workspaces - including all data in\\nOneLake. A customer-managed key provides greater flexibility, allowing you to manage its\\nrotation, control access, and usage auditing. It also helps organizations meet data governance\\nneeds and comply with data protection and encryption standards.\\nAll Fabric data stores are encrypted at rest with Microsoft-managed keys. Customer-managed\\nkeys use envelope encryption, where a Key Encryption Key (KEK) encrypts a Data Encryption\\nKey (DEK). When using customer-managed keys, the Microsoft managed DEK encrypts your\\ndata, and then the DEK is encrypted using your customer-managed KEK. Use of a KEK that\\nnever leaves Key Vault allows the data encryption keys themselves to be encrypted and\\ncontrolled. This ensures that all customer content in a CMK enabled workspace is encrypted\\nusing your customer-managed keys.\\nWorkspace admins can set up encryption using CMK at the workspace level. Once the\\nworkspace admin enables the setting in the portal, all customer content stored in that\\nworkspace is encrypted using the specified CMK. CMK integrates with AKV's access policies\\nand role-based access control (RBAC), allowing you flexibility to define granular permissions\\nbased on your organization's security model. If you choose to disable CMK encryption later,\\nthe workspace will revert to using Microsoft-managed keys. You can also revoke the key at any\\ntime and access to the encrypted data will be blocked within an hour of revocation. With\\nworkspace level granularity and control, you elevate the security of your data in Fabric.\\nCustomer-managed keys are currently supported for the following Fabric items:\\nLakehouse\\nWarehouse\\nNotebook\\nHow customer-managed keys work\\nEnable encryption with customer-managed keys\\nfor your workspace\\nSupported items\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 276, 'page_label': '277'}, page_content=\"Environment\\nSpark Job Definition\\nAPI for GraphQL\\nML model\\nExperiment\\nPipeline\\nDataflow\\nIndustry solutions\\nSQL Database (preview)\\nThis feature can't be enabled for a workspace that contains unsupported items. When\\ncustomer-managed key encryption for a Fabric workspace is enabled, only supported items can\\nbe created in that workspace. To use unsupported items, create them in a different workspace\\nthat does not have this feature enabled.\\nCustomer-managed key for Fabric workspaces requires an initial setup. This setup includes\\nenabling the Fabric encryption tenant setting, configuring Azure Key Vault, and granting the\\nFabric Platform CMK app access to Azure Key Vault. Once the setup is complete, a user with an\\nadmin workspace role can enable the feature on the workspace.\\nA Fabric administrator needs to make sure that the Apply customer-managed keys setting is\\nenabled. For more information, see Encryption tenant setting article.\\nFabric uses the Fabric Platform CMK app to access your Azure Key Vault. For the app to work, a\\nservice principal must be created for the tenant. This process is performed by a user that has\\nMicrosoft Entra ID privileges, such as a Cloud Application Administrator.\\nFollow the instructions in Create an enterprise application from a multitenant application in\\nMicrosoft Entra ID to create a service principal for an application called Fabric Platform CMK\\nwith app ID 61d6811f-7544-4e75-a1e6-1c59c0383311 in your Microsoft Entra ID tenant.\\nConfigure encryption with customer-managed keys\\nfor your workspace\\nStep 1: Enable the Fabric tenant setting\\nStep 2: Create a Service Principal for the Fabric Platform CMK\\napp\\nStep 3: Configure Azure Key Vault\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 277, 'page_label': '278'}, page_content=\"You need to configure your Key Vault so that Fabric can access it. This step is performed by a\\nuser that has Key Vault privileges, such as a Key Vault Administrator. For more information, see\\nAzure Security roles.\\n1. Open the Azure portal and navigate to your Key Vault. If you don't have Key Vault, follow\\nthe instructions in Create a key vault using the Azure portal.\\n2. In your Key Vault, configure the following settings:\\nSoft delete - Enabled\\nPurge protection - Enabled\\n3. In your Key Vault, open Access control (IAM).\\n4. From the Add dropdown, select Add Role assignment.\\n5. Select the Members tab and then click on Select members.\\n6. In the Select members pane, search for Fabric Platform CMK\\n7. Select the Fabric Platform CMK app and then Select.\\n8. Select the Role tab and search for Key Vault Crypto Service Encryption User or a role that\\nenables get, wrapkey, and unwrap key permissions.\\n9. Select Key Vault Crypto Service Encryption User.\\n10. Select Review + assign and then select Review + assign to confirm your choice.\\nTo create an Azure Key Vault key, follow the instructions in Create a key vault using the Azure\\nportal.\\nFabric only supports versionless customer-managed keys, which are keys in the format\\nhttps://{vault-name}.vault.azure.net/{key-type}/{key-name} for Vaults and https://{hsm-\\nname}.managedhsm.azure.net/{key-type}/{key-name} for Managed HSM. Fabric checks the key\\nvault daily for a new version, and uses the latest version available. To avoid having a period\\nwhere you can't access data in the workspace after a new key is created, wait 24 hours before\\ndisabling the older version.\\nKey Vault and Managed HSM must have both soft-delete and purge protection enabled and\\nthe key must be of RSA or RSA-HSM type. The supported key sizes are:\\nStep 4: Create an Azure Key Vault key\\nKey Vault requirements\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 278, 'page_label': '279'}, page_content=\"2,048 bit\\n3,072 bit\\n4,096 bit\\nFor more information, see About keys.\\nYou can also use Azure Key Vaults for which the firewall setting is enabled. When you disable\\npublic access to the Key Vault, you can choose the option to 'Allow Trusted Microsoft Services\\nto bypass this firewall.'\\nAfter completing the prerequisites, follow the steps in this section to enable customer-\\nmanaged keys in your Fabric workspace.\\n1. From your Fabric workspace, select Workspace settings.\\n2. From the Workspace settings pane, select Encryption.\\n3. Enable Apply customer-managed keys.\\n4. In the Key identifier field, enter your customer-managed key identifier.\\n5. Select Apply.\\nOnce you complete these steps, your workspace is encrypted with a customer-managed key.\\nThis means that all data in Onelake is encrypted and that existing and future items in the\\nworkspace will be encrypted by the customer-managed key you used for the setup. You can\\nreview the encryption status Active, In progress or Failed in the Encryption tab in workspace\\nsettings. Items for which encryption is in progress or failed are listed categorically too. The key\\nneeds to remain active in the Key Vault while encryption is in progress (Status: In progress).\\nRefresh the page to view the latest encryption status. If encryption has failed for some items in\\nthe workspace, you can retry using a different key.\\nTo revoke access to data in a workspace that's encrypted using a customer-managed key,\\nrevoke the key in the Azure Key Vault. Within 60 minutes from the time the key is revoked, read\\nand write calls to the workspace fail.\\n７  Note\\n4,096 bit keys are not supported for SQL database in Microsoft Fabric.\\nStep 5: Enable encryption using customer-managed keys\\nRevoke access\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 279, 'page_label': '280'}, page_content=\"You can revoke a customer-managed encryption key by changing the access policy, by\\nchanging the permissions on the key vault, or by deleting the key.\\nTo reinstate access, restore access to the customer-managed key in the Key Vault.\\nTo disable encrypting the workspace using a customer-managed key, go to Workspace settings\\ndisable Apply customer-managed keys. The workspace remains encrypted using Microsoft\\nManaged keys.\\nYou can track encryption configuration requests for your Fabric workspaces by audit log\\nentries. The following operation names are used in audit logs:\\nApplyWorkspaceEncryption\\nDisableWorkspaceEncryption\\nGetWorkspaceEncryption\\nBefore you configure your Fabric workspace with a customer-managed key, consider the\\nfollowing limitations:\\nThe data listed below isn't protected with customer-managed keys:\\nLakehouse column names, table format, table compression.\\nAll data stored in the Spark Clusters (data stored in temp discs as part of shuffle or\\ndata spills or RDD caches in a spark application) are not protected. This includes all the\\n７  Note\\nThe workspace does not automatically revalidate the key for SQL Database in Microsoft\\nFabric. Instead, the user must manually revalidate the CMK to restore access.\\nDisable the encryption\\n７  Note\\nYou can't disable customer-managed keys while encryption for any of the Fabric items in\\nyour workspace is in progress.\\nMonitoring\\nConsiderations and limitations\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 280, 'page_label': '281'}, page_content='Spark Jobs from Notebooks, Lakehouses, Spark Job Definitions, Lakehouse Table Load\\nand Maintenance jobs, Shortcut Transforms, Fabric Materialized View Refresh.\\nThe job logs stored in the history server\\nLibraries attached as part of environments or added as part of the Spark session\\ncustomization using magic commands are not protected\\nMetadata generated when creating a Pipeline and Copy job, such as DB name, table,\\nschema\\nMetadata of ML model and experiment, like the model name, version, metrics\\nWarehouse queries on Object Explored and backend cache, which is evicted after each\\nuse\\nCMK is supported on all F SKUs. Trial capacities cannot be used for encryption using CMK.\\nCMK cannot be enabled for workspaces that have BYOK enabled and CMK workspaces\\ncannot be moved to capacities for which BYOK is enabled either.\\nCMK can be enabled using the Fabric portal and does not have API support.\\nCMK can be enabled and disabled for the workspace while the tenant level encryption\\nsetting is on. Once the tenant setting is turned off, you can no longer enable CMK for\\nworkspaces in that tenant or disable CMK for workspaces that already have CMK turned\\non in that tenant. Data in workspaces that enabled CMK before the tenant setting was\\nturned off will remain encrypted with the customer managed key. Keep the associated\\nkey active to be able to unwrap data in that workspace.\\nSecurity fundamentals\\nMicrosoft Fabric licenses\\nLast updated on 11/18/2025\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 281, 'page_label': '282'}, page_content=\"What is a OneLake shared access signature\\n(SAS)?\\nArticle• 04/11/2025\\nA OneLake shared access signature (SAS) provides secure, short-term, and delegated access to\\nresources in your OneLake. With a OneLake SAS, you have granular control over how a client\\ncan access your data. For example:\\nWhat resources the client can access.\\nWhat permissions they have to the resources.\\nHow long the SAS is valid.\\nEvery OneLake SAS (and user delegation key) is always backed by a Microsoft Entra identity,\\nhas a maximum lifetime of 1 hour, and can only grant access to folders and files within a data\\nitem, like a lakehouse.\\nA shared access signature is a token appended to the URI for a OneLake resource. The token\\ncontains a special set of query parameters that indicate how the client can access the resource.\\nOne of the query parameters is the signature. It's constructed from the SAS parameters and\\nsigned with the key that was used to create the SAS. OneLake uses this signature to authorize\\naccess to the folder or file in OneLake. OneLake SAS uses the same format and properties as\\nAzure Storage user-delegated SAS, but with more security restrictions on the lifetime and\\nscope.\\nA OneLake SAS is signed with a user delegation key (UDK), which is backed by a Microsoft\\nEntra credential. You can request a user delegation key with the Get User Delegation Key\\noperation. Then, you use this key (while it's still valid) to build the OneLake SAS. The\\npermissions of that Microsoft Entra credential, along with the permissions explicitly granted to\\nthe SAS, determine the client's access to the resource.\\nWhen a client or application accesses OneLake with a OneLake SAS, the request is authorized\\nusing the Microsoft Entra credentials that requested the UDK used to create the SAS.\\nTherefore, all OneLake permissions granted to that Microsoft Entra identity apply to the SAS,\\nmeaning a SAS can never exceed the permissions of the user creating it. Furthermore, when\\ncreating a SAS you explicitly grant permissions, letting you provide even more scoped-down\\npermissions to the SAS. Between the Microsoft Entra identity, the explicitly granted\\nHow a shared access signature works\\nAuthorizing a OneLake SAS\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 282, 'page_label': '283'}, page_content=\"permissions, and the short-lifetime, OneLake follows security best practices for providing\\ndelegated access to your data.\\nOneLake SAS delegates secure and temporary access to OneLake, backed by a Microsoft Entra\\nidentity. Applications without native Microsoft Entra support can use a OneLake SAS to gain\\ntemporary access to load data without complicated set-up and integration work.\\nOneLake SAS also supports applications serving as proxies between users and their data. For\\nexample, some independent software vendors (ISVs) run between users and their Fabric\\nworkspace, providing extra functionality and possibly a different authentication model. By\\ndelegating access with a OneLake SAS, these ISVs can manage access to the underlying data\\nand provide direct access to data, even if their users don't have Microsoft Entra identities.\\nTwo settings in your Fabric tenant manage the use of OneLake SAS.\\nThe first setting is a tenant-level setting, Use short-lived user-delegated SAS tokens, which\\nmanages the generation of user delegation keys. Because user delegation keys are generated\\nat the tenant-level, they're controlled by a tenant setting. This setting is turned on by default,\\nsince these user delegation keys have equivalent permissions to the Microsoft Entra identity\\nrequesting them and are always short-lived.\\nThe second setting is a delegated workspace setting, Authenticate with OneLake user-\\ndelegated SAS tokens, which controls whether a workspace accepts OneLake SAS. This setting\\nis turned off by default. A workspace admin can turn on this setting to allow authentication\\nwith OneLake SAS in their workspace. A tenant admin can turn this setting on for all\\nworkspaces via the tenant setting, or leave it to workspace admins to turn on.\\nYou can also monitor the creation of user delegation keys in the Microsoft Purview portal. To\\nview all keys generated in your tenant, search for the operation name generateonelakeudk.\\nBecause creating a SAS is a client-side operation, you can't monitor or limit the creation of a\\nOneLake SAS, only the generation of a UDK.\\nWhen to use a OneLake SAS\\nManaging OneLake SAS\\n７  Note\\nTurning off this feature prevents all workspaces from using OneLake SAS, as all users will\\nbe unable to generate user delegation keys.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 283, 'page_label': '284'}, page_content=\"Always use HTTPS to create or distribute a SAS to protect against man-in-the-middle\\nattacks seeking to intercept the SAS.\\nTrack your, key, and SAS token expiry times. OneLake user delegation keys and SAS\\ntokens have a maximum lifetime of 1 hour. Attempting to request a UDK or build a SAS\\nwith a lifetime longer than 1 hour causes the request to fail. To prevent SAS being used to\\nextend the lifetime of expiring OAuth tokens, the lifetime of the token must also be\\nlonger than the expiry time of the user delegation key and the SAS.\\nBe careful with a SAS token's start time. Setting the start time for a SAS as the current\\ntime might cause failures for the first few minutes, due to differing start times between\\nmachines (clock skew). Setting the start time to be a few minutes in the past helps protect\\nagainst these errors.\\nGrant the least possible privileges to the SAS. Providing the minimum required privileges\\nto the fewest possible resources is a security best-practice and lessens the impact if a SAS\\nis compromised.\\nMonitor the generation of user delegation keys. You can audit the creation of user\\ndelegation keys in the Microsoft Purview portal. Search for the operation name\\ngenerateonelakeudk to view keys generated in your tenant.\\nUnderstand the limitations of OneLake SAS. Because OneLake SAS tokens can't have\\nworkspace-level permissions, they aren't compatible with some Azure Storage tools which\\nexpect container-level permissions to traverse data, like Azure Storage Explorer.\\nHow to create a OneLake SAS\\nGenerate a user delegation key\\nFabric and OneLake data security\\nCreate a user delegation SAS for a blob with Python\\nBest practices with OneLake SAS\\nRelated content\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 284, 'page_label': '285'}, page_content=\"Create a OneLake shared access signature\\nArticle• 05/20/2025\\nYou can create a OneLake shared access signature (SAS) to provide short-term, delegated\\naccess to a folder or file in OneLake backed by your Microsoft Entra credentials. A OneLake SAS\\ncan provide temporary access to applications that don't support Microsoft Entra. These\\napplications can then load data or serve as proxies between other customer applications or\\nsoftware development companies.\\nTo create a OneLake SAS, request a user delegation key by calling the Get User Delegation Key\\noperation. Then, use the key to sign the SAS.\\nA OneLake SAS can grant access to files and folders within data items only. You can't use it for\\nmanagement operations such as creating or deleting workspaces or items.\\nCreating a OneLake SAS is similar to creating an Azure Storage user-delegated SAS. You use\\nthe same parameters for compatibility with tools and applications that work with Azure\\nStorage.\\nRequesting a user delegation key is a tenant-level operation in Microsoft Fabric. The user or\\nsecurity principal who requests a user delegation key must have at least read permissions in\\none workspace in the Fabric tenant. The requesting user's identity is used to authenticate the\\nSAS, so the user must have permission to the data that they grant the SAS access to.\\nTo get the user delegation key, first request an OAuth 2.0 token from Microsoft Entra ID.\\nAuthorize the call to the Get User Delegation Key operation by providing the bearer token. For\\nmore information about requesting an OAuth token from Microsoft Entra ID, see the article\\nabout authentication flows and application scenarios.\\nCalling the Get User Delegation Key operation returns the key as a set of values that are used\\nas parameters on the user delegation SAS token. These parameters are described in the Get\\nUser Delegation Key reference and in the next section.\\nRequired permissions\\nAcquire an OAuth 2.0 token\\nRequest the user delegation key\\n７ Note\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 285, 'page_label': '286'}, page_content=\"When a client requests a user delegation key by using an OAuth 2.0 token, OneLake returns the\\nkey on behalf of the client. A SAS created with this user delegation key is granted, at most, the\\npermissions granted to the client. But they're scoped down to the permissions explicitly\\ngranted in the SAS.\\nYou can create any number of OneLake SAS tokens for the lifetime of the user delegation key.\\nHowever, a OneLake SAS and user delegation keys can be valid for no more than one hour.\\nThey can't exceed the lifetime of the token that requested them. These lifetime restrictions are\\nshorter than the maximum lifetime of an Azure Storage user delegation SAS.\\nThe following table summarizes the fields that are supported for a OneLake SAS token.\\nSubsequent sections provide more details about these parameters and how they differ from\\nAzure Storage SAS tokens. OneLake doesn't support every optional parameter that Azure\\nStorage supports. A OneLake SAS constructed with an unsupported parameter will be rejected.\\nSAS field name SAS token\\nparameter\\nStatus Description\\nsignedVersion sv Required This field indicates the version of the\\nstorage service that's used to construct the\\nsignature field. OneLake supports version\\n2020-02-10 and earlier, or version 2020-12-\\n06 and later.\\nsignedResource sr Required This field specifies which resources are\\naccessible via the shared access signature.\\nOnly blob (b) and directory (d) are\\napplicable to OneLake.\\nsignedStart st Optional This field specifies the time when the\\nshared access signature becomes valid. It's\\nin ISO 8601 UTC format.\\nsignedExpiry se Required This field specifies the time when the\\nshared access signature expires.\\nCalling the Get User Delegation Key operation by using a Fabric workload, such as a\\nPython notebook, requires the regional endpoint for OneLake. The capacity region\\ndetermines this endpoint. Otherwise, the received response is 200 Healthy instead of the\\ndelegation key.\\nConstruct a user delegation SAS\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 286, 'page_label': '287'}, page_content=\"SAS field name SAS token\\nparameter\\nStatus Description\\nsignedPermissions sp Required This field indicates which operations the\\nSAS can perform on the resource. For more\\ninformation, see the Specify permissions\\nsection.\\nsignedObjectId skoid Required This field identifies a Microsoft Entra\\nsecurity principal.\\nsignedtenantId sktid Required This field specifies the Microsoft Entra\\ntenant in which a security principal is\\ndefined.\\nsignedKeyStartTime skt Optional This field specifies the time in UTC when\\nthe signing key starts. The Get User\\nDelegation Key operation returns it.\\nsignedKeyExpiryTime ske Required This field specifies the time in UTC when\\nthe signing key ends. The Get User\\nDelegation Key operation returns it.\\nsignedKeyVersion skv Required This field specifies the storage service\\nversion that's used to get the user\\ndelegation key. The Get User Delegation\\nKey operation returns it. OneLake supports\\nversion 2020-02-10 and earlier, or version\\n2020-12-06 and later.\\nsignedKeyService sks Required This field indicates the valid service for the\\nuser delegation key. OneLake supports\\nonly Azure Blob Storage (sks=b).\\nsignature sig Required The signature is a hash-based message\\nauthentication code (HMAC) computed\\nover the string-to-sign and key by using\\nthe SHA256 algorithm, and then encoded\\nby using Base64 encoding.\\nsignedDirectoryDepth sdd Optional This field indicates the number of\\ndirectories within the root folder of the\\ndirectory specified in the\\ncanonicalizedResource field of the string-\\nto-sign. It's supported only when sr=d.\\nsignedProtocol spr Optional OneLake supports only HTTPS requests.\\nsignedAuthorizedObjectId saoid UnsupportedA OneLake SAS doesn't support this\\nfeature.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 287, 'page_label': '288'}, page_content=\"SAS field name SAS token\\nparameter\\nStatus Description\\nsignedUnauthorizedObjectId suoid UnsupportedA OneLake SAS doesn't support this\\nfeature.\\nsignedCorrelationId suoid UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nsignedEncryptionScope ses UnsupportedA OneLake SAS doesn't currently support\\ncustom encryption scopes.\\nsignedIP sip UnsupportedA OneLake SAS doesn't currently support\\nIP filtering.\\nCache-Control response\\nheader\\nrscc UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nContent-Disposition\\nresponse header\\nrscd UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nContent-Encoding response\\nheader\\nrsce UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nContent-Language response\\nheader\\nrscl UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nContent Type response\\nheader\\nrsct UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nThe permissions specified in the signedPermissions (sp) field on the SAS token indicate which\\noperations a client that possesses the SAS can perform on the resource.\\nPermissions can be combined to permit a client to perform multiple operations with the same\\nSAS. When you construct the SAS, you must include permissions in the following order:\\nracwdxyltmeopi.\\nExamples of valid permission settings include rw, rd, rl, wd, wl, and rl. You can't specify a\\npermission more than once.\\nTo ensure parity with existing Azure Storage tools, OneLake uses the same permission format\\nas Azure Storage. OneLake evaluates the permissions granted to a SAS in signedPermissions,\\nthe permissions of the signing identity in Fabric, and any OneLake data access roles, if\\napplicable.\\nSpecify permissions\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 288, 'page_label': '289'}, page_content=\"Remember that some operations, such as setting permissions or deleting workspaces, generally\\naren't permitted on OneLake via Azure Storage APIs. Granting that permission (sp=op) doesn't\\nallow a OneLake SAS to perform those operations.\\nPermission URI\\nsymbol\\nResource Allowed operations\\nRead r Directory,\\nblob\\nRead the content, blocklist, properties, and metadata of any\\nblob in the container or directory. Use a blob as the source\\nof a copy operation.\\nAdd a Directory,\\nblob\\nAdd a block to an append blob.\\nCreate c Directory,\\nblob\\nWrite a new blob, snapshot a blob, or copy a blob to a new\\nblob.\\nWrite w Directory,\\nblob\\nCreate or write content, properties, metadata, or a blocklist.\\nSnapshot or lease the blob. Use the blob as the destination\\nof a copy operation.\\nDelete d Directory,\\nblob\\nDelete a blob.\\nDelete versionx Blob Delete a blob version.\\nPermanent\\ndelete\\ny Blob Permanently delete a blob snapshot or version.\\nList l Directory List blobs nonrecursively.\\nTags t Blob Read or write the tags on a blob.\\nMove m Directory,\\nblob\\nMove a blob or a directory and its contents to a new\\nlocation.\\nExecute e Directory,\\nblob\\nGet the system properties. If the hierarchical namespace is\\nenabled for the storage account, get the POSIX access\\ncontrol list of a blob.\\nOwnership o Directory,\\nblob\\nSet the owner or owning group. This operation is\\nunsupported in OneLake.\\nPermissions p Directory,\\nblob\\nSet the permissions. This operation is unsupported in\\nOneLake.\\nSet immutability\\npolicy\\ni Blob Set or delete the immutability policy or legal hold on a blob.\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 289, 'page_label': '290'}, page_content='The signature (sig) field is used to authorize a request that a client made with the shared\\naccess signature. The string-to-sign is a unique string that\\'s constructed from the fields that\\nmust be verified to authorize the request. The signature is an HMAC that\\'s computed over the\\nstring-to-sign and key by using the SHA256 algorithm, and then encoded by using Base64\\nencoding.\\nTo construct the signature string of a user delegation SAS:\\n1. Create the string-to-sign from the fields that the request made.\\n2. Encode the string as UTF-8.\\n3. Compute the signature by using the HMAC SHA256 algorithm.\\nThe fields that are included in the string-to-sign must be URL decoded. The required fields\\ndepend on the service version that\\'s used for the authorization (sv) field. The following\\nsections describe the string-to-sign configurations for versions that support OneLake SASs.\\nHTTP\\nSpecify the signature\\nVersion 2020-12-06 and later\\nStringToSign =  signedPermissions + \"\\\\n\" +\\n                signedStart + \"\\\\n\" +\\n                signedExpiry + \"\\\\n\" +\\n                canonicalizedResource + \"\\\\n\" +\\n                signedKeyObjectId + \"\\\\n\" +\\n                signedKeyTenantId + \"\\\\n\" +\\n                signedKeyStart + \"\\\\n\" +\\n                signedKeyExpiry  + \"\\\\n\" +\\n                signedKeyService + \"\\\\n\" +\\n                signedKeyVersion + \"\\\\n\" +\\n                signedAuthorizedUserObjectId + \"\\\\n\" +\\n                signedUnauthorizedUserObjectId + \"\\\\n\" +\\n                signedCorrelationId + \"\\\\n\" +\\n                signedIP + \"\\\\n\" +\\n                signedProtocol + \"\\\\n\" +\\n                signedVersion + \"\\\\n\" +\\n                signedResource + \"\\\\n\" +\\n                signedSnapshotTime + \"\\\\n\" +\\n                signedEncryptionScope + \"\\\\n\" +\\n                rscc + \"\\\\n\" +\\n                rscd + \"\\\\n\" +\\n                rsce + \"\\\\n\" +\\n                rscl + \"\\\\n\" +\\n                rsct'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 290, 'page_label': '291'}, page_content='This configuration applies to version 2020-02-10 and earlier, except for version 2020-01-10\\n(which the next section describes).\\nHTTP\\nHTTP\\nVersion 2020-02-10 and earlier\\nStringToSign =  signedPermissions + \"\\\\n\" +  \\n                signedStart + \"\\\\n\" +  \\n                signedExpiry + \"\\\\n\" +  \\n                canonicalizedResource + \"\\\\n\" +  \\n                signedKeyObjectId + \"\\\\n\" +\\n                signedKeyTenantId + \"\\\\n\" +\\n                signedKeyStart + \"\\\\n\" +\\n                signedKeyExpiry  + \"\\\\n\" +\\n                signedKeyService + \"\\\\n\" +\\n                signedKeyVersion + \"\\\\n\" +\\n                signedAuthorizedUserObjectId + \"\\\\n\" +\\n                signedUnauthorizedUserObjectId + \"\\\\n\" +\\n                signedCorrelationId + \"\\\\n\" +\\n                signedIP + \"\\\\n\" +  \\n                signedProtocol + \"\\\\n\" +  \\n                signedVersion + \"\\\\n\" +  \\n                signedResource + \"\\\\n\" +\\n                rscc + \"\\\\n\" +\\n                rscd + \"\\\\n\" +  \\n                rsce + \"\\\\n\" +  \\n                rscl + \"\\\\n\" +  \\n                rsct\\nVersion 2020-01-10\\nStringToSign =  signedPermissions + \"\\\\n\" +\\n                signedStart + \"\\\\n\" +\\n                signedExpiry + \"\\\\n\" +\\n                canonicalizedResource + \"\\\\n\" +\\n                signedKeyObjectId + \"\\\\n\" +\\n                signedKeyTenantId + \"\\\\n\" +\\n                signedKeyStart + \"\\\\n\" +\\n                signedKeyExpiry  + \"\\\\n\" +\\n                signedKeyService + \"\\\\n\" +\\n                signedKeyVersion + \"\\\\n\" +\\n                signedAuthorizedUserObjectId + \"\\\\n\" +\\n                signedUnauthorizedUserObjectId + \"\\\\n\" +\\n                signedCorrelationId + \"\\\\n\" +\\n                signedIP + \"\\\\n\" +\\n                signedProtocol + \"\\\\n\" +\\n                signedVersion + \"\\\\n\" +'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 291, 'page_label': '292'}, page_content='The canonicalizedResource portion of the string is a canonical path to the resource. It must\\ninclude the OneLake endpoint and the resource name, and it must be URL decoded. A OneLake\\npath must include its workspace. A directory path must include the number of subdirectories\\nthat correspond to the sdd parameter.\\nThe following examples show how to convert your OneLake URL to the corresponding\\ncanonicalized resource. Remember that OneLake supports both Distributed File System (DFS)\\nand blob operations and endpoints. The account name for OneLake is always onelake.\\nHTTP\\nHTTP\\nThe following example shows a OneLake SAS URI with a OneLake SAS token appended to it.\\nThe SAS token provides read and write permissions to the Files folder in the lakehouse.\\n                signedResource + \"\\\\n\" +\\n                signedSnapshotTime + \"\\\\n\" +\\n                rscc + \"\\\\n\" +\\n                rscd + \"\\\\n\" +\\n                rsce + \"\\\\n\" +\\n                rscl + \"\\\\n\" +\\n                rsct\\nCanonicalized resource\\nBlob file\\nURL = \\nhttps://onelake.blob.fabric.microsoft.com/myWorkspace/myLakehouse.Lakehouse/Files/\\nsales.csv\\ncanonicalizedResource = \\n\"/blob/onelake/myWorkspace/myLakehouse.Lakehouse/Files/sales.csv\"\\nDFS directory\\nURL = \\nhttps://onelake.dfs.fabric.microsoft.com/myWorkspace/myLakehouse.Lakehouse/Files/\\ncanonicalizedResource = \"/blob/onelake/myWorkspace/myLakehouse.Lakehouse/Files/\"\\nOneLake SAS example'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 292, 'page_label': '293'}, page_content='HTTP\\nCreate a user delegation SAS\\nRequest a user delegation key\\nGet started with OneLake data access roles\\nhttps://onelake.blob.fabric.microsoft.com/myWorkspace/myLakehouse.Lakehouse/Files/\\n?sp=rw&st=2023-05-24T01:13:55Z&se=2023-05-24T09:13:55Z&skoid=<object-id>&sktid=\\n<tenant-id>&skt=2023-05-24T01:13:55Z&ske=2023-05-24T09:13:55Z&sks=b&skv=2022-11-\\n02&sv=2022-11-02&sr=d&sig=<signature>\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 293, 'page_label': '294'}, page_content='OneLake diagnostics\\nOneLake diagnostics provides end-to-end visibility into how data is accessed and used across\\nyour Microsoft Fabric environment. It enables organizations to answer critical questions like\\n\"who accessed what, when, and how,\" supporting data governance, operational insight, and\\ncompliance reporting.\\nWhen enabled at the workspace level, OneLake diagnostics streams data access events as\\nJSON logs into a Lakehouse of your choice within the same capacity. These logs can be easily\\ntransformed into analytics-ready Delta tables, allowing teams to build dashboards and reports\\nthat track usage patterns, top-accessed items, and trends over time.\\nAs all data in Fabric is unified in OneLake, diagnostics at the workspace level provide a\\nconsistent, trustworthy record of data activity—regardless of how or where the data is\\nconsumed. This includes:\\nUser actions in the Fabric web experience\\nProgrammatic access via APIs, pipelines, and analytics engines\\nCross-workspace shortcuts, with events captured from the source workspace\\nThis unified logging approach ensures that even when data is accessed through shortcuts or\\nacross workspaces, visibility is preserved.\\nDiagnostic events are captured for both Fabric and non-Fabric sources. For access through the\\nFabric UI and the Blob or Azure Data Lake Storage (ADLS) APIs, every operation is logged. For\\nFabric workloads access, it records that temporary access was granted, so you can look further\\nin the engine specific logs. This ensures efficient logging while maintaining visibility into how\\ndata is consumed across your organization.\\nSecurity investigation: Track which users accessed sensitive datasets, when, and from\\nwhere. Helps identify unauthorized access attempts or unusual patterns.\\nPerformance troubleshooting: Diagnose latency or failure issues by correlating diagnostic\\nevents with user actions or system interactions.\\nUsage analytics and optimization: Understand which datasets are most frequently\\naccessed, by whom, and how often. Supports data governance and resource optimization.\\nIntegration monitoring: Monitor external systems interacting with OneLake (via APIs or\\nconnectors), ensuring integrations are functioning as expected and diagnosing issues\\nwhen they arise.\\nExample scenarios supported by OneLake\\ndiagnostics'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 294, 'page_label': '295'}, page_content='To simplify management and improve access control, consider using a dedicated workspace to\\nstore diagnostic events. If you\\'re enabling diagnostics across multiple workspaces in the same\\ncapacity, consider centralizing logs in a single Lakehouse to make analysis easier.\\nCreate a Lakehouse to store OneLake diagnostic events.\\nThe Lakehouse must reside in the same capacity as the workspaces you want to enable\\ndiagnostics for.\\nIf the workspace uses private links for inbound network protection, it must be within the\\nsame virtual network as the Lakehouse.\\nYou must be a workspace admin for the workspace where you\\'re enabling OneLake\\ndiagnostics, and a contributor to the destination Lakehouse.\\nUse the following steps to enable OneLake diagnostics:\\n1. Open the workspace settings.\\n2. Navigate to the OneLake settings tab.\\n3. Toggle \"Add diagnostic events to a Lakehouse\" to On.\\nConfiguring OneLake diagnostics\\nBest practice recommendations\\nPrerequisites\\nEnabling OneLake diagnostics\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 295, 'page_label': '296'}, page_content=\"4. Select the Lakehouse where you want to store the diagnostic events.\\nOneLake diagnostic events can be made immutable, this means that the JSON files that contain\\ndiagnostic events can't be tampered with, or deleted, during the immutability retention period.\\nOneLake diagnostics immutability is built on the Immutable storage for Azure Blob Storage\\ncapability. For more information, please read Store business-critical blob data with immutable\\nstorage in a write once, read many (WORM) state\\nThe immutability period is configured on the workspace that contains diagnostic events. To\\nconfigure the immutability period, you must have previously configured a workspace to store\\ndiagnostic events in this workspace. The immutability period applies to all events stored in this\\nworkspace.\\n1. Enter the required immutability period\\n2. Press apply\\n７ Note\\nIt takes up to 1 hour for diagnostic events to begin flowing into the Lakehouse.\\nEnabling immutable diagnostic logs\\n\\uf80a\\n７ Note\\nOnce the immutability policy is applied, the files can't be modified or deleted until the\\nimmutability retention period passes. Use caution while applying the policy as it can't be\\nchanged once set.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 296, 'page_label': '297'}, page_content='1. Open the workspace settings.\\n2. Go to the OneLake settings tab.\\n3. Select Replace Lakehouse.\\n4. Choose a new Lakehouse.\\n1. Open the workspace settings.\\n2. Navigate to the OneLake settings tab.\\n3. Toggle \"Add diagnostic events to a Lakehouse\" to Off.\\nChanging the OneLake diagnostic Lakehouse\\n７ Note\\nPreviously captured diagnostic events remain in the original Lakehouse. New events are\\nstored in the newly selected Lakehouse.\\nDisabling OneLake diagnostics\\n７ Note\\nThe previously selected Lakehouse is retained. If you re-enable diagnostics, it uses the\\nsame Lakehouse as before.\\nOneLake diagnostic events\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 297, 'page_label': '298'}, page_content='OneLake diagnostic events are stored in the DiagnosticLogs folder within the Files section of a\\nLakehouse. JSON files are written to a folder with the following path:\\nFiles/DiagnosticLogs/OneLake/Workspaces/WorkspaceId/y=YYYY/m=MM/d=DD/h=HH/m=00/PT1H.json\\nThe JSON event contains the following attributes:\\nProperty Description\\nworkspaceId The GUID of the workspace with diagnostics enabled.\\nitemId The GUID of the fabric item, for example the Lakehouse, which is performing the\\nOneLake operation\\nitemType The kind of item that performed the OneLake operation\\ntenantId The tenant identifier that performed the OneLake operation\\nexecutingPrincipalIdThe GUID of the Microsoft Entra principle performing the OneLake operation\\ncorrelationId A GUID correlation identifier for the OneLake operation\\noperationName The OneLake operation being performed (not provided for internal Fabric\\noperations). See Operations below for more details.\\noperationCategory The broad category of the OneLake operation (for example, Read)\\nexecutingUPN The Microsoft Entra unique principal name performing the operation (not\\nprovided for internal Fabric operations)\\nexecutingPrincipalType The type of principal being used, for example User or Service Principal\\naccessStartTime The time the operation was performed. When temporary access is provided, the\\ntime temporary access started\\naccessEndTime The time the operation was completed. When temporary access is provided, the\\ntime temporary access completed\\noriginatingApp The workload performing the operation. For external access, then\\noriginatingApp is the user agent string\\nserviceEndpoint The OneLake service endpoint being used (DFS, Blob or Other)\\nResource The resources being accessed (relative to the workspace)\\ncapacityId The identifier of the capacity performing the OneLake operation\\nhttpStatusCode The status code returned to the user\\nisShortcut Indicates if access was performed via a shortcut\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 298, 'page_label': '299'}, page_content='Property Description\\naccessedViaResource The resource the data was accessed via. When a shortcut is used, this is the\\nlocation of the shortcut\\ncallerIPAddress The IP address of the caller\\nOneLake diagnostic events include executingUPN and callerIpAddress. To redact this data,\\ntenant admins can disable the setting \"Include end-user identifiers in OneLake diagnostic logs\"\\nin the Fabric Admin Portal. When disabled, these fields are excluded from new diagnostic\\nevents.\\nIf the Lakehouse selected for diagnostics is deleted:\\nDiagnostics will be automatically disabled for all workspaces that were pointing to it.\\nPreviously captured diagnostic data is not deleted—it remains in the deleted\\nLakehouse\\'s storage until the workspace itself is deleted. To resume diagnostics, select a\\nnew Lakehouse in the same workspace. OneLake will enable diagnostics, and all\\npreviously captured logs remain accessible.\\nIf a workspace is deleted, OneLake diagnostics for that workspace are also deleted.\\nIf the workspace is restored, the diagnostic data is restored.\\nOnce the workspace is permanently deleted, the associated diagnostic events are also\\npermanently removed.\\nWhen a workspace is moved to a different capacity, diagnostic logging is disabled.\\nYou must select a new Lakehouse within the new capacity to re-enable diagnostics.\\nPersonal Data\\nFrequently Asked Questions (FAQ)\\nWhat happens if the destination Lakehouse is deleted?\\nWhat happens if the workspace is deleted?\\nWhat happens when you change capacities?\\nWhat happens when BCDR is enabled for the workspace?'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 299, 'page_label': '300'}, page_content=\"When Business Continuity and Disaster Recovery (BCDR) is enabled, OneLake\\ndiagnostics data is replicated to the secondary region, and is accessible via the OneLake\\nAPIs if a failover occurs.\\nYes. When workspace monitoring is enabled, disabled, or the Lakehouse is updated, a\\nModifyOneLakeDiagnosticSettings event is captured in the Microsoft 365 security logs,\\nallowing you to audit changes to diagnostic settings.\\nOneLake diagnostics is comparable in cost to Azure Storage diagnostics when emitting to\\na storage account. For the latest details, see the official pricing page: OneLake\\nconsumption – Microsoft Fabric | Microsoft Learn.\\nOneLake diagnostics isn't currently compatible with Workspace outbound access protection\\n(OAP) across workspaces. If you require OneLake diagnostics and OAP to work together, you\\nmust select a Lakehouse in the same Workspace.\\nWhen OneLake diagnostics is configured, the selection of the workspace honors workspace\\nprivate link configuration by limiting your selection to workspaces within the same private\\nnetwork. However, OneLake diagnostics doesn't automatically respond to networking changes.\\nOperation Category\\nReadFileOrGetBlob Read\\nGetFileOrBlobProperties Read\\nGetActionFileOrBlobProperties Read\\nCheckAccessFileOrBlob Read\\nCan you audit OneLake diagnostics?\\nHow much consumption does OneLake diagnostics generate?\\nLimitations\\nOperations\\nGlobal operations\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 300, 'page_label': '301'}, page_content='Operation Category\\nDeleteFileOrBlob Delete\\nOperation Category\\nGetBlockList Read\\nListBlob Read\\nGetBlob Read\\nDeleteBlob Delete\\nUndeleteBlob Write\\nGetBlobMetadata Read\\nSetBlobExpiry Write\\nSetBlobMetadata Write\\nSetBlobProperties Write\\nSetBlobTier Write\\nLeaseBlob Write\\nAbortCopyBlob Write\\nPutBlockFromURL Write\\nPutBlock Write\\nPutBlockList Write\\nAppendBlockFromURL Write\\nAppendBlock Write\\nAppendBlobSeal Write\\nPutBlobFromURL Write\\nCopyBlob Write\\nPutBlob Write\\nBlob operations\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 301, 'page_label': '302'}, page_content='Operation Category\\nQueryBlobContents Read\\nGetBlobProperties Read\\nCreateContainer Write\\nDeleteContainer Delete\\nGetContainerMetadata Read\\nGetContainerProperties Read\\nSetContainerMetadata Write\\nSetContainerAcl Write\\nLeaseContainer Write\\nRestoreContainer Write\\nSnapshotBlob Write\\nCreateFastPathReadSession Read\\nCreateFastPathWriteSession Write\\nOperation Category\\nCreateFileSystem Write\\nPatchFileSystem Write\\nDeleteFileSystem Delete\\nGetFileSystemProperties Read\\nCreateDirectory Write\\nCreateFile Write\\nDeleteDirectory Delete\\nDeleteFile Delete\\nRenameFileOrDirectory Write\\nDFS operations\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 302, 'page_label': '303'}, page_content='Operation Category\\nListFilePath Read\\nAppendDataToFile Write\\nFlushDataToFile Write\\nSetFileProperties Write\\nSetAccessControlForFile Write\\nSetAccessControlForDirectory Write\\nLeasePath Write\\nGetPathStatus Read\\nGetAccessControlListForFile Read\\nOperation Category\\nFabricWorkloadAccess Read\\nLast updated on 12/09/2025\\nFabric operations\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 303, 'page_label': '304'}, page_content='Limit inbound requests with inbound\\naccess protection\\n08/21/2025\\nInbound access protection secures connections between your virtual network and Microsoft\\nFabric. By setting up private links, you can prevent access to your data in OneLake from the\\npublic internet.\\nTurning on inbound access protection restricts public access to your Fabric tenant or\\nworkspace. All inbound calls must use an approved private endpoint, either from your own\\nvirtual network or from an approved service such as another Fabric workspace. These private\\nendpoints ensure that connections come only from trusted sources and not the public internet.\\nTo learn more about how to set up private endpoints and block public access to your\\nworkspace, see Fabric workspace private links.\\nTo connect to your workspace over a private endpoint, you need to use the workspace fully\\nqualified domain name (FQDN). This workspace FQDN ensures the call goes directly to the\\nprivate endpoint, and is specific to each workspace. OneLake supports both Blob and DFS\\n(Data File System) versions of the workspace FQDN:\\nhttps://{workspaceid}.z{xy}.dfs.fabric.microsoft.com\\nhttps://{workspaceid}.z{xy}.blob.fabric.microsoft.com\\\\\\nWhere:\\n{workspaceid} is the workspace GUID without dashes.\\n{xy} is the first two characters of the workspace GUID.\\nTo connect to your tenant private endpoint, you can continue to use the OneLake global\\nFQDN:\\nhttps://onelake.dfs.fabric.microsoft.com\\nWhat is inbound access protection?\\n\\uf80a \\nOneLake and private links'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 304, 'page_label': '305'}, page_content=\"https://onelake.blob.fabric.microsoft.com\\nIf your environment doesn't have a workspace private link set up, the workspace FQDN\\nconnects over the public internet. If only a tenant private link is set up, the workspace FQDN\\nconnects to the tenant private link. If both a tenant and workspace private link are set up, the\\nworkspace FQDN connects to the workspace private link.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 305, 'page_label': '306'}, page_content=\"Limit outbound requests with outbound\\naccess protection\\nOutbound access protection protects data by limiting OneLake's outbound requests made\\nthrough shortcuts and copy operations.\\nOutbound access protection helps ensure that data is shared securely within your network\\nsecurity perimeter. For example, data exfiltration protection solutions use outbound access\\nprotection controls to limit a malicious actor's ability to move large amounts of data to an\\nuntrusted external location. Outbound protections only limit requests that originate in the\\nworkspace and communicate with different workspace or location. A comprehensive network\\nsecurity solution also involves inbound network protection through private links, combined\\nwith data access controls to limit access to your data.\\nTo learn more about managing outbound access protection, see Workspace outbound access\\nprotection.\\nThere are two scenarios where OneLake makes an outbound request: shortcuts and copy\\noperations. An outbound request is defined as any request made from within the workspace\\ntowards a location outside the workspace. Only the directionality of the call matters - both\\nreads and writes to external locations can exfiltrate sensitive information to untrusted locations.\\nShortcuts are objects in OneLake that point to other storage locations, which can be internal or\\nexternal to OneLake. The location that a shortcut points to is known as the target path, and the\\nlocation where the shortcut appears is the shortcut path. If the shortcut target is a different\\nworkspace or external storage location than the shortcut path, it's an outbound shortcut and\\nsubject to outbound access protection.\\nOutbound access protection doesn't restrict shortcuts with a source and target within the same\\nworkspace, because all OneLake calls remain within the boundary of the workspace.\\nWhat is outbound access protection?\\nWhen does OneLake make outbound requests?\\nShortcuts\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 306, 'page_label': '307'}, page_content='When you copy data between two OneLake workspaces using Azure Storage copy APIs,\\nOneLake makes an outbound call from the source workspace to the target workspace. If\\noutbound access protection is enabled on the source workspace, that outbound call is blocked,\\nand the copy operation fails. To allow data movement, you must create a managed private\\nendpoint from the source workspace to the target workspace.\\nThe following copy operation from Workspace A to Workspace B is blocked when outbound\\naccess protection is enabled, unless there\\'s an approved managed private endpoint from\\nWorkspace A to Workspace B. As a reminder, AzCopy operations always following the format\\nazcopy copy <source> <destination>.\\nSyntax\\nAzCopy\\nOutbound access protection doesn\\'t block copy operations that move data within a workspace.\\nWhen you copy data between Azure Storage and OneLake, the direction of the outbound\\nrequests is reversed. The destination account makes an outbound call to the source account.\\nThis behavior applies to copy operations made directly with Azure Storage Copy Blob from URL\\nand Put Block from URL APIs. It also applies to managed copy experiences with AzCopy and\\nAzure Storage Explorer. Outbound access protection restricts this outbound call from\\ndestination to source. However, this means outbound access protection does not restrict\\nyour workspace from being the source of a copy operation, as no outbound call is made\\nfrom the source workspace.\\nFor example, the following AzCopy sample moves data from the source Azure Storage account\\n\"source\" to the destination lakehouse in OneLake. If Workspace A has outbound protection\\n\\uf80a \\nCopying data within OneLake\\nazcopy copy \\n\"https://onelake.dfs.fabric.microsoft.com/WorkspaceA/LakehouseA.Lakehouse/Files/sale\\ns.csv\" \\n\"https://onelake.dfs.fabric.microsoft.com/WorkspaceB/LakehouseB.Lakehouse/Files/sale\\ns.csv\" --trusted-microsoft-suffixes \"fabric.microsoft.com\"\\nCopying data between Azure Storage and OneLake'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 307, 'page_label': '308'}, page_content='turned on, then the outbound call from Workspace A to the external Azure Storage account is\\nblocked, and the data isn\\'t loaded.\\nSyntax\\nAzCopy\\nHowever, in the following scenario, Workspace A is now the source of the copy operation, with\\nthe external Azure Data Lake Storage (ADLS) account as the destination. In this scenario,\\noutbound access protection does not block this call, as only inbound calls are made to\\nWorkspace A. To restrict these types of operations, see Protect inbound traffic.\\nSyntax\\nAzCopy\\nTo ensure you can continue to read and write data across workspaces, you can create a\\nmanaged private endpoint between workspaces. When a valid managed private endpoint\\nexists from one workspace to another, outbound requests are permitted from the source\\nworkspace to the target workspace even when outbound access is restricted.\\nFor example, creating a managed private endpoint from Workspace A to Workspace B lets\\nusers read data in Workspace B through a shortcut, or copy data from Workspace B to\\nWorkspace A using AzCopy.\\nFabric outbound access protection.\\nFabric inbound access protection.\\nManage inbound access to OneLake with workspace private links.\\nazcopy copy \"https://source.blob.core.windows.net/myContainer/sales.csv\" \\n\"https://onelake.dfs.fabric.microsoft.com/WorkspaceA/LakehouseA.Lakehouse/Files/sale\\ns.csv\" --trusted-microsoft-suffixes \"fabric.microsoft.com\"\\nazcopy copy \\n\"https://onelake.dfs.fabric.microsoft.com/WorkspaceA/LakehouseA.Lakehouse/Files/sale\\ns.csv\" \"https://source.blob.core.windows.net/myContainer/sales.csv\"  --trusted-\\nmicrosoft-suffixes \"fabric.microsoft.com\"\\nCross-workspace operations\\nRelated Content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 308, 'page_label': '309'}, page_content='Last updated on 11/26/2025'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 309, 'page_label': '310'}, page_content=\"Disaster recovery and data protection for\\nOneLake\\nArticle• 05/20/2025\\nAll data in OneLake is accessed through data items. These data items can reside in different\\nregions depending on their workspace, because a workspace is created under a capacity that's\\ntied to a specific region.\\nOneLake uses zone-redundant storage (ZRS) where that storage type is available. (See Azure\\nregions with availability zones.) Elsewhere, OneLake uses locally redundant storage (LRS). With\\nboth LRS and ZRS, your data is resilient to transient hardware failures within a datacenter.\\nJust like for Azure Storage, LRS replicates data within a single datacenter in the primary region.\\nLRS provides at least 99.999999999% (11 nines) durability of objects over a year. This durability\\nhelps protect against server rack and drive failures, but not against datacenter disasters.\\nMeanwhile, ZRS provides fault tolerance to datacenter failures by copying data synchronously\\nacross three Azure availability zones in the primary region. ZRS offers a durability of at least\\n99.9999999999% (12 nines) over a year.\\nThis article provides guidance on how to further protect your data from rare region-wide\\noutages.\\nYou can enable or disable business continuity and disaster recovery (BCDR) for a specific\\ncapacity through the capacity admin portal. If your capacity has BCDR activated, your data is\\nduplicated and stored in two geographic regions so that it's geo-redundant. The standard\\nregion pairings in Azure determine the choice of the secondary region. You can't modify the\\nsecondary region.\\nIf a disaster makes the primary region unrecoverable, OneLake might initiate a regional\\nfailover. After the failover finishes, you can use the OneLake APIs through the global endpoint\\nto access your data in the secondary region. Data replication to the secondary region is\\nasynchronous, so any data not copied during the disaster is lost. After a failover, the new\\nprimary datacenter has local redundancy only.\\nFor a comprehensive understanding of the end-to-end experience, see Reliability in Microsoft\\nFabric.\\nDisaster recovery\\nSoft deletion for OneLake files\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 310, 'page_label': '311'}, page_content=\"In OneLake, soft deletion prevents accidental file loss by retaining deleted files for seven days\\nbefore permanent removal. Soft-deleted data is billed at the same rate as active data.\\nYou can restore files and folders by using Azure Blob Storage REST APIs, Azure Storage SDKs,\\nand the Azure PowerShell Az.Storage module. Learn how to list and restore files by using these\\nPowerShell instructions and how to connect to OneLake with PowerShell.\\nYou can restore deleted Lakehouse files by using Azure Storage Explorer. First, connect to your\\nworkspace from Storage Explorer by using the workspace ID in the URL. For example, use\\nhttps://onelake.dfs.fabric.microsoft.com/aaaaaaaa-0000-1111-2222-bbbbbbbbbbbb. You can\\nfind the workspace ID from the Microsoft Fabric portal's browser URL (/groups/{workspaceID}).\\nEnsure that you use the GUID-based OneLake path to restore data.\\nAfter you connect to your workspace, follow these steps to restore soft-deleted data:\\n1. Select the dropdown button next to the path bar, and then select Active and soft deleted\\nblobs instead of the default Active blobs.\\n2. Go to the folder that contains the soft-deleted file.\\n3. Right-click the file, and then select Undelete.\\nOneLake compute and storage consumption\\nRestore soft-deleted files via Azure Storage Explorer\\nRelated content\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 311, 'page_label': '312'}, page_content='Use OneLake file explorer (preview) to\\naccess Fabric data\\n07/26/2025\\nThe OneLake file explorer application seamlessly integrates OneLake with Windows File\\nExplorer. This application automatically syncs all OneLake items that you have access to in\\nWindows File Explorer. \"Sync\" refers to pulling up-to-date metadata on files and folders, and\\nsending changes made locally to the OneLake service. Syncing doesn’t download the data, it\\ncreates placeholders. You must double-click on a file to download the data locally.\\nWhen you create, update, or delete a file via Windows File Explorer, it automatically syncs the\\nchanges to OneLake service. Updates to your item made outside of your File Explorer aren\\'t\\nautomatically synced. To pull these updates, you need to right-click on the item or subfolder in\\nWindows File Explorer and select OneLake > Sync from OneLake.\\nOneLake file explorer currently supports Windows and is validated on Windows 10 and 11.\\nTo install:\\n1. Download the OneLake file explorer.\\n2. Double-click the file to start installing.\\n\\uf80a\\n） Important\\nThis feature is in preview.\\nInstallation instructions'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 312, 'page_label': '313'}, page_content='The storage location on your PC for the placeholders and any downloaded content is\\n\\\\%USERPROFILE%\\\\OneLake - Microsoft\\\\.\\nOnce the application is installed and running, you can see your OneLake data in Windows File\\nExplorer.\\nStarting in version 1.0.13, the OneLake file explorer app notifies you when a new update is\\navailable. When a new version becomes available, you receive a Windows notification and the\\nOneLake icon changes. Right-click on the OneLake icon in the Windows notification area.\\nSelect Update Available and follow steps to update.\\nWorkspace names with the \"/\" character, encoded escape characters such as %23, and\\nnames that look like GUIDs fail to sync.\\nFiles or folders containing Windows reserved characters fail to sync. For more\\ninformation, see Naming files, paths, and namespaces.\\nIf Windows search is disabled, OneLake file explorer fails to start.\\nWindows File Explorer is case insensitive, while OneLake is case sensitive. You can create\\nfiles with the same name but different cases in the OneLake service using other tools, but\\nWindows File Explorer only shows the oldest of these files.\\nIf a file fails to sync due to a network issue, you have to trigger the sync to OneLake. To\\nprompt the sync process, open the file and save it. Alternatively, you can trigger a modify\\nevent using PowerShell by executing this command: (Get-Item -Path \"\\n<file_path>\").LastWriteTimeUtc = Get-Date\\nOneLake File Explorer does not support environments that require network proxy\\nconfigurations. Attempts to launch or authenticate the application behind a proxy may\\nresult in connection failures or sign-in issues.\\nOneLake File Explorer does not support syncing files marked as read-only. This limitation\\napplies specifically to files marked as read-only by the user on their local machine. This\\nbehavior is by design to prevent conflicts with local file system permissions and to avoid\\nunintended edits to protected content\\nThe following scenarios provide details for working with the OneLake file explorer.\\nLimitations and considerations\\nScenarios'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 313, 'page_label': '314'}, page_content=\"OneLake file explorer starts automatically at startup of Windows. You can disable the\\napplication from starting automatically by selecting Startup apps in Windows Task Manager\\nand then right-clicking OneLake, and selecting Disable.\\nTo manually start the application, search for OneLake using Windows search (Windows+S)\\nand select the OneLake application. The views for any folders that were previously synced\\nrefresh automatically.\\nTo exit, right-click on the OneLake icon in the Windows notification area, located at the\\nfar right of the taskbar, and select Exit. The sync pauses, and you can't access placeholder\\nfiles and folders. You continue to see the blue cloud icon for placeholders that were\\npreviously synced but not downloaded.\\nTo optimize performance during the initial sync, OneLake file explorer syncs the placeholder\\nfiles for the top-level workspaces and item names. When you open an item, OneLake file\\nexplorer syncs the files directly in that folder. Then, opening a folder within the item syncs the\\nfiles directly in that folder. This functionality allows you to navigate your OneLake content\\nseamlessly, without having to wait for all files to sync before starting to work.\\nWhen you create, update, or delete a file via OneLake file explorer, it automatically syncs the\\nchanges to OneLake service. Updates to your item made outside of your OneLake file explorer\\naren't automatically synced. To pull these updates, right-click on the workspace name, item\\nname, folder name, or file in OneLake file explorer and select OneLake > Sync from OneLake.\\nThis action refreshes the view for any folders that were previously synced. To pull updates for\\nall workspaces, right-click on the OneLake root folder and select OneLake > Sync from\\nOneLake.\\nStarting in version 1.0.9.0, when you install OneLake file explorer, you can choose which\\naccount to sign in with. To switch accounts, right-click the OneLake icon in the Windows\\nnotification area, select Account, and then Sign Out. Signing out exits OneLake file explorer\\nand pauses the sync. To sign in with another account, start OneLake file explorer again and\\nchoose the desired account.\\nWhen you sign in with another account, you see the list of workspaces and items refresh in\\nOneLake file explorer. If you navigate to workspaces associated with the previous account, you\\nStart and exit OneLake file explorer\\nSync updates from OneLake\\nSign in to different accounts\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 314, 'page_label': '315'}, page_content=\"can manually refresh the view by right-clicking the workspace, then selecting OneLake > Sync\\nfrom OneLake. Those workspaces are inaccessible while you're signed in to a different account.\\nStarting in version 1.0.10.0, you can transition between using OneLake file explorer and the\\nFabric web portal. From OneLake file explorer, right-click on a workspace and select OneLake >\\nView workspace online. This action opens the workspace browser on the Fabric web portal.\\nYou can right-click on an item, subfolder, or file and select OneLake > View item online. This\\naction opens the item browser on the Fabric web portal. If you select a subfolder or file, the\\nFabric web portal always opens the root folder of the item.\\nThe OneLake file explorer only syncs updates when you're online and the application is\\nrunning. When the application starts, the views for any folders that were previously synced\\nrefresh automatically. Any files that you added or updated while offline show as sync pending\\nuntil you save them again. Any files that you deleted while offline are recreated during the\\nrefresh if they still exist on the service.\\n1. Navigate to the OneLake section in Windows File Explorer.\\n2. Navigate to the appropriate folder in your item.\\n3. Right-click and select New folder or New file type.\\n1. Navigate to the OneLake section in Windows File Explorer.\\n2. Navigate to the Files or Tables folder in your item.\\nOption to open workspaces and items on the web portal\\nOffline support\\nCreate files or folders in OneLake file explorer\\n７ Note\\nOneLake sync fails if you write data to locations where you don't have write permission,\\nsuch as the root of the item or workspace. Clean up files or folders that fail to sync by\\nmoving them to the correct location or deleting them.\\nDelete files or folders in OneLake file explorer\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 315, 'page_label': '316'}, page_content=\"3. Select a file or folder and delete.\\nYou can open files using your favorite apps and make edits. Select Save to sync the file to\\nOneLake. Starting in version 1.0.11, you can also make updates with Excel to your files. Close\\nthe file after the update in Excel and it initiates the sync to OneLake.\\nIf you edit a file locally and select Save, the OneLake file explorer app detects if that file was\\nupdated elsewhere (by someone else) since you last selected Sync from OneLake. A Confirm\\nthe action dialog box appears:\\nIf you select Yes, your local changes overwrite any other changes made to the file since the last\\ntime you selected Sync from OneLake.\\nIf you select No, the local changes aren't sent to the OneLake service. You can then select Sync\\nfrom OneLake to revert your local changes and pull the file from the service. Or you can copy\\nthe file with a new name to avoid conflicts.\\nYou can copy files to, from, and within your items using standard keyboard shortcuts like\\nCtrl+C and Ctrl+V. You can also move files by dragging and dropping them.\\nWhen you upload or download files using the OneLake file explorer, the performance should\\nbe similar to using OneLake APIs. In general, the time it takes to sync changes from OneLake is\\nproportional to the number of files.\\nEdit files\\nCopy or move files\\nSupport for large files and a large number of files\\nOneLake shortcut support\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 316, 'page_label': '317'}, page_content=\"All folders in your items including OneLake shortcuts are visible. You can view, update, and\\ndelete the files and folders in those shortcuts.\\nStarting in version 1.0.10, you can find your client-side logs by right-clicking on the OneLake\\nicon in the Windows notification area, located at the far right of the taskbar. Select Diagnostic\\noperations > Open logs folder. This action opens your logs directory in a new Windows file\\nexplorer window.\\nClient-side logs are stored on your local machine under %temp%\\\\OneLake\\\\Diagnostics\\\\.\\nYou can enable more client-side logging by selecting Diagnostic operations > Enable tracing.\\nStarting in version 1.0.11, you can find information about each release of the OneLake file\\nexplorer by right-clicking on the OneLake icon in the Windows notification area, located at the\\nfar right of the taskbar. Select About > Release Notes. This action opens the OneLake file\\nexplorer release notes page in your browser window.\\nTo uninstall the app, search for OneLake in Windows. Select Uninstall in the list of options\\nunder OneLake.\\nTenant admins can restrict access to OneLake file explorer for their organization in the\\nMicrosoft Fabric admin portal. When the setting is disabled, no one in your organization can\\nstart the OneLake file explorer app. If the application is already running and the tenant admin\\ndisables the setting, the application exits. Placeholders and any downloaded content remain on\\nlocal machines, but users can't sync data to or from OneLake.\\nThese OneLake file explorer icons appear in Windows File Explorer to indicate the sync state of\\nthe file or folder.\\nClient-side logs\\nRelease Notes\\nUninstall instructions\\nTenant setting enables access to OneLake file explorer\\nOneLake file explorer icons\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 317, 'page_label': '318'}, page_content=\"Icon Icon\\ndescription\\nMeaning\\nBlue cloud\\nicon\\nThe file is only available online. Online-only files don’t take up space on your\\ncomputer.\\nGreen tick The file is downloaded to your local computer.\\nSync pending\\narrows\\nSync is in progress. This icon might appear when you're uploading files. If the sync\\npending arrows are persistent, then your file or folder might have an error\\nsyncing. You can find more information in the client-side logs on your local\\nmachine under %temp%\\\\OneLake\\\\Diagnostics\\\\.\\nLearn more about Fabric and OneLake security.\\nWhat's new in the latest OneLake file explorer?\\nRelated content\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 318, 'page_label': '319'}, page_content=\"What's new in the latest OneLake file\\nexplorer?\\n09/02/2025\\nContinue reading for information on major updates to OneLake file explorer.\\nOneLake File Explorer has been migrated to .NET 8, ensuring the application remains aligned\\nwith Microsoft’s latest supported frameworks. This upgrade enhances security and ensures\\ncontinued eligibility for long-term support.\\nTemporary files (.tmp) created during file edits— often created during edits in applications like\\nMicrosoft Excel —will no longer get stuck in the sync pending state. This issue typically\\noccurred when there were network connectivity problems, preventing .tmp files from\\nuploading and causing conflicts with the server. These files are now cleaned up automatically\\nonce the network connection is restored, eliminating persistent sync indicators on temporary\\nfiles.\\nWe’ve resolved a memory access violation that previously caused crashes when syncing files\\nthat had not been opened before. This fix improves reliability and prevents unexpected\\nshutdowns during background sync operations.\\n）  Important\\nThis feature is in preview.\\nSeptember 2025 Update (v 1.0.14.0)\\n.NET 8 Migration\\nSmarter sync on temporary files\\nStability improvements\\nApril 2024 Update\\nv 1.0.13.0 - Update Notifications\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 319, 'page_label': '320'}, page_content=\"We believe that staying informed about app updates is crucial. Whether it’s a bug fix,\\nperformance improvement, or exciting new features. Starting with this version, the OneLake file\\nexplorer app will now notify you when a new update is available. You’ll receive a Windows\\nnotification when a new version is available and the OneLake icon in the Windows notification\\narea will change. Simply right-click the icon to see if an update is available.\\nThis release includes minor internal fixes to enhance functionality.\\nWith this release users can make edits and updates with Excel to OneLake files, similar to the\\nexperience with OneDrive. Start by opening a csv or xlsx file using Excel, make updates and\\nclose the file. Closing the file will initiate the sync to OneLake. You can then view the updated\\nfile online in the Fabric web portal. This enhancement aims to streamline your workflow and\\nprovide a more intuitive approach to managing and editing your files with Excel.\\nWith this menu option, you can easily find details about what's new in the latest OneLake File\\nExplorer version. Right-click on the OneLake icon in the Windows notification area, located at\\nthe far right of the taskbar, and select About > Release Notes. This action opens the OneLake\\nfile explorer release notes page in your browser window.\\nOneLake file explorer will default to the latest TLS version supported by Windows, currently TLS\\n1.3. Support for TLS 1.3 is recommended for maintaining the security and privacy of data\\nexchanged over the internet.\\nNow you can seamlessly transition between using OneLake file explorer and the Fabric web\\nportal. Browse your OneLake data using OneLake file explorer, right-click on a workspace, and\\nv 1.0.12.0 - Internal update\\nDecember 2023 Update (v 1.0.11.0)\\nAbility to update files using Excel\\nMenu option to view Release Notes\\nTLS 1.3 support\\nSeptember 2023 Update (v 1.0.10.0)\\nOption to open workspaces and items on the web portal\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 320, 'page_label': '321'}, page_content='select OneLake > View Workspace Online. This action opens the workspace browser on the\\nFabric web portal.\\nIn addition, you can right-click on an item, subfolder, or file and select OneLake > View Item\\nOnline. This action opens the item browser on the Fabric web portal. If you select a subfolder\\nor file, the Fabric web portal always opens the root folder of the item.\\nWith this menu option, you can now easily find your client-side logs, which can help you\\ntroubleshoot issues. Right-click on the OneLake icon in the Windows notification area, located\\nat the far right of the taskbar, and select Select Diagnostic Operations > Open logs directory.\\nThis action opens your logs directory in a new Windows file explorer window.\\nWhen you install OneLake file explorer, you can now choose which account to sign-in with. To\\nswitch accounts, right-click the OneLake icon in the Windows notification area, select Account,\\nand then select Sign Out. Signing out exits OneLake file explorer and pauses the sync. To sign\\nin with another account, start OneLake file explorer again by searching for \"OneLake\" using\\nWindows search (Windows + S) and select the OneLake application. Previously, when you\\nstarted OneLake file explorer, it automatically used the Microsoft Entra ID currently logged into\\nWindows to sync Fabric workspaces and items.\\nWhen you sign in with another account, you see the list of workspaces and items refresh in\\nOneLake file explorer. If you continue to workspaces associated with the previous account, you\\ncan manually refresh the view by selecting Sync from OneLake. Those workspaces are\\ninaccessible while you\\'re signed into a different account.\\nNow when you move a folder (cut and paste or drag and drop) from a location outside of\\nOneLake to OneLake, the contents sync to OneLake successfully. Previously, you had to trigger\\na sync by either opening the files and saving them or moving them back out of OneLake and\\nthen copying and pasting (versus moving).\\nMenu option to open logs directory\\nJuly 2023 Update (v 1.0.9.0)\\nOption to sign in to different accounts\\nFix known issue during folder moves from outside of OneLake\\nto OneLake'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 321, 'page_label': '322'}, page_content='Only the most recent version of OneLake file explorer is supported. If you contact support for\\nOneLake file explorer, they ask you to upgrade to the most recent version. Download the latest\\nOneLake file explorer.\\nUse OneLake file explorer to access Fabric data\\nSupport lifecycle\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 322, 'page_label': '323'}, page_content='Get the size of OneLake items\\nArticle• 05/09/2025\\nLearn how to get the size of your OneLake data to manage and plan storage costs. Capacity\\nadmins can use the Microsoft Fabric Capacity Metrics app to find the total size of OneLake data\\nstored in a given capacity or workspace. But you might want a way to measure size at a more\\ngranular level.\\nThis article describes Azure Storage PowerShell commands that you can use to understand the\\nsize of data in a specific item or folder. Because OneLake is compatible with Azure Data Lake\\nStorage (ADLS) tools, many of the commands work by just replacing the ADLS Gen2 URL with a\\nOneLake URL.\\nTo automate the steps in this article, use REST API commands to get the workspace and item\\ninformation instead of providing them manually. For more information, see List workspaces\\nand List items.\\nAzure PowerShell. For more information, see How to install Azure PowerShell\\nThe Azure Storage PowerShell module.\\nPowerShell\\nSign in to PowerShell with your Azure account.\\nPowerShell\\nEach time you run an Azure Storage command against OneLake, you need to include the -\\nContext parameter with an Azure Storage context object. To create a context object that points\\nto OneLake, run the New-AzStorageContext command with the following values:\\nPrerequisites\\nInstall-Module Az.Storage -Repository PSGallery -Force\\nConnect-AzAccount\\nCreate a context object for OneLake\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 323, 'page_label': '324'}, page_content=\"Parameter Value\\n-StorageAccountName 'onelake'\\n-UseConnectedAccount None; instructs the cmdlet to use your Azure account.\\n-Endpoint 'fabric.microsoft.com'\\nFor ease of reuse, create this context as a local variable:\\nPowerShell\\nTo get an item size, use the Get-AzDataLakeGen2ChildItem command with the following\\nvalues:\\nParameter Value\\n-Context An Azure Storage context object. For more information, see Create a context object for\\nOneLake.\\n-FileSystem Fabric workspace name or GUID. Azure Storage naming criteria for containers only\\nsupports lowercase letters, numbers, and hyphens. If you have any other characters in\\nyour workspace name, use its GUID instead.\\n-Path Local path to the item or folder inside the workspace. Azure Storage naming criteria for\\ncontainers only supports lowercase letters, numbers, and hyphens. If you have any other\\ncharacters in any resources in your item path, use the equivalent GUID instead.\\n-Recurse None; instructs the cmdlet to recursively get the child item.\\n-\\nFetchProperty\\nNone; instructs the cmdlet to fetch the item properties.\\nUse a pipeline to pass the output of the Get-AzDataLakeGen2ChildItem command to the\\nMeasure-object command with the following values:\\n$ctx = New-AzStorageContext -StorageAccountName 'onelake' -UseConnectedAccount -\\nendpoint 'fabric.microsoft.com'\\nGet the size of an item or folder\\nﾉ Expand table\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 324, 'page_label': '325'}, page_content='Parameter Value\\n-Property Length\\n-Sum None; indicates that the cmdlet displays the sum of the values of the specified property.\\nCombined, the full command looks like the following example:\\nPowerShell\\nPowerShell\\nPowerShell\\nPowerShell\\nGet-AzDataLakeGen2ChildItem -Context <CONTEXT_OBJECT> -FileSystem <WORKSPACE_NAME> \\n-Path <ITEM_PATH> -Recurse -FetchProperty | Measure-Object -property Length -sum\\nExample: Get the size of an item\\n$ctx = New-AzStorageContext -StorageAccountName \\'onelake\\' -UseConnectedAccount -\\nendpoint \\'fabric.microsoft.com\\'\\n$workspaceName = \\'myworkspace\\'\\n$itemPath = \\'mylakehouse.lakehouse\\'\\n$colitems = Get-AzDataLakeGen2ChildItem -Context $ctx -FileSystem $workspaceName -\\nPath $itemPath -Recurse -FetchProperty | Measure-Object -property Length -sum\\n\"Total file size: \" + ($colitems.sum / 1GB) + \" GB\"\\nExample: Get the size of a folder\\n$ctx = New-AzStorageContext -StorageAccountName \\'onelake\\' -UseConnectedAccount -\\nendpoint \\'fabric.microsoft.com\\'\\n$workspaceName = \\'myworkspace\\'\\n$itemPath = \\'mylakehouse.lakehouse/Files/folder1\\'\\n$colitems = Get-AzDataLakeGen2ChildItem -Context $ctx -FileSystem $workspaceName -\\nPath $itemPath -Recurse -FetchProperty | Measure-Object -property Length -sum\\n\"Total file size: \" + ($colitems.sum / 1GB) + \" GB\"\\nExample: Get the size of a table with GUIDs\\n$ctx = New-AzStorageContext -StorageAccountName \\'onelake\\' -UseConnectedAccount -\\nendpoint \\'fabric.microsoft.com\\'\\n$workspaceName = \\'aaaaaaaa-0000-1111-2222-bbbbbbbbbbbb\\'\\n$itemPath = \\'bbbbbbbb-1111-2222-3333-cccccccccccc/Tables/table1\\''),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 325, 'page_label': '326'}, page_content='These PowerShell commands don’t work on shortcuts that point to ADLS containers directly.\\nInstead, we recommended that you create ADLS shortcuts to directories that are at least one\\nlevel below a container.\\n$colitems = Get-AzDataLakeGen2ChildItem -Context $ctx -FileSystem $workspaceName -\\nPath $itemPath -Recurse -FetchProperty | Measure-Object -property Length -sum\\n\"Total file size: \" + ($colitems.sum / 1GB) + \" GB\"\\nLimitations'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 326, 'page_label': '327'}, page_content=\"OneLake catalog overview\\n10/23/2025\\nOneLake catalog is a centralized place that helps you find, explore, and use the Fabric items\\nyou need, and govern the data you own. It features two tabs:\\nExplore tab: The explore tab has an items list with an in-context item details view that\\nmakes it possible to browse through and explore items without losing your list context. It\\nalso provides selectors and filters to narrow down and focus the list, making it easier to\\nfind what you need. By default, the OneLake catalog opens on the Explore tab. Along with\\nthe OneLake catalog, you can open and work across multiple workspaces side by side\\nusing the object explorer.\\nGovern tab: The govern tab provides insights that help you understand the governance\\nposture of all the data you own in Fabric, and presents recommended actions you can\\ntake to improve the governance status of your data.\\nSecure tab: The OneLake catalog Secure tab centralizes security management in\\nMicrosoft Fabric by providing a unified view of workspace roles and OneLake security\\nroles across items. It enables admins to audit permissions, view user access, and create,\\nedit, or delete security roles from a single location, ensuring streamlined governance and\\nconsistent enforcement of data access policies.\\nTo open the OneLake catalog, select the OneLake icon in the Fabric navigation pane. Select the\\ntab you're interested if it's not displayed by default.\\nOpen the OneLake catalog\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 327, 'page_label': '328'}, page_content='Discover and explore Fabric items in the OneLake catalog\\nView item details\\nGovern your data in Fabric\\nEndorsement\\nFabric domains\\nLineage in Fabric\\nMonitor hub\\n\\uf80a \\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 328, 'page_label': '329'}, page_content=\"OneLake compute and storage\\nconsumption\\n10/07/2025\\nOneLake usage is defined by data stored and the number of transactions. For OneLake security,\\ncapacity usage is based on the number of rows in the table being secured. This page contains\\ninformation on how all of OneLake usage is billed and reported.\\nOneLake storage is billed at a pay-as-you-go rate per GB of data used and doesn't consume\\nFabric Capacity Units (CUs). Fabric items like lakehouses and warehouses consume OneLake\\nstorage. For Mirroring storage, data up to a certain limit is free based on the purchased\\ncompute capacity SKU you provision. For more information about pricing, see Fabric pricing.\\nFor native mirrored storage, OneLake storage isn't billed as it's included in the cost of items\\nlike Power BI import semantic models and Fabric SQL database.\\nYou can visualize your OneLake storage usage in the Fabric Capacity Metrics app in the Storage\\ntab. Also note that soft-deleted data is billed at the same rate as active data. For more\\ninformation about monitoring usage, see the Metrics app Storage page. To understand\\nOneLake consumption more, see the OneLake Capacity Consumption page\\nRequests to OneLake, such as reading or writing data, consume Fabric Capacity Units. The rates\\nin this page define how much capacity units are consumed for a given type of operation.\\nOneLake uses the same mappings as Azure Data Lake Storage (ADLS) to classify the operation\\nto the category.\\nOneLake supports two access paths: redirect and proxy. Applications will use one of these\\npaths based on specific circumstances, some determined by the workload and others outside\\nits control. Requests via proxy and redirect share the same consumption rate.\\nThis table defines CU consumption when OneLake data is accessed using applications that\\nredirect certain requests.\\nStorage\\nTransactions\\nOperation types\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 329, 'page_label': '330'}, page_content='Operation in Metrics AppDescription Operation Unit of\\nMeasure\\nConsumption\\nrate\\nOneLake Read via Redirect OneLake Read via Redirect Every 4 MB, per\\n10,000*\\n104 CU seconds\\nOneLake Write via Redirect OneLake Write via Redirect Every 4 MB, per\\n10,000*\\n1626 CU\\nseconds\\nOneLake Other Operations\\nvia Redirect\\nOneLake Other Operations\\nvia Redirect\\nPer 10,000 104 CU seconds\\nOneLake Iterative Read via\\nRedirect\\nOneLake Iterative Read via\\nRedirect\\nPer 10,000 1626 CU\\nseconds\\nOneLake Iterative Write via\\nRedirect\\nOneLake Iterative Write via\\nRedirect\\nPer 100 1300 CU\\nseconds\\nThis table defines CU consumption when OneLake data is accessed using applications that\\nproxy requests, which match the rate for transactions via redirect.\\nOperation in Metrics AppDescription Operation Unit of\\nMeasure\\nConsumption\\nrate\\nOneLake Read via Proxy OneLake Read via Proxy Every 4 MB, per\\n10,000*\\n104 CU seconds\\nOneLake Write via Proxy OneLake Write via ProxyEvery 4 MB, per\\n10,000*\\n1626 CU seconds\\nOneLake Other OperationsOneLake Other OperationsPer 10,000 104 CU seconds\\nOneLake Iterative Read via\\nProxy\\nOneLake Iterative Read via\\nProxy\\nPer 10,000 1626 CU seconds\\nOneLake Iterative Write via\\nProxy\\nOneLake Iterative Write via\\nProxy\\nPer 100 1300 CU seconds\\n*For files > 4 MB in size, OneLake counts a transaction for every 4 MB block of data read or\\nwritten. For files < 4 MB, a full transaction is counted. For example, if you do 10,000 read\\noperations via Redirect and each file read is 16 MB in size, your capacity consumption is 40,000\\ntransactions or 416 CU seconds.\\nﾉ Expand table\\nﾉ Expand table\\nShortcuts'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 330, 'page_label': '331'}, page_content='When you access data via OneLake shortcuts, the transaction usage counts against the capacity\\ntied to the workspace where the shortcut is created. The capacity where the data is ultimately\\nstored (that the shortcut points to) is billed for the data stored.\\nWhen you access data via a shortcut to a source external to OneLake, such as to ADLS Gen2,\\nOneLake does not count the CU usage for that external request. The transactions would be\\ncharged directly to you by the external service such as ADLS Gen2.\\nWhen a capacity is paused, the data stored continues to be billed using the pay-as-you-go rate\\nper GB. All transactions to that capacity are rejected when it is paused, so no Fabric CUs are\\nconsumed due to OneLake transactions. To access your data or delete a Fabric item, the\\ncapacity needs to be resumed. You can delete the workspace while a capacity is paused.\\nThe consumption of the data via shortcuts is always counted against the consumer’s capacity,\\nso the capacity where the data is stored can be paused without disrupting downstream\\nconsumers in other capacities. See an example on the OneLake Capacity Consumption page\\nOneLake usage when disaster recovery is enabled is also defined by the amount of data stored\\nand the number of transactions.\\nWhen disaster recovery is enabled, the data in OneLake gets geo-replicated. Thus, the storage\\nis billed as Business Continuity and Disaster Recovery (BCDR) Storage. For more information\\nabout pricing, see Fabric pricing.\\nWhen disaster recovery is enabled for a given capacity, write operations consume higher\\ncapacity units.\\nThis table defines CU consumption when disaster recovery is enabled and OneLake data is\\naccessed using applications that redirect certain requests. Redirection is an implementation\\nthat reduces consumption of OneLake compute.\\nPaused Capacity\\nDisaster recovery\\nDisaster recovery storage\\nDisaster recovery transactions\\nDisaster recovery operation types'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 331, 'page_label': '332'}, page_content='Operation Description Operation Unit of\\nMeasure\\nCapacity\\nUnits\\nOneLake BCDR Read via\\nRedirect\\nOneLake BCDR Read via\\nRedirect\\nEvery 4 MB, per\\n10,000\\n104 CU\\nseconds\\nOneLake BCDR Write via\\nRedirect\\nOneLake BCDR Write via\\nRedirect\\nEvery 4 MB, per\\n10,000\\n3056 CU\\nseconds\\nOneLake BCDR Other\\nOperations Via Redirect\\nOneLake BCDR Other\\nOperations Via Redirect\\nPer 10,000 104 CU\\nseconds\\nOneLake BCDR Iterative Read\\nvia Redirect\\nOneLake BCDR Iterative Read\\nvia Redirect\\nPer 10,000 1626 CU\\nseconds\\nOneLake BCDR Iterative Write\\nvia Redirect\\nOneLake BCDR Iterative Write\\nvia Redirect\\nPer 100 2730 CU\\nseconds\\nThis table defines CU consumption when disaster recovery is enabled and OneLake data is\\naccessed using applications that proxy requests, which match the rate for transactions via\\nredirect.\\nOperation Description Operation Unit of\\nMeasure\\nCapacity\\nUnits\\nOneLake BCDR Read via Proxy OneLake BCDR Read via ProxyEvery 4 MB, per\\n10,000\\n104 CU\\nseconds\\nOneLake BCDR Write via\\nProxy\\nOneLake BCDR Write via\\nProxy\\nEvery 4 MB, per\\n10,000\\n3056 CU\\nseconds\\nOneLake BCDR Other\\nOperations\\nOneLake BCDR Other\\nOperations\\nPer 10,000 104 CU\\nseconds\\nOneLake BCDR Iterative Read\\nvia Proxy\\nOneLake BCDR Iterative Read\\nvia Proxy\\nPer 10,000 1626 CU\\nseconds\\nOneLake BCDR Iterative Write\\nvia Proxy\\nOneLake BCDR Iterative Write\\nvia Proxy\\nPer 100 2730 CU\\nseconds\\nOneLake security consumes capacity for row level security (RLS) transactions based on the\\nnumber of rows in the table secured by RLS. When you access a table secured with RLS, the\\nﾉ Expand table\\nﾉ Expand table\\nOneLake security'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 332, 'page_label': '333'}, page_content=\"capacity consumption applies to the Fabric item used to execute the query according to the\\ntable below.\\nOperation Description Operation Unit of Measure Capacity Units\\nOneLake security RLS OneLake security RLSMillion rows in the table 0.1 CU seconds\\nOneLake diagnostics consumes capacity when diagnostic events are captured and written to a\\ndestination Lakehouse according to the table below.\\nOperation Description Operation Unit of\\nMeasure\\nCapacity\\nUnits\\nOneLake Diagnostics Event\\nOperation\\nOneLake diagnostic write operationsEvery 4 MB, per\\n10,000\\n1626 CU\\nseconds\\nOneLake BCDR Diagnostics\\nEvent Operation\\nOneLake diagnostic write operations\\nwhen BCDR is enabled\\nEvery 4 MB, per\\n10,000\\n3056 CU\\nseconds\\nOneLake Diagnostics Data\\nTransfer\\nOneLake diagnostic data transferPer GB 1.389 CU\\nHours\\nConsumption rates are subject to change at any time. Microsoft will use reasonable efforts to\\nprovide notice via email or through in-product notification. Changes shall be effective on the\\ndate stated in Microsoft's Release Notes or Microsoft Fabric Blog. If any change to a Microsoft\\nFabric Workload Consumption Rate materially increases the Capacity Units (CU) required to use\\na particular workload, customers may use the cancellation options available for the chosen\\npayment method.\\nDisaster recovery guidance for OneLake\\nﾉ Expand table\\nOneLake diagnostics\\nﾉ Expand table\\nChanges to Microsoft Fabric workload\\nconsumption rate\\nRelated content\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 333, 'page_label': '334'}, page_content='Fabric capacity and OneLake\\nconsumption\\nArticle• 12/26/2024\\nYou only need one capacity to drive all your Microsoft Fabric experiences, including\\nMicrosoft OneLake. Keep reading if you want a detailed example of how OneLake\\nconsumes storage and compute.\\nOneLake comes automatically with every Fabric tenant and is designed to be the single\\nplace for all your analytics data. All the Fabric data items are prewired to store data in\\nOneLake. For example, when you store data in a lakehouse or warehouse, your data is\\nnatively stored in OneLake.\\nWith OneLake, you pay for the data stored, similar to services like Azure Data Lake\\nStorage (ADLS) Gen2 or Amazon S3. However, unlike other services, OneLake doesn\\'t\\ninclude a separate charge for transactions (for example, reads, writes) to your data.\\nInstead, transactions consume from existing Fabric capacity that is also used to run your\\nother Fabric experiences. For information about pricing, which is comparable to ADLS\\nGen2, see Fabric pricing.\\nTo illustrate, let’s walk through an example.\\nLet’s say you purchase an F2 SKU with 2 Capacity Units (CU) every second. Let’s\\nname this Capacity1.\\nYou then create Workspace1 and upload a 450 MB file to a lakehouse using the\\nFabric portal. This action consumes both OneLake storage and OneLake\\ntransactions.\\nNow, let’s dive into each of these dimensions.\\nSince OneLake storage operates on a pay-as-you-go model, a separate charge for\\n\"OneLake Storage\" appears in your bill corresponding to the 450 MB of data stored.\\nIf you\\'re a capacity admin, you can view your storage consumption in the Fabric\\nCapacity Metrics app. Open the Storage tab and choose Experience as lake to see the\\nOverview\\nOneLake Storage'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 334, 'page_label': '335'}, page_content=\"cost of OneLake storage. If you have multiple workspaces in the capacity, you can see\\nthe storage per workspace.\\nThe following image, shows two columns: Billable storage and Current Storage. Billable\\nstorage reflects cumulative data usage over the month. Because the total charge for\\ndata stored isn't taken on one day of the month, but on a pro-rated basis throughout\\nthe month. You can estimate the monthly price as the billable storage (GB) multiplied by\\nthe price per GB per month.\\nFor example, storing 1 TB of data on day 1, adds to 33 GB daily billable storage. On day\\none it's 1 TB / 30 days = 33 GB and every day adds 33 GB until the month ends.\\nOneLake soft delete protects individual files from accidental deletion by retaining files\\nfor a default retention period before it's permanently deleted. Soft-deleted data is billed\\nat the same rate as active data.\\nRequests to OneLake (e.g., read, write, or list) consume Fabric capacity. OneLake maps\\nAPIs to operations like ADLS. Capacity usage for each operation is visible in the Capacity\\nMetrics app. In the above example, the file upload resulted in a write transaction\\nconsuming 127.46 CU seconds. This consumption is reported as OneLake Write via\\nProxy under the operation name column in the capacity metrics App.\\n\\uf80a\\n\\uf80a\\nOneLake Compute\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 335, 'page_label': '336'}, page_content='Now if you read this data using a notebook. You consume 1.39 CU seconds of read\\ntransactions. This consumption is reported as OneLake Read via Redirect in the metrics\\napp. See OneLake consumption page to learn how each type of operation consumes\\ncapacity units.\\nTo understand more about the various terminologies on the metrics app, see\\nUnderstand the metrics app compute page - Microsoft Fabric.\\nYou may be wondering, how do shortcuts affect my OneLake usage? In the above\\nexample, both storage and compute are billed to Capacity1. Now, let’s say you have a\\nsecond capacity Capacity2, that contains Workspace2. You create a lakehouse and create\\na shortcut to the parquet file you uploaded in Workspace1. You create a notebook to\\nquery the parquet file. As Capacity2 accesses the data, the compute or transaction cost\\nfor this read operation consumes CU from Capacity2. The storage continues to be billed\\nto Capacity1.\\nIf Capacity2 is paused but Capacity1 is active, you can’t read the data via the\\nshortcut in Workspace2 (Capacity2) but can access the data directly in Workspace1\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 336, 'page_label': '337'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\n(Capacity1).\\nIf Capacity1 is paused and Capacity2 is active, you can’t read the data in\\nWorkspace1 (Capacity1) but you can still use the data using the shortcut in\\nWorkspace2. In both cases, as the data is still stored in Capacity1, storage costs\\nremain billed to Capacity1\\nIf your CU consumption exceeds the capacity limit, throttling may occur, causing\\ntransactions to be delayed or rejected temporarily.\\nStart Fabric’s 60-day free trial to explore OneLake and other features, and visit the Fabric\\nforum for questions.\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 337, 'page_label': '338'}, page_content=\"Explore OneLake events in Fabric Real-Time\\nhub\\n07/22/2025\\nOneLake events inform you about changes in your data lake, such as the creation, modification,\\nor deletion of files and folders.\\nReal-Time Hub enables you to discover and subscribe to these changes within OneLake,\\nallowing you to react instantly. For instance, you can monitor changes in Lakehouse files and\\nfolders and utilize Activator's alerting capabilities to set up alerts based on specific conditions\\nand define actions to take when those conditions are met. This article guides you on how to\\nexplore OneLake events using the Real-Time Hub\\n1. In Real-Time hub, select Fabric events.\\n2. Select OneLake events from the list.\\n3. You should see the detail view for OneLake events.\\n７ Note\\nConsuming Fabric and Azure events via Eventstream or Fabric Activator isn't supported if\\nthe capacity region of the Eventstream or Activator is in the following regions: West India,\\nIsrael Central, Korea Central, Qatar Central, Singapore, UAE Central, Spain Central, Brazil\\nSoutheast, Central US, South Central US, West US 2, West US 3.\\nView OneLake events detail page\\n\\uf80a\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 338, 'page_label': '339'}, page_content=\"At the top of the detail page, you see the following two actions.\\nCreate eventstream, which lets you create an eventstream based on events from the\\nselected OneLake item.\\nSet alert, which lets you set an alert when an operation is done for a OneLake item, such\\nas a new file is created.\\nThese actions are also available in the Fabric events list view.\\nThis section shows the artifacts using OneLake events. Here are the columns and their\\ndescriptions:\\n\\uf80a\\nActions\\nSee what's using this category\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 339, 'page_label': '340'}, page_content=\"Column Description\\nName Name of the artifact that's using OneLake events.\\nType Artifact type – Activator or Eventstream\\nWorkspace Workspace where the artifact lives.\\nSource Name of the workspace that is source of the events.\\nHere are the supported OneLake events:\\nEvent type name Description\\nMicrosoft.Fabric.OneLake.FileCreated Raised when a file is created or replaced in OneLake.\\nMicrosoft. Fabric.OneLake.FileDeleted Raised when a file is deleted in OneLake.\\nMicrosoft. Fabric.OneLake.FileRenamed Raised when a file is renamed in OneLake.\\nMicrosoft.Fabric.OneLake.FolderCreatedRaised created when a folder is created in OneLake.\\nMicrosoft. Fabric.OneLake.FolderDeletedRaised when a folder is deleted in OneLake.\\nOneLake events profile\\nEvent types\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 340, 'page_label': '341'}, page_content=\"Event type name Description\\nMicrosoft. Fabric.OneLake.FolderRenamed Raised when a folder is renamed in OneLake.\\nAn event has the following top-level data:\\nProperty Type Description Example\\nsource string Identifies the\\ncontext in which\\nan event\\nhappened.\\n/aaaaaaaa-0000-1111-2222-\\nbbbbbbbbbbbb/workspaces/bbbbbbbb-1111-2222-\\n3333-cccccccccccc/items/cccccccc-2222-3333-\\n4444-dddddddddddd\\nsubject string Identifies the\\nsubject of the\\nevent in the\\ncontext of the\\nevent producer.\\n/Files/FolderA/FileName.txt\\ntype string One of the\\nregistered event\\ntypes for this\\nevent source.\\nMicrosoft.Fabric.OneLake.FileCreated\\ntime timestampThe time the event\\nis generated\\nbased on the\\nprovider's UTC\\ntime.\\n2017-06-26T18:41:00.9584103Z\\nid string Unique identifier\\nfor the event.\\nbbbbbbbb-1111-2222-3333-cccccccccccc\\ndata object Event data. See the next table for details.\\ndataschemaversion string The version of the\\ndata schema.\\n1.0\\nspecversion string The version of the\\nCloud Event spec.\\n1.0\\nThe data object has the following properties:\\nSchemas\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 341, 'page_label': '342'}, page_content='Property Type Description Example\\neTag string The value that\\nyou can use to\\nrun operations\\nconditionally.\\n\"\\\\\"0x8D4BCC2E4835CD0\\\\\"\\ncontentLength string Size of the file\\nin bytes.\\n0\\ncontentType string Content type\\nspecified for the\\nfile.\\ntext/plain\\nblobUrl string Blob URL to the\\npath of the file.\\nhttps://onelake.blob.fabric.microsoft.com/55556666-\\nffff-7777-aaaa-8888bbbb9999 < 66667777-aaaa-8888-\\nbbbb-9999cccc0000/Files/FolderA/File1.txt\\nurl string OneLake URL to\\nthe path of the\\nfile.\\nhttps://onelake.dfs.fabric.microsoft.com/eeeeeeee-\\n4444-5555-6666-ffffffffffff < aaaaaaaa-6666-7777-\\n8888-bbbbbbbbbbbb/Files/FolderA/File1.txt\\napi string The operation\\nthat triggered\\nthe event.\\nCreateFile\\nclientRequestId string A client-\\nprovided\\nrequest ID for\\nthe storage API\\noperation.\\naaaabbbb-0000-cccc-1111-dddd2222eeee\\nrequestId string Service-\\ngenerated\\nrequest ID for\\nthe storage API\\noperation.\\naaaabbbb-0000-cccc-1111-dddd2222eeee\\ncontentOffset numberThe offset in\\nbytes of a write\\noperation taken\\nat the point\\nwhere the\\nevent-triggering\\napplication\\ncompleted\\nwriting to the\\nfile.\\n0\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 342, 'page_label': '343'}, page_content='Property Type Description Example\\nsequencer string An opaque\\nstring value\\nrepresenting the\\nlogical\\nsequence of\\nevents.\\n00000000000004420000000000028963\\nFor more information, see subscribe permission for Fabric events.\\nExplore Azure blob storage events\\nSubscribe permission\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 343, 'page_label': '344'}, page_content=\"Get OneLake events in Fabric Real-Time\\nhub\\n07/22/2025\\nThis article describes how to get OneLake events as an eventstream in Fabric Real-Time hub.\\nReal-Time hub allows you to discover and subscribe to changes in files and folders in OneLake,\\nand then react to those changes in real-time. For example, you can react to changes in files and\\nfolders in Lakehouse and use Activator alerting capabilities to set up alerts based on conditions\\nand specify actions to take when the conditions are met. This article explains how to explore\\nOneLake events in Real-Time hub.\\nWith Fabric event streams, you can capture these OneLake events, transform them, and route\\nthem to various destinations in Fabric for further analysis. This seamless integration of OneLake\\nevents within Fabric event streams gives you greater flexibility for monitoring and analyzing\\nactivities in your OneLake.\\nHere are the supported OneLake events:\\nEvent type name Description\\nMicrosoft.Fabric.OneLake.FileCreated Raised when a file is created or replaced in OneLake.\\nMicrosoft. Fabric.OneLake.FileDeleted Raised when a file is deleted in OneLake.\\nMicrosoft. Fabric.OneLake.FileRenamed Raised when a file is renamed in OneLake.\\nMicrosoft.Fabric.OneLake.FolderCreatedRaised created when a folder is created in OneLake.\\nMicrosoft. Fabric.OneLake.FolderDeletedRaised when a folder is deleted in OneLake.\\nMicrosoft. Fabric.OneLake.FolderRenamed Raised when a folder is renamed in OneLake.\\nFor more information, see Explore OneLake events.\\nEvent types\\nﾉ Expand table\\n７ Note\\nConsuming Fabric and Azure events via Eventstream or Fabric Activator isn't supported if\\nthe capacity region of the Eventstream or Activator is in the following regions: West India,\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 344, 'page_label': '345'}, page_content='Access to a workspace in the Fabric capacity license mode (or) the Trial license mode with\\nContributor or higher permissions.\\nSusbcribeOneLakeEvent permission on the data sources.\\nYou can create streams for OneLake events in Real-Time hub using one of the ways:\\nUsing the Data sources page\\nUsing the Fabric events page\\n1. Sign in to Microsoft Fabric .\\n2. If you see Power BI at the bottom-left of the page, switch to the Fabric workload by\\nselecting Power BI and then by selecting Fabric.\\n3. Select Real-Time on the left navigation bar.\\nIsrael Central, Korea Central, Qatar Central, Singapore, UAE Central, Spain Central, Brazil\\nSoutheast, Central US, South Central US, West US 2, West US 3.\\nPrerequisites\\nCreate streams for OneLake events\\nData sources page'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 345, 'page_label': '346'}, page_content='4. On the Real-Time hub page, select + Data sources under Connect to on the left\\nnavigation menu.\\nYou can also get to the Data sources page from the Real-Time hub page by selecting the\\n+ Add data button in the top-right corner.\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 346, 'page_label': '347'}, page_content='4. On the Data sources page, select OneLake events category at the top, and then select\\nConnect on the OneLake events tile. You can also use the search bar to search for\\nOneLake events.\\nNow, use instructions from the Configure and create an eventstream section.\\n\\uf80a\\n\\uf80a\\nFabric events page'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 347, 'page_label': '348'}, page_content='In Real-Time hub, select Fabric events on the left menu. You can use either the list view of\\nFabric events or the detail view of OneLake events to create an eventstream for OneLake\\nevents.\\nMove the mouse over OneLake events, and select the Create Eventstream link or select ...\\n(Ellipsis) and then select Create Eventstream.\\n1. On the Fabric events page, select OneLake events from the list of Fabric events\\nsupported.\\n2. On the Detail page, select + Create eventstream from the menu.\\nNow, use instructions from the Configure and create an eventstream section, but skip the\\nfirst step of using the Add source page.\\n1. On the Connect page, for Event types, select the event types that you want to monitor.\\nUsing the list view\\n\\uf80a\\nUsing the detail view\\n\\uf80a\\nConfigure and create an eventstream'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 348, 'page_label': '349'}, page_content='2. This step is optional. To see the schemas for event types, select View selected event type\\nschemas. If you select it, browse through schemas for the events, and then navigate back\\nto previous page by selecting the backward arrow button at the top.\\n3. Select Add a OneLake source under Select data source for events.\\n4. On the Choose the data you want to connect page:\\na. View all available data sources or only your data sources (My data) or your favorite\\ndata sources by using the category buttons at the top. You can use the Filter by\\nkeyword text box to search for a specific source. You can also use the Filter button to\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 349, 'page_label': '350'}, page_content=\"filter based on the type of the resource (KQL Database, Lakehouse, SQL Database,\\nWarehouse). The following example uses the My data option.\\nb. Select the data source from the list.\\nc. Select Next at the bottom of the page.\\n5. Select all tables or a specific table that you're interested in, and then select Add.\\n\\uf80a\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 350, 'page_label': '351'}, page_content='6. Now, on the Configure connection settings page, you can add filters to set the filter\\nconditions by selecting fields to watch and the alert value. To add a filter:\\na. Select + Filter.\\nb. Select a field.\\nc. Select an operator.\\nd. Select one or more values to match.\\n\\uf80a\\n７ Note\\nOneLake events are supported for data in OneLake. However, events for data in\\nOneLake via shortcuts are not yet available.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 351, 'page_label': '352'}, page_content='7. In the Stream details section to the right, follow these steps.\\na. Select the workspace where you want to save the eventstream.\\nb. Enter a name for the eventstream. The Stream name is automatically generated for\\nyou.\\n8. Then, select Next at the bottom of the page.\\n9. On the Review + connect page, review settings, and select Connect.\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 352, 'page_label': '353'}, page_content=\"10. When the wizard succeeds in creating a stream, use Open eventstream link to open the\\neventstream that was created for you. Select Finish to close the wizard.\\nSelect Real-Time hub on the left navigation menu, and confirm that you see the stream you\\ncreated. Refresh the page if you don't see it.\\n\\uf80a\\n\\uf80a\\nView stream from the Real-Time hub page\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 353, 'page_label': '354'}, page_content='For detailed steps, see View details of data streams in Fabric Real-Time hub.\\nTo learn about consuming data streams, see the following articles:\\nProcess data streams\\nAnalyze data streams\\nSet alerts on data streams\\n\\uf80a\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 354, 'page_label': '355'}, page_content=\"Set alerts on OneLake events in Real-Time\\nhub\\n10/15/2025\\nThis article describes how to set alerts on OneLake events in Real-Time hub.\\n1. Sign in to Microsoft Fabric .\\n2. If you see Power BI at the bottom-left of the page, switch to the Fabric workload by\\nselecting Power BI and then by selecting Fabric.\\n3. Select Real-Time on the left navigation bar.\\n７ Note\\nConsuming Fabric and Azure events via Eventstream or Fabric Activator isn't supported if\\nthe capacity region of the Eventstream or Activator is in the following regions: West India,\\nIsrael Central, Korea Central, Qatar Central, Singapore, UAE Central, Spain Central, Brazil\\nSoutheast, Central US, South Central US, West US 2, West US 3.\\nNavigate to Real-Time hub\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 355, 'page_label': '356'}, page_content='Do steps from one of the following sections, which opens a side panel where you can configure\\nthe following options:\\nEvents you want to monitor.\\nConditions you want to look for in the events.\\nAction you want Activator to take.\\n1. In Real-Time hub, select Fabric events.\\n2. Move the mouse over OneLake events, and do one of the following steps:\\nSelect the Alert button.\\nSelect ellipsis (...), and select Set alert.\\n\\uf80a\\nLaunch the Set alert page\\nUsing the events list\\n\\uf80a\\nUsing the event detail page'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 356, 'page_label': '357'}, page_content='1. Select OneLake events from the list see the detail page.\\n2. On the detail page, select Set alert button at the top of page.\\nOn the Add rule page, in the Details section, for Rule name, enter a name for the rule.\\n1. In the Monitor section, for Source, choose Select source events.\\n\\uf80a\\nDetails section\\nMonitor section'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 357, 'page_label': '358'}, page_content='2. In the Connect data source wizard, do these steps:\\na. For Event types, select event types that you want to monitor.\\nb. Select Add a OneLake source button in the Select data source for events section.\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 358, 'page_label': '359'}, page_content='c. On the Choose the data you want to connect page:\\ni. View all available data sources or only your data sources (My data) or your favorite\\ndata sources by using the category buttons at the top. You can use the Filter by\\nkeyword text box to search for a specific source. You can also use the Filter button\\nto filter based on the type of the resource (KQL Database, Lakehouse, SQL Database,\\nWarehouse). The following example uses the My data option.\\nii. Select the data source from the list.\\niii. Select Next at the bottom of the page.\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 359, 'page_label': '360'}, page_content=\"iv. Select all tables or a specific table that you're interested in, and then select Add.\\nd. Now, on the Configure connection settings page, you can add filters to set the filter\\nconditions by selecting fields to watch and the alert value. To add a filter:\\ni. Select + Filter.\\nii. Select a field.\\niii. Select an operator.\\niv. Select one or more values to match.\\n\\uf80a\\n７ Note\\nOneLake events are supported for data in OneLake. However, events for data in\\nOneLake via shortcuts aren't yet available.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 360, 'page_label': '361'}, page_content='e. Select Next at the bottom of the page.\\nf. On the Review + connect page, review the settings, and select Save.\\nIn the Condition section, for Check, select On each event.\\n\\uf80a\\nCondition section'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 361, 'page_label': '362'}, page_content='In the Action section, select one of the following actions:\\nTo configure the alert to send an email when the condition is met, follow these steps:\\n1. For Select action, select Send email.\\n2. For To, enter the email address of the receiver or use the drop-down list to select a\\nproperty whose value is an email address.\\n3. For Subject, enter a subject for the email.\\n4. For Headline, enter a headline for the email.\\n5. For Notes, enter notes for the emails.\\n6. For Context, select values from the drop-down list that you want to include in the\\ncontext.\\nAction section\\nEmail\\n７ Note\\nWhen entering subject, headline, or notes, you can refer to properties in the data by\\ntyping @ or by selecting the button next to the text boxes. For example,\\n@BikepointID.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 362, 'page_label': '363'}, page_content='To configure the alert to send a Teams message to an individual or a group chat or a channel\\nwhen the condition is met, follow these steps:\\n1. For Select action, select Teams -> Message to individuals or Group chat message, or\\nChannel post.\\n2. Follow one of these steps depending on the option you selected in the previous step:\\nIf you selected the Message to individuals option, enter email addresses of\\nreceivers or use the drop-down list to select a property whose value is an email\\naddress. When the condition is met, an email is sent to specified individuals.\\nIf you selected the Group chat message option, select a group chat from the drop-\\ndown list. When the condition is met, a message is posted to the group chat.\\nIf you selected the Channel post option, select a team and a channel. When the\\ncondition is met, a message is posted in the channel.\\n3. For Headline, enter a headline for the email.\\n4. For Notes, enter notes for the emails.\\n5. For Context, select values from the drop-down list that you want to include in the\\ncontext.\\nTeams message\\n７ Note\\nWhen entering headline, or notes, you can refer to properties in the data by typing @\\nor by selecting the button next to the text boxes. For example, @BikepointID.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 363, 'page_label': '364'}, page_content='To configure the alert to launch a Fabric item (pipeline, notebook, Spark job, etc.) when the\\ncondition is met, follow these steps:\\n1. For Selection action, select Run a Fabric item.\\n2. Choose Select Fabric item to run, and then select the Fabric item from the list.\\n3. Select Add parameter and specify the name of the parameter for the Fabric item and a\\nvalue for it. You can add more than one parameter.\\nRun a Fabric item'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 364, 'page_label': '365'}, page_content=\"In the Save location section, for Workspace, select the workspace where you want to Fabric\\nactivator item to be created or that already exists. If you're creating a new activator item, enter\\na name for the activator item.\\nSave location section\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 365, 'page_label': '366'}, page_content='1. Select Create at the bottom of the page to create the alert.\\nCreate alert'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 366, 'page_label': '367'}, page_content='2. You see the Alert created page with a link to open the rule in the Fabric activator user\\ninterface in a separate tab. Select Done to close the Alert created page.'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 367, 'page_label': '368'}, page_content='3. You see a page with the activator item created by the Add rule wizard. If you are on the\\nFabric events page, select Job events to see this page.\\n\\uf80a'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 368, 'page_label': '369'}, page_content=\"4. Move the mouse over the Activator item, and select Open.\\n5. You see the Activator item in the Fabric Activator editor user interface. Select the rule if\\nit's not already selected. You can update the rule in this user interface. For example,\\nupdate the subject, headline, or change the action from email to Teams message.\\n\\uf80a\\n\\uf80a\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 369, 'page_label': '370'}, page_content='Set alerts on Azure blob storage events\\nSet alerts on Fabric workspace item events\\n\\uf80a\\nRelated content'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 370, 'page_label': '371'}, page_content='OneLake Shortcuts\\nService:Core\\nAPI Version:v1\\nCreate Shortcut Creates a new shortcut or updates an existing shortcut.\\nCreates Shortcuts In\\nBulk\\nCreates bulk shortcuts.\\nDelete Shortcut Deletes the shortcut but does not delete the destination storage folder.\\nGet Shortcut Returns shortcut properties.\\nList Shortcuts Returns a list of shortcuts for the item, including all the subfolders\\nexhaustively.\\nReset Shortcut CacheDeletes any cached files that were stored while reading from shortcuts.\\nOperations\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 371, 'page_label': '372'}, page_content='OneLake Data Access Security\\nService:Core\\nAPI Version:v1\\nCreate Or Update Data Access Roles Creates or updates data access roles in OneLake.\\nList Data Access Roles Returns a list of OneLake roles.\\nOperations\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 372, 'page_label': '373'}, page_content='OneLake security for SQL analytics\\nendpoints (Preview)\\nWith OneLake security, Microsoft Fabric is expanding how organizations can manage and\\nenforce data access across workloads. This new security framework gives administrators greater\\nflexibility to configure permissions. Administrators can choose between centralized\\ngovernance through OneLake or granular SQL-based control within the SQL analytics\\nendpoint.\\nWhen using the SQL analytics endpoint, the selected access mode determines how data\\nsecurity is enforced. Fabric supports two distinct access models, each offering different benefits\\ndepending on your operational and compliance needs:\\nUser identity mode: Enforces security using OneLake roles and policies. In this mode, the\\nSQL analytics endpoint passes the signed-in user’s identity to OneLake, and read access is\\ngoverned entirely by the security rules defined within OneLake. SQL-level permissions\\non tables are supported, ensuring consistent governance across tools like Power BI,\\nnotebooks, and lakehouse.\\nDelegated identity mode: Provides full control through SQL. In this mode, the SQL\\nanalytics endpoint connects to OneLake using the identity of the workspace or item\\nowner, and security is governed exclusively by SQL permissions defined inside the\\ndatabase. This model supports traditional security approaches including GRANT, REVOKE,\\ncustom roles, Row-Level Security, and Dynamic Data Masking.\\nEach mode supports different governance models. Understanding their implications is essential\\nfor choosing the right approach in your Fabric environment.\\nHere’s a clear and concise comparison table focused on how and where you set security in user\\nidentity mode versus delegated identity mode—broken down by object type and data access\\npolicies:\\nAccess modes in SQL analytics endpoint\\nComparison between access modes\\nﾉ Expand table'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 373, 'page_label': '374'}, page_content=\"Security target User identity mode Delegated identity mode\\nTables Access is controlled by OneLake security roles.\\nSQL GRANT/REVOKE isn't allowed.\\nFull control using SQL\\nGRANT/REVOKE.\\nViews Use SQL GRANT/REVOKE to assign\\npermissions.\\nUse SQL GRANT/REVOKE to\\nassign permissions.\\nStored procedures Use SQL GRANT EXECUTE to assign\\npermissions.\\nUse SQL GRANT EXECUTE to\\nassign permissions.\\nFunctions Use SQL GRANT EXECUTE to assign\\npermissions.\\nUse SQL GRANT EXECUTE to\\nassign permissions.\\nRow-Level Security\\n(RLS)\\nDefined in OneLake UI as part of OneLake\\nsecurity roles.\\nDefined using SQL CREATE\\nSECURITY POLICY.\\nColumn-Level\\nSecurity (CLS)\\nDefined in OneLake UI as part of OneLake\\nsecurity roles.\\nDefined using SQL GRANT\\nSELECT with column list.\\nDynamic Data\\nMasking (DDM)\\nNot supported in OneLake security. Defined using SQL ALTER TABLE\\nwith MASKED option.\\nIn user identity mode, the SQL analytics endpoint uses a passthrough authentication\\nmechanism to enforce data access. When a user connects to the SQL analytics endpoint, their\\nEntra ID identity is passed through to OneLake, which performs the permission check. All read\\noperations against tables are evaluated using the security rules defined within the OneLake\\nLakehouse, not by any SQL-level GRANT or REVOKE statements.\\nThis mode lets you manage security centrally, ensuring consistent enforcement across all Fabric\\nexperiences, including Power BI, notebooks, lakehouse, and SQL analytics endpoint. It's\\ndesigned for governance models where access should be defined once in OneLake and\\nautomatically respected everywhere.\\nIn user identity mode:\\nTable access is governed entirely by OneLake security. SQL GRANT/REVOKE statements\\non tables are ignored.\\nRLS (Row-Level Security), CLS (Column-Level Security), and Object-Level Security are all\\ndefined in the OneLake experience.\\nSQL permissions are allowed for nondata objects like views, stored procedures, and\\nfunctions, enabling flexibility for defining custom logic or user-facing entry points to data.\\nUser identity mode in OneLake security\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 374, 'page_label': '375'}, page_content='Write operations aren\\'t supported at the SQL analytics endpoint. All writes must occur\\nthrough the Lakehouse UI and are governed by workspace roles (Admin, Member,\\nContributor).\\nUsers with the Admin, Member, or Contributor role at the workspace level aren\\'t subject to\\nOneLake security enforcement. These roles have elevated privileges and will bypass RLS, CLS,\\nand OLS policies entirely. Follow these requirements to ensure OneLake security is respected:\\nAssign users the Viewer role in the workspace, or\\nShare the Lakehouse or SQL analytics endpoint with users using read-only permissions.\\nOnly users with read-only access have their queries filtered according to OneLake security\\nroles.\\nIf a user belongs to multiple OneLake roles, the most permissive role defines their effective\\naccess. For example:\\nIf one role grants full access to a table and another applies RLS to restrict rows, the RLS\\nwill not be enforced.\\nThe broader access role takes precedence. This behavior ensures users aren\\'t\\nunintentionally blocked, but it requires careful role design to avoid conflicts. It\\'s\\nrecommended to keep restrictive and permissive roles mutually exclusive when enforcing\\nrow- or column-level access controls.\\nFor more information, see the data access control model for OneLake security.\\n） Important\\nThe SQL Analytics Endpoint requires a one-to-one mapping between item permissions\\nand members in a OneLake security role to sync correctly. If you grant an identity access\\nto a OneLake security role, that same identity needs to have Fabric Read permission to the\\nlakehouse as well. For example, if a user assigns \"user123@microsoft.com\" to a OneLake\\nsecurity role then \"user123@microsoft.com\" must also be assigned to that lakehouse.\\nWorkspace role behavior\\nRole precedence: Most permissive access wins'),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 375, 'page_label': '376'}, page_content=\"A critical component of user identity mode is the security sync service. This background\\nservice monitors changes made to security roles in OneLake and ensures those changes are\\nreflected in the SQL analytics endpoint.\\nThe security sync service is responsible for the following:\\nDetecting changes to OneLake roles, including new roles, updates, user assignments, and\\nchanges to tables.\\nTranslating OneLake-defined policies (RLS, CLS, OLS) into equivalent SQL-compatible\\ndatabase role structures.\\nEnsuring shortcut objects (tables sourced from other lakehouses) are properly validated\\nso that the original OneLake security settings are honored, even when accessed remotely.\\nThis synchronization ensures that OneLake security definitions stay authoritative, eliminating\\nthe need for manual SQL-level intervention to replicate security behavior. Because security is\\ncentrally enforced:\\nYou can't define RLS, CLS, or OLS directly using T-SQL in this mode.\\nYou can still apply SQL permissions to views, functions, and stored procedures using\\nGRANT or EXECUTE statements.\\nScenario Behavior in user\\nidentity mode\\nBehavior in\\ndelegated\\nmode\\nCorrective\\naction\\nNotes\\nRLS policy\\nreferences a\\ndeleted or\\nrenamed\\ncolumn\\nError: *Row-level\\nsecurity policy\\nreferences a column\\nthat no longer\\nexists.*Database\\nenters error state\\nuntil policy is fixed.\\nError: Invalid\\ncolumn name\\n<column\\nname>\\nUpdate or\\nremove one or\\nmore affected\\nroles, or restore\\nthe missing\\ncolumn.\\nThe update will need to be\\nmade in the lakehouse\\nwhere the role was\\ncreated.\\nCLS policy\\nreferences a\\nError: *Column-level\\nsecurity policy\\nError: Invalid\\ncolumn name\\nUpdate or\\nremove one or\\nThe update will need to be\\nmade in the lakehouse\\nSecurity sync between OneLake and SQL analytics\\nendpoint\\nSecurity sync errors & resolution\\nﾉ Expand table\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 376, 'page_label': '377'}, page_content=\"Scenario Behavior in user\\nidentity mode\\nBehavior in\\ndelegated\\nmode\\nCorrective\\naction\\nNotes\\ndeleted or\\nrenamed\\ncolumn\\nreferences a column\\nthat no longer\\nexists.*Database\\nenters error state\\nuntil policy is fixed.\\n<column\\nname>\\nmore affected\\nroles, or restore\\nthe missing\\ncolumn.\\nwhere the role was\\ncreated.\\nRLS/CLS policy\\nreferences a\\ndeleted or\\nrenamed table\\nError: Security policy\\nreferences a table\\nthat no longer exists.\\nNo error\\nsurfaced;\\nquery fails\\nsilently if\\ntable is\\nmissing.\\nUpdate or\\nremove one or\\nmore affected\\nroles, or restore\\nthe missing\\ntable.\\nThe update will need to be\\nmade in the lakehouse\\nwhere the role was\\ncreated.\\nDDM (Dynamic\\nData Masking)\\npolicy\\nreferences a\\ndeleted or\\nrenamed\\ncolumn\\nDDM not supported\\nfrom OneLake\\nSecurity, must be\\nimplemented\\nthrough SQL.\\nError: Invalid\\ncolumn name\\n<column\\nname>\\nUpdate or\\nremove one or\\nmore affected\\nDDM rules, or\\nrestore the\\nmissing column.\\nUpdate the DDM policy in\\nthe SQL Analytics\\nEndpoint.\\nSystem error\\n(unexpected\\nfailure)\\nError: An unexpected\\nsystem error\\noccurred. Try again or\\ncontact support.\\nError: An\\ninternal error\\nhas occurred\\nwhile\\napplying\\ntable changes\\nto SQL.\\nRetry operation;\\nif issue persists,\\ncontact\\nMicrosoft\\nSupport.\\nN/A\\nUser doesn't\\nhave\\npermission on\\nthe artifact\\nError: User doesn't\\nhave permission on\\nthe artifact\\nError: User\\ndoesn't have\\npermission\\non the\\nartifact\\nProvide user with\\nobjectID\\n{objectID}\\npermission to the\\nartifact.\\nThe object ID must be an\\nexact match between the\\nOneLake security role\\nmember and the Fabric\\nitem permissions. If a\\ngroup is added to the role\\nmembership, then that\\nsame group must be given\\nthe Fabric Read\\npermission. Adding a\\nmember from that group\\nto the item does not count\\nas a direct match.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 377, 'page_label': '378'}, page_content=\"Scenario Behavior in user\\nidentity mode\\nBehavior in\\ndelegated\\nmode\\nCorrective\\naction\\nNotes\\nUser principal\\nis not\\nsupported.\\nError: User principal\\nis not supported.\\nError: User\\nprincipal is\\nnot\\nsupported.\\nPlease remove\\nuser {username}\\nfrom role\\nDefaultReader\\nThis error occurs if the\\nuser is no longer a valid\\nEntra ID, such as if the user\\nhas left your organization\\nor been deleted. Remove\\nthem from the role to\\nresolve this error.\\nOneLake security is enforced at the source of truth, so security sync disables ownership\\nchaining for tables and views involving shortcuts. This ensures that source system permissions\\nare always evaluated and honored, even for queries from another database.\\nAs a result:\\nUsers must have valid access on both the shortcut source (current Lakehouse or SQL\\nanalytics endpoint) and the destination where the data physically resides.\\nIf the user lacks permission on either side, queries will fail with an access error.\\nWhen designing your applications or views that reference shortcuts, ensure that role\\nassignments are properly configured on both ends of the shortcut relationship.\\nThis design preserves security integrity across Lakehouse boundaries, but it introduces\\nscenarios where access failures might occur if cross-Lakehouse roles aren't aligned.\\nIn Delegated Identity Mode, the SQL analytics endpoint uses the same security model that\\nexists today in Microsoft Fabric. Security and permissions are managed entirely at the SQL\\nlayer, and OneLake roles or access policies aren't enforced for table-level access. When a user\\nconnects to the SQL analytics endpoint and issues a query:\\nSQL validates access based on SQL permissions (GRANT, REVOKE, RLS, CLS, DDM, roles,\\netc.).\\nIf the query is authorized, the system proceeds to access data stored in OneLake.\\nThis data access is performed using the identity of the Lakehouse or SQL analytics\\nendpoint owner—also known as the item account.\\nShortcuts behavior with security sync\\nDelegated mode in OneLake security\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 378, 'page_label': '379'}, page_content=\"In this model:\\nThe signed-in user isn't passed through to OneLake.\\nAll enforcement of access is assumed to be handled at the SQL layer.\\nThe item owner is responsible for having sufficient permissions in OneLake to read the\\nunderlying files on behalf of the workload.\\nBecause this is a delegated pattern, any misalignment between SQL permissions and OneLake\\naccess for the owner results in query failures. This mode provides full compatibility with:\\nSQL GRANT/REVOKE at all object levels\\nSQL-defined Row-Level Security, Column-Level Security, and Dynamic Data Masking\\nExisting T-SQL tooling and practices used by DBAs or applications\\nThe access mode determines how data access is authenticated and enforced when querying\\nOneLake through SQL analytics endpoint. You can switch between user identity mode and\\ndelegated identity mode using the following steps:\\n1. Navigate to your Fabric workspace and open your lakehouse. From top right hand corner,\\nswitch from lakehouse to SQL analytics endpoint.\\n2. From the top navigation, go to Security tab and select the one of the following OneLake\\naccess modes:\\nUser identity – Uses the signed-in user's identity. It enforces OneLake roles.\\nDelegated identity – Uses the item owner's identity; enforces only SQL permissions.\\n3. A pop-up launches to confirm your selection. Select Yes to confirm the change.\\nSwitching to user identity mode\\nSQL RLS, CLS, and table-level permissions are ignored.\\nOneLake roles must be configured for users to maintain access.\\nHow to change the OneLake access mode\\nConsiderations when switching between modes\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 379, 'page_label': '380'}, page_content=\"Only users with Viewer permissions or shared read-only access will be governed by\\nOneLake security.\\nExisting SQL Roles are deleted and can't be recovered.\\nSwitching to delegated identity mode\\nOneLake roles and security policies are no longer applied.\\nSQL roles and security policies become active.\\nThe item owner must have valid OneLake access, or all queries may fail.\\nApplies only to readers: OneLake Security governs users accessing data as Viewers. Users\\nin other workspace roles (Admin Member, or Contributor) bypass OneLake Security and\\nretain full access.\\nSQL objects do not inherit ownership: Shortcuts are surfaced in SQL analytics endpoint\\nas tables. When accessing these tables, directly or through views, stored procedures, and\\nother derived SQL objects don't carry object-level ownership; all permissions are checked\\nat runtime to prevent the security bypass.\\nShortcut changes trigger validation downtime: When a shortcut target changes (for\\nexample, rename, URL update), the database enters single-user mode briefly while the\\nsystem validates the new target. During this period queries are blocked, these operations\\na fairly quick, but sometimes depending on different internal process can take up to 5\\nminutes to synchronize.\\nCreating schema shortcuts might cause a known error that affects validation and\\ndelays metadata sync.\\nDelayed permission propagation: Permission changes aren't instantaneous. Switching\\nbetween security modes (User Identity vs. Delegated) may require time to propagate\\nbefore taking effect, but should take less than 1 minute.\\nControl-plane dependency: Permissions can't be applied to users or groups that don't\\nalready exist in the workspace control plane. You either need to share the source item, or\\nthe user must be member of Viewer workspace role. Note that the exact same object ID\\nmust be in both places. A group and a member of that group do not count as a match.\\nMost-permissive access prevails: When users belong to multiple groups or roles, the\\nmost permissive effective permission is honored Example: If a user has both DENY\\nthrough one role and GRANT through another, the GRANT takes precedence.\\nLimitations\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 380, 'page_label': '381'}, page_content=\"Delegated mode limitations: In Delegated mode, metadata sync on shortcut tables can\\nfail if the source item has OneLake Security policies that don't grant full table access to\\nthe item owner.\\nDENY behavior: When multiple roles apply to a single shortcut table, the intersection of\\npermissions follows SQL Server semantics: DENY overrides GRANT. This can produce\\nunexpected access results.\\nExpected error conditions: Users may encounter errors in scenarios such as:\\nShortcut target renamed or invalid\\nExample: If the source of table was deleted.\\nRLS (Row-Level Security) misconfiguration\\nSome expressions for RLS filtering aren't supported in OneLake and it might allow\\nunauthorized data access.\\nDropping the column used on the filter expression invalidates the RLS and\\nMetadata Sync will be stale until the RLS is fixed on OneLake Security Panel.\\nFor Public Preview, we only support single expression tables. Dynamic RLS and\\nMulti-Table RLS aren't supported at the moment.\\nColumn-Level Security (CLS) limitations\\nCLS works by maintaining an allowlist of columns. If an allowed column is removed\\nor renamed, the CLS policy becomes invalid.\\nWhen CLS is invalid, metadata sync is blocked until the CLS rule is fixed in the\\nOneLake Security panel.\\nMetadata or permission sync failure\\nIf there are changes to the table, like renaming a column, security isn't replicated on\\nthe new object, and you receive UI errors showing that the column doesn't exist.\\nTable renames do not preserve security policies: If OneLake Security (OLS) roles are\\ndefined on Schema level, those roles remain in effect only as long as the table name is\\nunchanged. Renaming the table breaks the association, and security policies won't be\\nmigrated automatically. This can result in unintended data exposure until policies are\\nreapplied.\\nOneLake security roles can't have names longer than 124 characters; otherwise, security\\nsync can't synchronize the roles.\"),\n",
       " Document(metadata={'producer': 'Microsoft Learn PDF 1.0.25309.01', 'creator': 'Microsoft Learn', 'creationdate': '2025-12-12T17:01:10+00:00', 'title': 'fabric onelake | Microsoft Learn', 'moddate': '2025-12-12T17:01:10+00:00', 'source': './Docs/fabric-onelake.pdf', 'total_pages': 382, 'page': 381, 'page_label': '382'}, page_content=\"OneLake security roles are propagated on the SQL analytics endpoint with the OLS_\\nprefix.\\nUser changes on the OLS_ roles are not supported, and can cause unexpected behaviors.\\nMail enabled security groups and distribution lists are not supported.\\nThe owner of the lakehouse must be a member of the admin, member, or contributor\\nworkspace roles; otherwise, security isn't applied to the SQL analytics endpoint.\\nThe owner of the lakehouse cannot be a service principal for security sync to work.\\nBest practices to secure data in OneLake\\nOneLake security access control model\\nLast updated on 10/30/2025\\nRelated content\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"./Docs/fabric-onelake.pdf\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c51c7ee",
   "metadata": {},
   "source": [
    "### **Creating own Metadata for PDF Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c92d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in docs:\n",
    "\n",
    "    i.metadata = {\"source\": \"fabric-onelake.pdf\",\n",
    "                  \"developer\": \"Microsoft\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec08d1a",
   "metadata": {},
   "source": [
    "#### **STEP 2: Splitting the Document into CHUNKS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eaa6635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Tell us about your PDF experience.\\nOneLake in Microsoft Fabric\\ndocumentation\\nOneLake is a single, unified, logical data lake for the whole organization. OneLake comes\\nautomatically with every Microsoft Fabric tenant with no infrastructure to manage.\\nAbout OneLake\\nｅOVERVIEW\\nWhat is OneLake?\\nOneLake security\\nOneLake catalog\\nOneLake access and APIs\\n｀DEPLOY\\nImplement medallion lakehouse architecture\\nｂGET STARTED\\nCreate a lakehouse with OneLake\\nOneLake file explorer\\nFind data in the OneLake catalog\\nUse Iceberg tables in OneLake\\nOneLake shortcuts\\nｐCONCEPT\\nWhat are shortcuts?\\nｂGET STARTED\\nCreate a shortcut\\nｃHOW-TO GUIDE'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Access shortcuts\\nOneLake and Azure integration\\nｃHOW-TO GUIDE\\nIntegrate OneLake with Azure Databricks\\nIntegrate OneLake with Azure HDInsight\\nIntegrate OneLake with Azure Storage Explorer\\nIntegrate OneLake with Azure Synapse Analytics'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake, the OneDrive for data\\nArticle• 07/25/2024\\nOneLake is a single, unified, logical data lake for your whole organization. A data Lake\\nprocesses large volumes of data from various sources. Like OneDrive, OneLake comes\\nautomatically with every Microsoft Fabric tenant and is designed to be the single place\\nfor all your analytics data. OneLake brings customers:\\nOne data lake for the entire organization\\nOne copy of data for use with multiple analytical engines\\nBefore OneLake, it was easier for customers to create multiple lakes for different\\nbusiness groups rather than collaborating on a single lake, even with the extra overhead\\nof managing multiple resources. OneLake focuses on removing these challenges by\\nimproving collaboration. Every customer tenant has exactly one OneLake. There can\\nnever be more than one and if you have Fabric, there can never be zero. Every Fabric\\ntenant automatically provisions OneLake, with no extra resources to set up or manage.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"tenant automatically provisions OneLake, with no extra resources to set up or manage.\\nThe concept of a tenant is a unique benefit of a SaaS service. Knowing where a\\ncustomer’s organization begins and ends provides a natural governance and compliance\\nboundary, which is under the control of a tenant admin. Any data that lands in OneLake\\nis governed by default. While all data is within the boundaries set by the tenant admin,\\nit's important that this admin doesn't become a central gatekeeper preventing other\\nparts of the organization from contributing to OneLake.\\nWithin a tenant, you can create any number of workspaces. Workspaces enable different\\nparts of the organization to distribute ownership and access policies. Each workspace is\\npart of a capacity that is tied to a specific region and is billed separately.\\nOne data lake for the entire organization\\nGoverned by default with distributed ownership for\\ncollaboration\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Within a workspace, you can create data items and you access all data in OneLake\\nthrough data items. Similar to how Office stores Word, Excel, and PowerPoint files in\\nOneDrive, Fabric stores lakehouses, warehouses, and other items in OneLake. Items can\\ngive tailored experiences for each persona, such the Apache Spark developer experience\\nin a lakehouse.\\nFor more information on how to get started using OneLake, see Creating a lakehouse\\nwith OneLake.\\nOneLake is open at every level. OneLake is built on top of Azure Data Lake Storage\\n(ADLS) Gen2 and can support any type of file, structured or unstructured. All Fabric data\\nitems like data warehouses and lakehouses store their data automatically in OneLake in\\nDelta Parquet format. If a data engineer loads data into a lakehouse using Apache Spark,\\nand then a SQL developer uses T-SQL to load data in a fully transactional data\\nwarehouse, both are contributing to the same data lake. OneLake stores all tabular data\\nin Delta Parquet format.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"in Delta Parquet format.\\nOneLake supports the same ADLS Gen2 APIs and SDKs to be compatible with existing\\nADLS Gen2 applications, including Azure Databricks. You can address data in OneLake as\\nif it's one big ADLS storage account for the entire organization. Every workspace\\nappears as a container within that storage account, and different data items appear as\\nfolders within those containers.\\n\\uf80a\\nOpen at every level\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='For more information on APIs and endpoints, see OneLake access and APIs. For\\nexamples of OneLake integrations with Azure, see Azure Synapse Analytics, Azure\\nstorage explorer, Azure Databricks, and Azure HDInsight articles.\\nOneLake is the OneDrive for data. Just like OneDrive, you can easily explore OneLake\\ndata from Windows using the OneLake file explorer for Windows. You can navigate all\\nyour workspaces and data items, easily uploading, downloading, or modifying files just\\nlike you do in Office. The OneLake file explorer simplifies working with data lakes,\\nallowing even nontechnical business users to use them.\\nFor more information, see OneLake file explorer.\\nOneLake aims to give you the most value possible out of a single copy of data without\\ndata movement or duplication. You no longer need to copy data just to use it with\\nanother engine or to break down silos so you can analyze the data with data from other\\nsources.\\n\\uf80a\\nOneLake file explorer for Windows\\nOne copy of data'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='sources.\\n\\uf80a\\nOneLake file explorer for Windows\\nOne copy of data\\nShortcuts connect data across domains without data\\nmovement'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Shortcuts allow your organization to easily share data between users and applications\\nwithout having to move and duplicate information unnecessarily. When teams work\\nindependently in separate workspaces, shortcuts enable you to combine data across\\ndifferent business groups and domains into a virtual data product to fit a user’s specific\\nneeds.\\nA shortcut is a reference to data stored in other file locations. These file locations can be\\nwithin the same workspace or across different workspaces, within OneLake or external to\\nOneLake in ADLS, S3, or Dataverse — with more target locations coming soon. No\\nmatter the location, shortcuts make files and folders look like you have them stored\\nlocally.\\nFor more information on how to use shortcuts, see OneLake shortcuts.\\nWhile applications might have separation of storage and computing, the data is often\\noptimized for a single engine, which makes it difficult to reuse the same data for'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"optimized for a single engine, which makes it difficult to reuse the same data for\\nmultiple applications. With Fabric, the different analytical engines (T-SQL, Apache Spark,\\nAnalysis Services, etc.) store data in the open Delta Parquet format to allow you to use\\nthe same data across multiple engines.\\nThere's no longer a need to copy data just to use it with another engine. You're always\\nable to choose the best engine for the job that you're trying to do. For example, imagine\\nyou have a team of SQL engineers building a fully transactional data warehouse. They\\ncan use the T-SQL engine and all the power of T-SQL to create tables, transform data,\\n\\uf80a\\nOne copy of data with multiple analytical engines\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nand load the data to tables. If a data scientist wants to make use of this data, they no\\nlonger need to go through a special Spark/SQL driver. OneLake stores all data in Delta\\nParquet format. Data scientists can use the full power of the Spark engine and its open-\\nsource libraries directly over the data.\\nBusiness users can build Power BI reports directly on top of OneLake using the new\\nDirect Lake mode in the Analysis Services engine. The Analysis Services engine is what\\npowers Power BI semantic models, and it has always offered two modes of accessing\\ndata: import and direct query. Direct Lake mode gives users all the speed of import\\nwithout needing to copy the data, combining the best of import and direct query. For\\nmore information, see Direct Lake.\\nExample diagram showing loading data using Spark, querying using T-SQL, and viewing\\nthe data in a Power BI report.\\nCreating a lakehouse with OneLake\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='the data in a Power BI report.\\nCreating a lakehouse with OneLake\\n\\uf80a\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Bring your data to OneLake with\\nLakehouse\\nArticle• 03/13/2025\\nThis tutorial is a quick guide to creating a lakehouse and getting started with the basic\\nmethods of interacting with it. After completing this tutorial, you'll have a lakehouse\\nprovisioned inside of Microsoft Fabric working on top of OneLake.\\n1. Sign in to Microsoft Fabric .\\n2. Select Workspaces from the left-hand menu.\\n3. To open your workspace, enter its name in the search textbox located at the top\\nand select it from the search results.\\n4. In the upper left corner of the workspace home page, select New item and then\\nchoose Lakehouse from the Store data section.\\n5. Give your lakehouse a name and select Create.\\n6. A new lakehouse is created and, if this lakehouse is your first OneLake item,\\nOneLake is provisioned behind the scenes.\\nAt this point, you have a lakehouse running on top of OneLake. Next, add some data\\nand start organizing your lake.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='and start organizing your lake.\\n1. In the file browser on the left, select more options (...) next to Files and then select\\nNew subfolder. Name your subfolder and select Create.\\nCreate a lakehouse\\n\\uf80a\\nLoad data into a lakehouse'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='2. You can repeat this step to add more subfolders as needed.\\n3. Select more options (...) next to your folder, and then select Upload > Upload files\\nfrom the menu.\\n4. Choose the file you want from your local machine and then select Upload.\\n5. You now have data in OneLake. To add data in bulk or schedule data loads into\\nOneLake, use the Get data button to create pipelines. Find more details about\\noptions for getting data in Microsoft Fabric decision guide: copy activity, dataflow,\\nor Spark.\\n6. Select more options (...) for the file you uploaded and select Properties from the\\nmenu.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nThe Properties screen shows the various details for the file, including the URL and\\nAzure Blob File System (ABFS) path for use with Notebooks. You can copy the ABFS\\ninto a Fabric Notebook to query the data using Apache Spark. To learn more about\\nnotebooks in Fabric, see Explore the data in your lakehouse with a notebook.\\nNow you have your first lakehouse with data stored in OneLake.\\nLearn how to connect to existing data sources with OneLake shortcuts.\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Transform data with Apache Spark and\\nquery with SQL\\nArticle• 03/13/2025\\nIn this guide, you will:\\nUpload data to OneLake with the OneLake file explorer.\\nUse a Fabric notebook to read data on OneLake and write back as a Delta table.\\nAnalyze and transform data with Spark using a Fabric notebook.\\nQuery one copy of data on OneLake with SQL.\\nBefore you begin, you must:\\nDownload and install OneLake file explorer.\\nCreate a workspace with a Lakehouse item.\\nDownload the WideWorldImportersDW dataset. You can use Azure Storage\\nExplorer to connect to\\nhttps://fabrictutorialdata.blob.core.windows.net/sampledata/WideWorldImporters\\nDW/csv/full/dimension_city and download the set of csv files. Or you can use your\\nown csv data and update the details as required.\\nIn this section, you upload test data into your lakehouse using OneLake file explorer.\\n1. In OneLake file explorer, navigate to your lakehouse and create a subdirectory\\nnamed dimension_city under the /Files directory.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='named dimension_city under the /Files directory.\\n2. Copy your sample csv files to the OneLake directory /Files/dimension_city using\\nOneLake file explorer.\\nPrerequisites\\nUpload data'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"3. Navigate to your lakehouse in the Power BI or Fabric service and view your files.\\nIn this section, you convert the unmanaged CSV files into a managed table using Delta\\nformat.\\n1. In your lakehouse, select Open notebook, then New notebook to create a\\nnotebook.\\n\\uf80a\\nCreate a Delta table\\n７ Note\\nAlways create, load, or create a shortcut to Delta-Parquet data directly under the\\nTables section of the lakehouse. Don't nest your tables in subfolders under the\\nTables section. The lakehouse doesn't recognize subfolders as tables and labels\\nthem as Unidentified.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='2. Using the Fabric notebook, convert the CSV files to Delta format. The following\\ncode snippet reads data from user created directory /Files/dimension_city and\\nconverts it to a Delta table dim_city.\\nCopy the code snippet into the notebook cell editor. Replace the placeholders with\\nyour own workspace details, then select Run cell or Run all.\\nPython\\n3. To see your new table, refresh your view of the /Tables directory. Select more\\noptions (...) next to the Tables directory, then select Refresh.\\nimport os\\nfrom pyspark.sql.types import *\\nfor filename in os.listdir(\"/lakehouse/default/Files/dimension_city\"):\\n    \\ndf=spark.read.format(\\'csv\\').options(header=\"true\",inferSchema=\"true\").l\\noad(\"abfss://<YOUR_WORKSPACE_NAME>@onelake.dfs.fabric.microsoft.com/<YO\\nUR_LAKEHOUSE_NAME>.Lakehouse/Files/dimension_city/\"+filename,on_bad_lin\\nes=\"skip\")\\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/dim_city\")\\n\\uea80 Tip\\nYou can retrieve the full ABFS path to your directory by right-clicking on the'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='\\uea80 Tip\\nYou can retrieve the full ABFS path to your directory by right-clicking on the\\ndirectory name and selecting Copy ABFS path.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='In this section, you use a Fabric notebook to interact with the data in your table.\\n1. Query your table with SparkSQL in the same Fabric notebook.\\nPython\\n2. Modify the Delta table by adding a new column named newColumn with data type\\ninteger. Set the value of 9 for all the records for this newly added column.\\nPython\\nYou can also access any Delta table on OneLake via a SQL analytics endpoint. A SQL\\nanalytics endpoint references the same physical copy of Delta table on OneLake and\\noffers the T-SQL experience.\\n1. Navigate to your lakehouse, then select Lakehouse > SQL analytics endpoint from\\nthe drop-down menu.\\n2. Select New SQL Query to query the table using T-SQL.\\n3. Copy and paste the following code into the query editor, then select Run.\\nSQL\\nQuery and modify data\\n%%sql\\nSELECT * from <LAKEHOUSE_NAME>.dim_city LIMIT 10;\\n%%sql\\nALTER TABLE <LAKEHOUSE_NAME>.dim_city ADD COLUMN newColumn int;\\nUPDATE <LAKEHOUSE_NAME>.dim_city SET newColumn = 9;'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='UPDATE <LAKEHOUSE_NAME>.dim_city SET newColumn = 9;\\nSELECT City,newColumn FROM <LAKEHOUSE_NAME>.dim_city LIMIT 10;'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nConnect to ADLS using a OneLake shortcut\\nSELECT TOP (100) * FROM [<LAKEHOUSE_NAME>].[dbo].[dim_city];\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Connect to ADLS and transform the\\ndata with Azure Databricks\\nArticle• 11/29/2023\\nIn this guide, you will:\\nCreate a Delta table in your Azure Data Lake Storage (ADLS) Gen2 account using\\nAzure Databricks.\\nCreate a OneLake shortcut to a Delta table in ADLS.\\nUse Power BI to analyze data via the ADLS shortcut.\\nBefore you start, you must have:\\nA workspace with a Lakehouse item\\nAn Azure Databricks workspace\\nAn ADLS Gen2 account to store Delta tables\\n1. Using an Azure Databricks notebook, create a Delta table in your ADLS Gen2\\naccount.\\nPython\\nPrerequisites\\nCreate a Delta table, create a shortcut, and\\nanalyze the data\\n # Replace the path below to refer to your sample parquet data with \\nthis syntax \"abfss://<storage name>@<container \\nname>.dfs.core.windows.net/<filepath>\"\\n # Read Parquet files from an ADLS account\\n df = \\nspark.read.format(\\'Parquet\\').load(\"abfss://datasetsv1@olsdemo.dfs.core.\\nwindows.net/demo/full/dimension_city/\")\\n # Write Delta tables to ADLS account'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='2. In your lakehouse, select the ellipses (…) next to Tables and then select New\\nshortcut.\\n3. In the New shortcut screen, select the Azure Data Lake Storage Gen2 tile.\\n4. Specify the connection details for the shortcut and select Next.\\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://datasetsv1@ols\\ndemo.dfs.core.windows.net/demo/adb_dim_city_delta/\")\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='5. Specify the shortcut details. Provide a Shortcut Name and Sub path details and\\nthen select Create. The sub path should point to the directory where the Delta\\ntable resides.\\n6. The shortcut appears as a Delta table under Tables.\\n\\uf80a \\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='7. You can now query this data directly from a notebook.\\nPython\\n8. To access and analyze this Delta table via Power BI, select New Power BI semantic\\nmodel.\\n9. Select the shortcut and then select Confirm.\\ndf = spark.sql(\"SELECT * FROM \\nlakehouse1.adls_shortcut_adb_dim_city_delta LIMIT 1000\")\\ndisplay(df)'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='10. When the data is published, select Start from scratch.\\n11. In the report authoring experience, the shortcut data appears as a table along with\\nall its attributes.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\n12. To build a Power BI report, drag the attributes to the pane on the left-hand side.\\nIngest data into OneLake and analyze with Azure Databricks\\nRelated content\\n\\ue8e1 Yes \\ue8e0 No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Ingest data into OneLake and analyze\\nwith Azure Databricks\\nArticle• 02/25/2025\\nIn this guide, you will:\\nCreate a pipeline in a workspace and ingest data into your OneLake in Delta\\nformat.\\nRead and modify a Delta table in OneLake with Azure Databricks.\\nBefore you start, you must have:\\nA workspace with a Lakehouse item.\\nA premium Azure Databricks workspace. Only premium Azure Databricks\\nworkspaces support Microsoft Entra credential passthrough. When creating your\\ncluster, enable Azure Data Lake Storage credential passthrough in the Advanced\\nOptions.\\nA sample dataset.\\n1. Navigate to your lakehouse in the Power BI service and select Get data and then\\nselect New data pipeline.\\nPrerequisites\\nIngest data and modify the Delta table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='2. In the New Pipeline prompt, enter a name for the new pipeline and then select\\nCreate.\\n3. For this exercise, select the NYC Taxi - Green sample data as the data source.\\n4. On the preview screen, select Next.\\n5. For data destination, select the name of the lakehouse you want to use to store the\\nOneLake Delta table data. You can choose an existing lakehouse or create a new\\none.\\n6. Select where you want to store the output. Choose Tables as the Root folder. Enter\\n\"nycsample\" as the table name and select Next.\\n7. On the Review + Save screen, select Start data transfer immediately and then\\nselect Save + Run.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='8. When the job is complete, navigate to your lakehouse and view the delta table\\nlisted under /Tables folder.\\n9. Right-click on the created table name, select Properties, and copy the Azure Blob\\nFilesystem (ABFS) path.\\n10. Open your Azure Databricks notebook. Read the Delta table on OneLake.\\nPython\\n11. Update the Delta table data by changing a field value.\\nPython\\nTransform data with Apache Spark and query with SQL\\nolsPath = \"abfss://<replace with workspace \\nname>@onelake.dfs.fabric.microsoft.com/<replace with item \\nname>.Lakehouse/Tables/nycsample\" \\ndf=spark.read.format(\\'delta\\').option(\"inferSchema\",\"true\").load(olsPath\\n)\\ndf.show(5)\\n%sql\\nupdate delta.`abfss://<replace with workspace \\nname>@onelake.dfs.fabric.microsoft.com/<replace with item \\nname>.Lakehouse/Tables/nycsample` set vendorID = 99999 where vendorID = \\n1;\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Understand medallion lakehouse\\narchitecture for Microsoft Fabric with\\nOneLake\\nThe medallion lakehouse architecture, commonly known as medallion architecture, is a design\\npattern that's used to organize data in a lakehouse. It's the recommended design approach for\\nFabric. Since OneLake is the data lake for Fabric, medallion architecture is implemented by\\ncreating lakehouses in OneLake.\\nMedallion architecture comprises three distinct layers. The three medallion layers are: bronze\\n(raw data), silver (enriched data), and gold (curated data). Each layer indicates the quality of\\ndata stored in the lakehouse, with higher levels representing higher quality.\\nMedallion architecture helps your data stay accurate and reliable according to the principles of\\natomicity, consistency, isolation, and durability (ACID). Your data starts in its raw form, and the\\noriginal copies are preserved as a source of truth while your pipelines of validations and\\ntransformations prepares the data for analytics.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"transformations prepares the data for analytics.\\nFor more information, see What is the medallion lakehouse architecture?.\\nThis article introduces medallion lake architecture and describes how you can implement the\\ndesign pattern in Microsoft Fabric. It's targeted at multiple audiences:\\nData engineers: Technical staff who design, build, and maintain infrastructures and\\nsystems that enable their organization to collect, store, process, and analyze large\\nvolumes of data.\\nCenter of Excellence, IT, and BI teams: The teams that are responsible for overseeing\\nanalytics throughout the organization.\\nFabric administrators: The administrators who are responsible for overseeing Fabric in\\nthe organization.\\nThe goal of medallion architecture is to incrementally improve the structure and quality of\\ndata. Think of medallion architecture as a three-stage cleaning and organizing process for your\\ndata. Each layer makes your data more reliable and easier to use.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='data. Each layer makes your data more reliable and easier to use.\\n1. Bronze (Raw): Store everything exactly as it arrives. No changes are allowed.\\nAudience\\nWhat is medallion architecture?'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"2. Silver (Enriched): Fix errors, standardize formats, and remove duplicates.\\n3. Gold (Curated): Organize for reports and dashboards.\\nKeep each layer separated in its own lakehouse or data warehouse in OneLake, with data\\nmoving between the layers as it's transformed and refined.\\nIn a typical medallion architecture implementation in Fabric, the bronze layer stores the data in\\nthe same format as the data source. When the data source is a relational database, Delta tables\\nare a good choice. The silver and gold layers should contain Delta tables.\\nConsider the following example of an e-commerce company that applies medallion\\narchitecture to its data:\\nBronze Layer:\\nStore raw sales data from website (JSON)\\nStore raw inventory data from warehouse (CSV)\\nStore raw customer data from CRM (SQL export)\\nSilver Layer:\\nStandardize date formats across all sources\\nConvert all currency to USD\\n\\uea80 Tip\\nTo learn how to create a lakehouse, work through the Lakehouse end-to-end scenario\\ntutorial.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='\\uea80 Tip\\nTo learn how to create a lakehouse, work through the Lakehouse end-to-end scenario\\ntutorial.\\nReal-world example'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Remove test transactions\\nMatch customer records across systems\\nGold Layer:\\nCreate daily sales dashboard table\\nBuild customer lifetime value table\\nGenerate inventory forecasting table\\nThe basis of a modern data warehouse is a data lake. Microsoft OneLake is a single, unified,\\nlogical data lake for your entire organization. It comes automatically provisioned with every\\nFabric tenant, and it's the single location for all your analytics data.\\nTo store data in OneLake, you create a lakehouse in Fabric. A lakehouse is a data architecture\\nplatform for storing, managing, and analyzing structured and unstructured data in a single\\nlocation. It can scale to large data volumes of all file types and sizes, and because the data is\\nstored in a single location, it can be shared and reused across the organization.\\nFor more information, see What is a lakehouse in Microsoft Fabric?.\\nWhen you create a lakehouse in OneLake, two physical storage locations are provisioned\\nautomatically:\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='automatically:\\nTables stores tables of all formats in Apache Spark (CSV, Parquet, or Delta).\\nFiles stores data in any file format. If you want to create a table based on data in the files\\narea, you can create a shortcut that points to the folder that contains the table files.\\nIn the bronze layer, you store data in its original format, which might be either tables or files. If\\nthe source data is from OneLake, Azure Data Lake Store Gen2 (ADLS Gen2), Amazon S3, or\\nGoogle, create a shortcut in the bronze layer instead of copying the data across.\\nIn the silver and gold layers, you typically store data in Delta tables. However, you can also\\nstore data in Parquet or CSV files. If you do that, you must explicitly create a shortcut or an\\nexternal table with a location that points to the unmanaged folder that contains the Delta Lake\\nfiles in Apache Spark.\\nIn Microsoft Fabric, the Lakehouse explorer provides a unified graphical representation of the'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='In Microsoft Fabric, the Lakehouse explorer provides a unified graphical representation of the\\nwhole Lakehouse for users to navigate, access, and update their data.\\nMedallion architecture in OneLake\\nTables and files'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Delta Lake is an optimized storage layer that provides the foundation for storing data and\\ntables. It supports ACID transactions for big data workloads, and for this reason it's the default\\nstorage format in a Fabric lakehouse.\\nDelta Lake delivers reliability, security, and performance in the lakehouse for both streaming\\nand batch operations. Internally, it stores data in Parquet file format, however, it also maintains\\ntransaction logs and statistics that provide features and performance improvement over the\\nstandard Parquet format.\\nDelta Lake format delivers the following benefits compared to generic file formats:\\nSupport for ACID properties, especially durability to prevent data corruption.\\nFaster read queries.\\nIncreased data freshness.\\nSupport for both batch and streaming workloads.\\nSupport for data rollback by using Delta Lake time travel.\\nEnhanced regulatory compliance and audit by using Delta Lake table history.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Enhanced regulatory compliance and audit by using Delta Lake table history.\\nFabric standardizes storage file format with Delta Lake. By default, every workload engine in\\nFabric creates Delta tables when you write data to a new table. For more information, see\\nLakehouse and Delta Lake tables.\\nTo implement medallion architecture in Fabric, you can either use lakehouses (one for each\\nlayer), a data warehouse, or combination of both. Your decision should be based on your\\npreference and the expertise of your team. With Fabric, you can use different analytic engines\\nthat work on the one copy of your data in OneLake.\\nHere are two patterns to consider:\\nPattern 1: Create each layer as a lakehouse. In this case, business users access data by\\nusing the SQL analytics endpoint.\\nPattern 2: Create the bronze and silver layers as lakehouses, and the gold layer as a data\\nwarehouse. In this case, business users access data by using the data warehouse\\nendpoint.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='warehouse. In this case, business users access data by using the data warehouse\\nendpoint.\\nWhile you can create all lakehouses in a single Fabric workspace, we recommend that you\\ncreate each lakehouse in its own, separate workspace. This approach provides you with more\\ncontrol and better governance at the layer level.\\nDelta Lake storage\\nDeployment model'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='For the bronze layer, we recommend that you store the data in its original format, or use\\nParquet or Delta Lake. Whenever possible, keep the data in its original format. If the source\\ndata is from OneLake, Azure Data Lake Store Gen2 (ADLS Gen2), Amazon S3, or Google, create\\na shortcut in the bronze layer instead of copying the data across.\\nFor the silver and gold layers, we recommend that you use Delta tables because of the extra\\ncapabilities and performance enhancements they provide. Fabric standardizes on Delta Lake\\nformat, and by default every engine in Fabric writes data in this format. Further, these engines\\nuse V-Order write-time optimization to the Parquet file format. That optimization enables fast\\nreads by Fabric compute engines, such as Power BI, SQL, Apache Spark, and others. For more\\ninformation, see Delta Lake table optimization and V-Order.\\nLastly, today many organizations face massive growth in data volumes, together with an'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Lastly, today many organizations face massive growth in data volumes, together with an\\nincreasing need to organize and manage that data in a logical way while facilitating more\\ntargeted and efficient use and governance. That can lead you to establish and manage a\\ndecentralized or federated data organization with governance. To meet this objective, consider\\nimplementing a data mesh architecture. Data mesh is an architectural pattern that focuses on\\ncreating data domains that offer data as a product.\\nYou can create a data mesh architecture for your data estate in Fabric by creating data\\ndomains. You might create domains that map to your business domains, for example,\\nmarketing, sales, inventory, human resources, and others. You can then implement medallion\\narchitecture by setting up data layers within each of your domains. For more information about\\ndomains, see Domains.\\nMaterialized lake views in Microsoft Fabric help you to implement medallion architecture in'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Materialized lake views in Microsoft Fabric help you to implement medallion architecture in\\nyour lakehouse. Rather than building complex pipelines to transform data between bronze,\\nsilver, and gold layers, you can define materialized lake views that automatically manage the\\ntransformations.\\nKey benefits of using materialized lake views for medallion architecture include:\\nDeclarative pipelines: Define data transformations using SQL statements rather than\\nbuilding manual pipelines between layers.\\nAutomatic dependency management: Fabric automatically determines the correct\\nexecution order based on view dependencies.\\nData quality rules: Built-in support for defining and enforcing data quality constraints as\\ndata moves through layers.\\nOptimal refresh: The system automatically determines whether to perform incremental,\\nfull, or no refresh for each view.\\nUse materialized lake views for medallion architecture'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Visualization and monitoring: View lineage across all layers and track execution progress.\\nFor example, you can create a silver layer view that cleanses and joins data from bronze tables,\\nand then create gold layer views that aggregate the silver layer data for reporting. The system\\nhandles the refresh orchestration automatically.\\nFor more information, see Implement medallion architecture with materialized lake views.\\nThis section describes other guidance related to implementing a medallion lakehouse\\narchitecture in Fabric.\\nGenerally, a big data platform performs better when it has a few large files rather than many\\nsmall files. Performance degradation occurs when the compute engine has many metadata and\\nfile operations to manage. For better query performance, we recommend that you aim for data\\nfiles that are approximately 1 GB in size.\\nDelta Lake has a feature called predictive optimization. Predictive optimization automates'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Delta Lake has a feature called predictive optimization. Predictive optimization automates\\nmaintenance operations for Delta tables. When this feature is enabled, Delta Lake identifies\\ntables that would benefit from maintenance operations and then optimizes their storage. While\\nthis feature should form part of your operational excellence and your data preparation work,\\nFabric can optimize data files during data write, too. For more information, see Predictive\\noptimization for Delta Lake.\\nBy default, Delta Lake maintains a history of all changes made, so the size of historical\\nmetadata grows over time. Based on your business requirements, keep historical data only for\\na certain period of time to reduce your storage costs. Consider retaining historical data for only\\nthe last month, or other appropriate period of time.\\nYou can remove older historical data from a Delta table by using the VACUUM command.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You can remove older historical data from a Delta table by using the VACUUM command.\\nHowever, by default you can\\'t delete historical data within the last seven days. That restriction\\nmaintains the consistency in data. Configure the default number of days with the table\\nproperty delta.deletedFileRetentionDuration = \"interval <interval>\". That property\\ndetermines the period of time that a file must be deleted before it can be considered a\\ncandidate for a vacuum operation.\\nUnderstand Delta table data storage\\nFile size\\nHistorical retention'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"When you store data in each layer, we recommended that you use a partitioned folder\\nstructure wherever applicable. This technique improves data manageability and query\\nperformance. Generally, partitioned data in a folder structure results in faster search for specific\\ndata entries because of partition pruning/elimination.\\nTypically, you append data to your target table as new data arrives. However, in some cases\\nyou might merge data because you need to update existing data at the same time. In that case,\\nyou can perform an upsert operation by using the MERGE command. When your target table is\\npartitioned, be sure to use a partition filter to speed up the operation. That way, the engine can\\neliminate partitions that don't require updating.\\nYou should plan and control who needs access to specific data in the lakehouse. You should\\nalso understand the various transaction patterns they're going to use while accessing this data.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"also understand the various transaction patterns they're going to use while accessing this data.\\nYou can then define the right table partitioning scheme, and data collocation with Delta Lake\\nZ-order indexes.\\nFor more information about implementing medallion lakehouse architecture, see the following\\nresources.\\nTutorial: Lakehouse end-to-end scenario\\nTutorial: Implement medallion architecture with materialized lake views\\nLakehouse and Delta Lake tables\\nMicrosoft Fabric decision guide: choose a data store\\nThe need for optimize write on Apache Spark\\nQuestions? Try asking the Fabric community.\\nSuggestions? Contribute ideas to improve Fabric .\\n） Note: The author created this article with assistance from AI. Learn more\\nLast updated on 12/03/2025\\nTable partitions\\nData access\\nRelated content\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake shortcuts\\nArticle• 05/19/2025\\nShortcuts in Microsoft OneLake allow you to unify your data across domains, clouds, and\\naccounts by creating a single virtual data lake for your entire enterprise. All Fabric experiences\\nand analytical engines can directly connect to your existing data sources such as Azure,\\nAmazon Web Services (AWS), and OneLake through a unified namespace. OneLake manages all\\npermissions and credentials, so you don't need to separately configure each Fabric workload to\\nconnect to each data source. Additionally, you can use shortcuts to eliminate edge copies of\\ndata and reduce process latency associated with data copies and staging.\\nShortcuts are objects in OneLake that point to other storage locations. The location can be\\ninternal or external to OneLake. The location that a shortcut points to is known as the target\\npath of the shortcut. The location where the shortcut appears is known as the shortcut path.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"path of the shortcut. The location where the shortcut appears is known as the shortcut path.\\nShortcuts appear as folders in OneLake and any workload or service that has access to OneLake\\ncan use them. Shortcuts behave like symbolic links. They're an independent object from the\\ntarget. If you delete a shortcut, the target remains unaffected. If you move, rename, or delete a\\ntarget path, the shortcut can break.\\nYou can create shortcuts in lakehouses and Kusto Query Language (KQL) databases.\\nFurthermore, the shortcuts you create within these items can point to other OneLake locations,\\nAzure Data Lake Storage (ADLS) Gen2, Amazon S3 storage accounts, or Dataverse. You can\\neven create shortcuts to on-premises or network-restricted locations with the use of the Fabric\\non-premises data gateway (OPDG).\\nWhat are shortcuts?\\n\\uf80a\\nWhere can I create shortcuts?\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"You can use the Fabric UI to create shortcuts interactively, and you can use the REST API to\\ncreate shortcuts programmatically.\\nWhen creating shortcuts in a lakehouse, you must understand the folder structure of the item.\\nLakehouses are composed of two top-level folders: the Tables folder and the Files folder. The\\nTables folder represents the managed portion of the lakehouse for structured datasets. While\\nthe Files folder is the unmanaged portion of the lakehouse for unstructured or semi-structured\\ndata.\\nIn the Tables folder, you can only create shortcuts at the top level. Shortcuts aren't supported\\nin subdirectories of the Tables folder. Shortcuts in the tables section typically point to internal\\nsources within OneLake or link to other data assets that conform to the Delta table format. If\\nthe target of the shortcut contains data in the Delta\\\\Parquet format, the lakehouse\\nautomatically synchronizes the metadata and recognizes the folder as a table. Shortcuts in the\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"automatically synchronizes the metadata and recognizes the folder as a table. Shortcuts in the\\ntables section can link to either a single table or a schema, which is a parent folder for multiple\\ntables.\\nIn the Files folder, there are no restrictions on where you can create shortcuts. You can create\\nthem at any level of the folder hierarchy. Table discovery doesn't happen in the Files folder.\\nShortcuts here can point to both internal (OneLake) and external storage systems with data in\\nany format.\\nLakehouse\\n７ Note\\nThe Delta format doesn't support tables with space characters in the name. Any shortcut\\ncontaining a space in the name won't be discovered as a Delta table in the lakehouse.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='When you create a shortcut in a KQL database, it appears in the Shortcuts folder of the\\ndatabase. The KQL database treats shortcuts like external tables. To query the shortcut, use the\\nexternal_table function of the Kusto Query Language.\\nKQL database'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Any Fabric or non-Fabric service that can access data in OneLake can use shortcuts. Shortcuts\\nare transparent to any service accessing data through the OneLake API. Shortcuts just appear\\nas another folder in the lake. Apache Spark, SQL, Real-Time Intelligence, and Analysis Services\\ncan all use shortcuts when querying data.\\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake.\\nRelative file paths can be used to directly read data from shortcuts. Additionally, if you create a\\nshortcut in the Tables section of the lakehouse and it is in the Delta format, you can read it as a\\nmanaged table using Apache Spark SQL syntax.\\nPython\\nWhere can I access shortcuts?\\nApache Spark\\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")\\ndisplay(df)'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Python\\nYou can read shortcuts in the Tables section of a lakehouse through the SQL analytics endpoint\\nfor the lakehouse. You can access the SQL analytics endpoint through the mode selector of the\\nlakehouse or through SQL Server Management Studio (SSMS).\\nSQL\\nShortcuts in KQL databases are recognized as external tables. To query the shortcut, use the\\nexternal_table function of the Kusto Query Language.\\nKusto\\nYou can create semantic models for lakehouses containing shortcuts in the Tables section of\\nthe lakehouse. When the semantic model runs in Direct Lake mode, Analysis Services can read\\ndata directly from the shortcut.\\nApplications and services outside of Fabric can also access shortcuts through the OneLake API.\\nOneLake supports a subset of the ADLS Gen2 and Blob storage APIs. To learn more about the\\nOneLake API, see OneLake access with APIs.\\nHTTP\\ndf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000\")\\ndisplay(df)\\nSQL\\nSELECT TOP (100) *\\nFROM [MyLakehouse].[dbo].[MyShortcut]'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"display(df)\\nSQL\\nSELECT TOP (100) *\\nFROM [MyLakehouse].[dbo].[MyShortcut]\\nReal-Time Intelligence\\nexternal_table('MyShortcut')\\n| take 100\\nAnalysis Services\\nNon-Fabric\\nhttps://onelake.dfs.fabric.microsoft.com/MyWorkspace/MyLakhouse/Tables/MyShortcut/\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake shortcuts support multiple filesystem data sources. These include internal OneLake\\nlocations, Azure Data Lake Storage (ADLS) Gen2, Amazon S3, S3 Compatible, Google Cloud\\nStorage(GCS) and Dataverse.\\nInternal OneLake shortcuts allow you to reference data within existing Fabric items, including:\\nKQL databases\\nLakehouses\\nMirrored Azure Databricks Catalogs\\nMirrored Databases\\nSemantic models\\nSQL databases\\nWarehouses\\nWarehouse snapshots\\nThe shortcut can point to a folder location within the same item, across items within the same\\nworkspace, or even across items in different workspaces. When you create a shortcut across\\nitems, the item types don't need to match. For instance, you can create a shortcut in a\\nlakehouse that points to data in a data warehouse.\\nWhen a user accesses data through a shortcut to another OneLake location, OneLake uses the\\nidentity of the calling user to authorize access to the data in the target path of the shortcut.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='identity of the calling user to authorize access to the data in the target path of the shortcut.\\nThis user must have permissions in the target location to read the data.\\nWhen you create shortcuts to Azure Data Lake Storage (ADLS) Gen2 storage accounts, the\\ntarget path can point to any folder within the hierarchical namespace. At a minimum, the target\\nMyFile.csv\\nTypes of shortcuts\\nInternal OneLake shortcuts\\n） Important\\nWhen users access shortcuts through Power BI semantic models or T-SQL, the calling\\nuser’s identity is not passed through to the shortcut target. The calling item owner’s\\nidentity is passed instead, delegating access to the calling user.\\nAzure Data Lake Storage shortcuts'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='path must include a container name.\\nADLS shortcuts must point to the DFS endpoint for the storage account.\\nExample: https://accountname.dfs.core.windows.net/\\nIf your storage account is protected by a storage firewall, you can configure trusted service\\naccess. For more information, see Trusted workspace access\\nADLS shortcuts use a delegated authorization model. In this model, the shortcut creator\\nspecifies a credential for the ADLS shortcut and all access to that shortcut is authorized using\\nthat credential. ADLS shortcuts support the following delegated authorization types:\\nOrganizational account - must have Storage Blob Data Reader, Storage Blob Data\\nContributor, or Storage Blob Data Owner role on the storage account; or Delegator role\\non the storage account plus file or directory access granted within the storage account.\\nService principal - must have Storage Blob Data Reader, Storage Blob Data Contributor,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Service principal - must have Storage Blob Data Reader, Storage Blob Data Contributor,\\nor Storage Blob Data Owner role on the storage account; or Delegator role on the storage\\naccount plus file or directory access granted within the storage account.\\nWorkspace identity - must have Storage Blob Data Reader, Storage Blob Data\\nContributor, or Storage Blob Data Owner role on the storage account; or Delegator role\\non the storage account plus file or directory access granted within the storage account.\\nShared Access Signature (SAS) - must include at least the following permissions: Read,\\nList, and Execute.\\nMicrosoft Entra ID delegated authorization types (organizational account, service principal, or\\nworkspace identity) require the Generate a user delegation key action at the storage account\\nlevel. This action is included as part of the Storage Blob Data Reader, Storage Blob Data\\nContributor, Storage Blob Data Owner, and Delegator roles. If you don't want to give a user\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Contributor, Storage Blob Data Owner, and Delegator roles. If you don't want to give a user\\nreader, contributor, or owner permissions for the whole storage account, assign them the\\nDelegator role instead. Then, define detailed data access rights using Access control lists (ACLs)\\nin Azure Data Lake Storage.\\n７ Note\\nYou must have hierarchical namespaces enabled on your ADLS Gen 2 storage account.\\nAccess\\nAuthorization\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Azure Blob Storage shortcut can point to the account name or URL for the Storage account.\\nExample: accountname or https://accountname.blob.core.windows.net/\\nBlob storage shortcuts use a delegated authorization model. In this model, the shortcut creator\\nspecifies a credential for the shortcut and all access to that shortcut is authorized using that\\ncredential. Blob shortcuts support the following delegated authorization types:\\nOrganizational account - must have Storage Blob Data Reader, Storage Blob Data\\nContributor, or Storage Blob Data Owner role on the storage account; or Delegator role\\non the storage account plus file or directory access granted within the storage account.\\nService principal - must have Storage Blob Data Reader, Storage Blob Data Contributor,\\nor Storage Blob Data Owner role on the storage account; or Delegator role on the storage\\naccount plus file or directory access granted within the storage account.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"account plus file or directory access granted within the storage account.\\nWorkspace identity - must have Storage Blob Data Reader, Storage Blob Data\\nContributor, or Storage Blob Data Owner role on the storage account; or Delegator role\\non the storage account plus file or directory access granted within the storage account.\\nShared Access Signature (SAS) - must include at least the following permissions: Read,\\nList, and Execute.\\nWhen you create shortcuts to Amazon S3 accounts, the target path must contain a bucket\\nname at a minimum. S3 doesn't natively support hierarchical namespaces but you can use\\nprefixes to mimic a directory structure. You can include prefixes in the shortcut path to further\\n） Important\\nCurrently, when workspace identity is used as the delegated authorization type for an\\nADLS shortcut, users can authenticate directly to the storage account without needing to\\ncreate a delegation key. However, this behavior will be restricted in the future. We\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"create a delegation key. However, this behavior will be restricted in the future. We\\nrecommend making sure that all users have the Generate a user delegation key action to\\nensure that your users' access isn't affected when this behavior changes.\\nAzure Blob Storage shortcuts\\nAccess\\nAuthorization\\nS3 shortcuts\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='narrow the scope of data accessible through the shortcut. When you access data through an S3\\nshortcut, prefixes are represented as folders.\\nS3 shortcuts must point to the https endpoint for the S3 bucket.\\nExample: https://bucketname.s3.region.amazonaws.com/\\nS3 shortcuts use a delegated authorization model. In this model, the shortcut creator specifies\\na credential for the S3 shortcut and all access to that shortcut is authorized using that\\ncredential. The supported delegated credential is a key and secret for an IAM user.\\nThe IAM user must have the following permissions on the bucket that the shortcut is pointing\\nto:\\nS3:GetObject\\nS3:GetBucketLocation\\nS3:ListBucket\\nS3 shortcuts support S3 buckets that use S3 Bucket Keys for SSE-KMS encryption. To access\\ndata encrypted with SSE-KMS encryption, the user must have encrypt/decrypt permissions for\\nthe bucket key, otherwise they receive a \"Forbidden\" error (403). For more information, see'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='the bucket key, otherwise they receive a \"Forbidden\" error (403). For more information, see\\nConfiguring your bucket to use an S3 Bucket Key with SSE-KMS for new objects.\\n７ Note\\nS3 shortcuts are read-only. They don\\'t support write operations regardless of the user\\'s\\npermissions.\\nAccess\\n７ Note\\nYou don\\'t need to disable the S3 Block Public Access setting for your S3 account for the S3\\nshortcut to function.\\nAccess to the S3 endpoint must not be blocked by a storage firewall or Virtual Private\\nCloud.\\nAuthorization'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Shortcuts can be created to Google Cloud Storage(GCS) using the XML API for GCS. When you\\ncreate shortcuts to Google Cloud Storage, the target path must contain a bucket name at a\\nminimum. You can also restrict the scope of the shortcut by further specifying the prefix/folder\\nyou want to point to within the storage hierarchy.\\nWhen configuring the connection for a GCS shortcut, you can either specify the global\\nendpoint for the storage service or use a bucket-specific endpoint.\\nGlobal endpoint example: https://storage.googleapis.com\\nBucket-specific endpoint example: https://<BucketName>.storage.googleapis.com\\nGCS shortcuts use a delegated authorization model. In this model, the shortcut creator\\nspecifies a credential for the GCS shortcut and all access to that shortcut is authorized using\\nthat credential. The supported delegated credential is an HMAC key and secret for a Service\\naccount or User account.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"account or User account.\\nThe account must have permission to access the data within the GCS bucket. If the bucket-\\nspecific endpoint was used in the connection for the shortcut, the account must have the\\nfollowing permissions:\\nstorage.objects.get\\nstoage.objects.list\\nIf the global endpoint was used in the connection for the shortcut, the account must also have\\nthe following permission:\\nstorage.buckets.list\\nGoogle Cloud Storage shortcuts\\n７ Note\\nGCS shortcuts are read-only. They don't support write operations regardless of the user's\\npermissions.\\nAccess\\nAuthorization\\nDataverse shortcuts\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Dataverse direct integration with Microsoft Fabric enables organizations to extend their\\nDynamics 365 enterprise applications and business processes into Fabric. This integration is\\naccomplished through shortcuts, which can be created in two ways: through the PowerApps\\nmaker portal, or through Fabric directly.\\nAuthorized PowerApps users can access the PowerApps maker portal and use the Link to\\nMicrosoft Fabric feature. From this single action, a lakehouse is created in Fabric and shortcuts\\nare automatically generated for each table in the Dataverse environment.\\nFor more information, see Dataverse direct integration with Microsoft Fabric .\\nFabric users can also create shortcuts to Dataverse. When users create shortcuts, they can\\nselect Dataverse, supply their environment URL, and browse the available tables. This\\nexperience allows users to choose which tables to bring into Fabric rather than bringing in all\\ntables.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"tables.\\nDataverse shortcuts use a delegated authorization model. In this model, the shortcut creator\\nspecifies a credential for the Dataverse shortcut, and all access to that shortcut is authorized\\nusing that credential. The supported delegated credential type is organizational account\\n(OAuth2). The organizational account must have the system administrator permission to access\\ndata in Dataverse Managed Lake.\\n７ Note\\nDataverse shortcuts are read-only. They don't support write operations regardless of the\\nuser's permissions.\\nCreating shortcuts through PowerApps maker portal\\nCreating shortcuts through Fabric\\n７ Note\\nDataverse tables must first be available in the Dataverse Managed Lake before they're\\nvisible in the Fabric create shortcuts UX. If your tables aren't visible from Fabric, use the\\nLink to Microsoft Fabric feature from the PowerApps maker portal.\\nAuthorization\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Shortcut caching can reduce egress costs associated with cross-cloud data access. As files are\\nread through an external shortcut, the files are stored in a cache for the Fabric workspace.\\nSubsequent read requests are served from cache rather than the remote storage provider. The\\nretention period for cached files can be set from 1-28 days. Each time the file is accessed, the\\nretention period is reset. If the file in remote storage provider is more recent than the file in the\\ncache, the request is served from remote storage provider and the updated file will then be\\nstored in cache. If a file hasn’t been accessed for more than the selected retention period, it's\\npurged from the cache. Individual files greater than 1 GB in size aren't cached.\\nTo enable caching for shortcuts, open the Workspace settings panel. Choose the OneLake tab.\\nToggle the cache setting to On and select the Retention Period.\\nThe cache can also be cleared at any time. From the same settings page, select the Reset cache\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The cache can also be cleared at any time. From the same settings page, select the Reset cache\\nbutton. This action removes all files from the shortcut cache in this workspace.\\n７ Note\\nService principals added to the fabric workspace must have the admin role to authorize\\nthe Dataverse shortcut.\\nCaching\\n７ Note\\nShortcut caching is currently supported for GCS, S3, S3 compatible, and on-premises data\\ngateway shortcuts.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"ADLS and S3 shortcut authorization is delegated by using cloud connections. When you create\\na new ADLS or S3 shortcut, you either create a new connection or select an existing connection\\nfor the data source. Setting a connection for a shortcut is a bind operation. Only users with\\npermission on the connection can perform the bind operation. If you don't have permissions\\non the connection, you can't create new shortcuts using that connection.\\nShortcuts require certain permissions to manage and use. OneLake shortcut security looks at\\nthe permissions required to create shortcuts and access data using them.\\n\\uf80a\\nHow shortcuts utilize cloud connections\\nShortcut security\\nHow do shortcuts handle deletions?\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Shortcuts don't perform cascading deletes. When you delete a shortcut, you only delete the\\nshortcut object. The data in the shortcut target remains unchanged. However, if you delete a\\nfile or folder within a shortcut, and you have permissions in the shortcut target to perform the\\ndelete operation, the files or folders are deleted in the target.\\nFor example, consider a lakehouse with the following path in it:\\nMyLakehouse\\\\Files\\\\MyShortcut\\\\Foo\\\\Bar. MyShortcut is a shortcut that points to an ADLS Gen2\\naccount that contains the Foo\\\\Bar directories.\\nYou can perform a delete operation on the following path: MyLakehouse\\\\Files\\\\MyShortcut. In\\nthis case, the MyShortcut shortcut is deleted from the lakehouse but the files and directories in\\nthe ADLS Gen2 account Foo\\\\Bar remain unaffected.\\nYou can also perform a delete operation on the following path:\\nMyLakehouse\\\\Files\\\\MyShortcut\\\\Foo\\\\Bar. In this case, if you write permissions in the ADLS Gen2\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"MyLakehouse\\\\Files\\\\MyShortcut\\\\Foo\\\\Bar. In this case, if you write permissions in the ADLS Gen2\\naccount, the Bar directory is deleted from the ADLS Gen2 account.\\nWhen creating shortcuts between multiple Fabric items within a workspace, you can visualize\\nthe shortcut relationships through the workspace lineage view. Select the Lineage view button\\n(\\n  ) in the upper right corner of the Workspace explorer.\\nThe maximum number of shortcuts per Fabric item is 100,000. In this context, the term\\nitem refers to: apps, lakehouses, warehouses, reports, and more.\\nWorkspace lineage view\\n\\uf80a\\n７ Note\\nThe lineage view is scoped to a single workspace. Shortcuts to locations outside the\\nselected workspace don't appear.\\nLimitations and considerations\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The maximum number of shortcuts in a single OneLake path is 10.\\nThe maximum number of direct shortcuts to shortcut links is 5.\\nADLS and S3 shortcut target paths can\\'t contain any reserved characters from RFC 3986\\nsection 2.2. For allowed characters, see RFC 3968 section 2.3.\\nOneLake shortcut names, parent paths, and target paths can\\'t contain \"%\" or \"+\"\\ncharacters.\\nShortcuts don\\'t support non-Latin characters.\\nCopy Blob API isn\\'t supported for ADLS or S3 shortcuts.\\nCopy function doesn\\'t work on shortcuts that directly point to ADLS containers. It\\'s\\nrecommended to create ADLS shortcuts to a directory that is at least one level below a\\ncontainer.\\nMore shortcuts can\\'t be created inside ADLS or S3 shortcuts.\\nLineage for shortcuts to Data Warehouses and Semantic Models isn\\'t currently available.\\nA Fabric shortcut syncs with the source almost instantly, but propagation time might vary\\ndue to data source performance, cached views, or network connectivity issues.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"due to data source performance, cached views, or network connectivity issues.\\nIt might take up to a minute for the Table API to recognize new shortcuts.\\nOneLake shortcuts don't support connections to ADLS Gen2 storage accounts that use\\nmanaged private endpoints. For more information, see managed private endpoints for\\nFabric.\\nCreate a OneLake shortcut\\nUse OneLake shortcuts REST APIs\\nRelated content\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Shortcuts file transformations\\nShortcut transformations convert raw files (CSV, Parquet, and JSON) into Delta tables that stay always in sync with the source\\ndata. The transformation is executed by Fabric Spark compute, which copies the data referenced by a OneLake shortcut into a\\nmanaged Delta table so you don't have to build and orchestrate traditional extract, transform, load (ETL) pipelines yourself.\\nWith automatic schema handling, deep flattening capabilities, and support for multiple compression formats, shortcut\\ntransformations eliminate the complexity of building and maintaining ETL pipelines.\\nNo manual pipelines – Fabric automatically copies and converts the source files to Delta format; you don’t have to\\norchestrate incremental loads.\\nFrequent refresh – Fabric checks the shortcut every 2 minutes and synchronizes any changes almost immediately.\\nOpen & analytics-ready – Output is a Delta Lake table that any Apache Spark–compatible engine can query.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Unified governance – The shortcut inherits OneLake lineage, permissions, and Microsoft Purview policies.\\nSpark based – Transforms build for scale.\\nRequirement Details\\nMicrosoft Fabric SKU Capacity or Trial that supports Lakehouse workloads.\\nSource data A folder that contains homogeneous CSV, Parquet, or JSON files.\\nWorkspace role Contributor or higher.\\nAll data sources supported in OneLake are supported.\\nSource\\nfile\\nformat\\nDestinationSupported ExtensionsSupported Compression types Notes\\nCSV\\n(UTF-8,\\nUTF-\\n16)\\nDelta Lake\\ntable in the\\nLakehouse\\n/ Tables\\nfolder\\n.csv,.txt(delimiter),.tsv(tab-\\nseparated),.psv(pipe-\\nseparated),\\n.csv.gz,.csv.bz2 .csv.zip,.csv.snappy\\naren't supported\\nas of date\\nParquetDelta Lake\\ntable in the\\nLakehouse\\n/ Tables\\nfolder\\n.parquet .parquet.snappy,.parquet.gzip,.parquet.lz4,.parquet.brotli,.parquet.zstd\\n７ Note\\nShortcut transformations are currently in public preview and are subject to change.\\nWhy use shortcut transformations?\\nPrerequisites\\nﾉExpand table\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Why use shortcut transformations?\\nPrerequisites\\nﾉExpand table\\nSupported sources, formats and destinations\\nﾉExpand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Source\\nfile\\nformat\\nDestinationSupported ExtensionsSupported Compression types Notes\\nJSON Delta Lake\\ntable in the\\nLakehouse\\n/ Tables\\nfolder\\n.json,.jsonl,.ndjson.json.gz,.json.bz2,.jsonl.gz,.ndjson.gz,.jsonl.bz2,.ndjson.bz2.json.zip,\\n.json.snappy aren't\\nsupported as of\\ndate\\nExcel file support is part of roadmap\\nAI Transformations available to support unstructured file formats (.txt, .doc, .docx) with Text Analytics use case live with\\nmore enhancements upcoming\\n1. In your lakehouse, select New Table Shortcut in Tables section which is Shortcut transformation (preview) and choose\\nyour source (for example, Azure Data Lake, Azure Blob Storage, Dataverse, Amazon S3, GCP, SharePoint, OneDrive etc.).\\n2. Choose file, Configure transformation & create shortcut – Browse to an existing OneLake shortcut that points to the\\nfolder with your CSV files, configure parameters, and initiate creation.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='folder with your CSV files, configure parameters, and initiate creation.\\nDelimiter in CSV files – Select the character used to separate columns (comma, semicolon, pipe, tab, ampersand,\\nspace).\\nFirst row as headers – Indicate whether the first row contains column names.\\nTable Shortcut name – Provide a friendly name; Fabric creates it under /Tables.\\n3. Track refreshes and view logs for transparency in Manage Shortcut monitoring hub.\\nFabric Spark compute copies the data into a Delta table and shows progress in the Manage shortcut pane. Shortcut\\ntransformations are available in Lakehouse items. They create Delta Lake tables in the Lakehouse / Tables folder.\\nAfter the initial load, Fabric Spark compute:\\nPolls the shortcut target every 2 minutes.\\nDetects new or modified files and appends or overwrites rows accordingly.\\nDetects deleted files and removes corresponding rows.\\nShortcut transformations include monitoring and error handling to help you track ingestion status and diagnose issues.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Open the lakehouse and right-click the shortcut that feeds your transformation.\\nSet up a shortcut transformation\\nHow synchronization works\\nMonitor and troubleshoot'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='2. Select Manage shortcut.\\n3. In the details pane, you can view:\\nStatus – Last scan result and current sync state.\\nRefresh history – Chronological list of sync operations with row counts and any error details.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='4. View more details in logs to troubleshoot\\nCurrent limitations of shortcut transformations:\\nOnly CSV, Parquet, JSON file formats are supported.\\nFiles must share an identical schema; schema drift isn’t yet supported.\\nTransformations are read-optimized; MERGE INTO or DELETE statements directly on the table are blocked.\\nAvailable only in Lakehouse items (not Warehouses or KQL databases).\\nUnsupported datatypes for CSV: Mixed data type columns, Timestamp_Nanos, Complex logical types -\\nMAP/LIST/STRUCT, Raw binary\\nUnsupported datatype for Parquet: Timestamp_nanos, Decimal with INT32/INT64, INT96, Unassigned integer types -\\nUINT_8/UINT_16/UINT_64, Complex logical types - MAP/LIST/STRUCT)\\nUnsupported datatypes for JSON: Mixed data types in an array, Raw binary blobs inside JSON, Timestamp_Nanos\\nFlattening of Array data type in JSON: Array data type shall be retained in delta table and data accessible with Spark'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"SQL & Pyspark where for further transformations Fabric Materialized Lake Views could be used for silver layer\\nSource format: Only CSV, JSON, and Parquet files are supported as of date.\\nFlattening depth in JSON: Nested structures are flattened up to five levels deep. Deeper nesting requires preprocessing.\\nWrite operations: Transformations are read-optimized; direct MERGE INTO or DELETE statements on the transformation\\ntarget table aren't supported.\\nWorkspace availability: Available only in Lakehouse items (not Data Warehouses or KQL databases).\\nFile schema consistency: Files must share an identical schema.\\n７ Note\\nPause or Delete the transformation from this tab is an upcoming feature part of roadmap\\nLimitations\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='To stop synchronization, delete the shortcut transformation from the lakehouse UI.\\nDeleting the transformation doesn’t remove the underlying files.\\n） Note: The author created this article with assistance from AI. Learn more\\nLast updated on 12/01/2025\\n７ Note\\nAdding support for some of the above and reducing limitations is part of our roadmap. Track our release\\ncommunications for further updates.\\nClean up'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='AI-powered transforms in OneLake\\nshortcut transformations\\n07/17/2025\\nModern data lakes are brimming with raw, unstructured text, product reviews, support emails,\\nIoT device logs, and more. Turning that text into actionable insights typically requires custom\\ncode, orchestration pipelines, and constant maintenance. OneLake Shortcut Transformations\\nremove that overhead: you point to your files once, choose an AI transform, and Fabric does\\nthe rest.\\nBenefit What it means for you\\nAccelerate time-to-\\ninsight\\nGo from raw text to a queryable Delta table in minutes, no ETL required.\\nLower maintenance The transformation engine watches the source folder on a 2-minute schedule,\\nso outputs stay up to date automatically.\\nEnterprise-grade\\nsecurity\\nPII detection helps you comply with GDPR, HIPAA, and other regulations by\\nredacting sensitive data before it lands in analytics.\\nConsistent, repeatable\\nresults\\nBuilt-in AI models provide standardized sentiment scores, entity tags, and'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='results\\nBuilt-in AI models provide standardized sentiment scores, entity tags, and\\ntranslations, eliminating manual data-prep drift.\\nOneLake Shortcut Transformations in Microsoft Fabric include a set of built-in, AI-powered\\ntransforms that you can apply directly to .txt files referenced through shortcuts, without\\nwriting code or building pipelines. The engine automatically keeps the output Delta table in\\nsync, so your data is query-ready for Power BI, notebooks, pipelines, and other Fabric\\nexperiences.\\n） Important\\nAI transforms for OneLake shortcuts are currently Public Preview. Features and behavior\\nmay change before general availability.\\nWhy use AI-powered transforms?\\nﾉ Expand table\\nSupported AI transforms'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Transform Purpose\\nSummarization Generates concise summaries from long-form text.\\nTranslation Translates text between supported languages.\\nSentiment\\nanalysis\\nLabels text sentiment as positive, negative, or neutral.\\nPII detection Finds and redacts personally identifiable information (names, phone numbers,\\nemails).\\nName recognitionExtracts named entities such as people, organizations, or locations.\\nCustomer feedback stored in a data lake may contain sensitive details (names, emails, phone\\nnumbers). Apply the PII detection transform to automatically scan and redact this content and\\nproduce a privacy-compliant Delta table for analysis.\\n1. Create a shortcut\\nReference a folder of .txt files in Azure Data Lake, Amazon S3, or another OneLake\\nshortcuts source.\\n2. Select an AI transform\\nPick one of the supported transforms during shortcut creation.\\n3. Automatic sync\\nThe engine checks the source folder every 2 minutes. New, modified, or deleted files are\\nreflected in the Delta table.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='reflected in the Delta table.\\n4. Query-ready output\\nUse the resulting table immediately in reports, notebooks, or downstream pipelines.\\nﾉ Expand table\\n７ Note\\nAI transforms currently support .txt files only as input.\\nExample — PII detection in customer feedback\\nHow it works\\nRegional availability'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='AI-powered transforms are currently available in these regions: Azure AI Language regional\\nsupport'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Create a OneLake shortcut\\nArticle• 03/31/2025\\nIn this article, you learn how to create a OneLake shortcut inside a Fabric lakehouse. You\\ncan use a lakehouse, a data warehouse, or a Kusto Query Language (KQL) database as\\nthe source for your shortcut.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts\\nprogrammatically, see OneLake shortcuts REST APIs.\\nA lakehouse in OneLake. If you don't have a lakehouse, create one by following these\\nsteps: Create a lakehouse with OneLake.\\n1. Open a lakehouse.\\n2. Right-click on a directory within the Explorer pane of the lakehouse.\\n3. Select New shortcut.\\nPrerequisite\\nCreate a shortcut\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Under Internal sources, select Microsoft OneLake.\\n2. Select the data source that you want to connect to, and then select Next.\\n3. Expand Files or Tables to view the available subfolders. Subfolders in the tables\\ndirectory that contain valid Delta or Iceberg tables are indicated with a table icon.\\nFiles or unidentified folders in the tables section are indicated with a folder icon.\\nSelect a source\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='4. Select one or more subfolders to connect to, then select Next.\\nYou can select up to 50 subfolders when creating OneLake shortcuts.\\n5. Review your selected shortcut locations. Use the edit action to change the default\\nshortcut name. Use the delete action to remove any undesired selections. Select\\nCreate to generate shortcuts.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The lakehouse automatically refreshes. The shortcut appears under the selected\\ndirectory in the Explorer pane. You can differentiate a regular file or table from the\\nshortcut from its properties. The properties have a Shortcut Type parameter that\\nindicates the item is a shortcut.\\nEditing shortcuts requires write permission on the item being edited. The admin,\\nmember, and contributor roles grant write permissions.\\n1. To edit a shortcut, right-click on the shortcut and select Manage shortcut.\\n\\uf80a\\nEdit a shortcut'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='2. In the Manage shortcut view, you can edit the following fields:\\nName\\nTarget connection\\nNot all shortcut types use the target connection feature.\\nTarget location and Target subpath\\nBoth of these fields are editable by selecting the Target location.\\nShortcut location\\nYou can also edit shortcuts by using the OneLake shortcuts REST APIs.\\nTo delete a shortcut, select the ... icon next to the shortcut file or table and select Delete.\\nTo delete shortcuts programmatically, see OneLake shortcuts REST APIs.\\nRemove a shortcut'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nCreate an Azure Data Lake Storage Gen2 shortcut\\nCreate an Amazon S3 shortcut\\nUse OneLake shortcuts REST APIs\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Create an Azure Data Lake Storage Gen2\\nshortcut\\nArticle• 07/25/2024\\nIn this article, you learn how to create an Azure Data Lake Storage (ADLS) Gen2 shortcut\\ninside a Microsoft Fabric lakehouse.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts\\nprogrammatically, see OneLake shortcuts REST APIs.\\nIf you don't have a lakehouse, create one by following these steps: Create a\\nlakehouse with OneLake.\\nYou must have Hierarchical Namespaces enabled on your ADLS Gen 2 storage\\naccount.\\n1. Open a lakehouse.\\n2. Right-click on a directory within the Lake view of the lakehouse.\\n3. Select New shortcut.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Under External sources, select Azure Data Lake Storage Gen2.\\n2. Enter the Connection settings according to the following table:\\nSelect a source\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Field Description Value\\nURL The connection\\nstring for your\\ndelta container.\\nhttps://StorageAccountName.dfs.core.windows.net\\nConnection Previously\\ndefined\\nconnections for\\nthe specified\\nstorage location\\nappear in the\\ndrop-down. If no\\nconnections\\nexist, create a\\nnew connection.\\nCreate new connection.\\nConnection\\nname\\nThe Azure Data\\nLake Storage\\nGen2 connection\\nname.\\nA name for your connection.\\nAuthentication\\nkind\\nThe\\nauthorization\\nmodel. The\\nsupported\\nmodels are:\\nOrganizational\\naccount,\\nAccount key,\\nShared Access\\nSignature (SAS),\\nand Service\\nprincipal. For\\nDependent on the authorization model. Once you\\nselect an authentication kind, fill in the required\\ncredentials.\\n\\uf80a\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Field Description Value\\nmore\\ninformation, see\\nADLS shortcuts.\\n3. Select Next.\\n4. Browse to the target location for the shortcut.\\nIf you just used the storage account in the connection URL, all of your available\\ncontainers appear in the left navigation view. If you specified a container in\\nconnection URL, only the specified container and its contents appear in the\\nnavigation view.\\nNavigate the storage account by selecting a folder or clicking on the expansion\\narrow next to a folder.\\nIn this view, you can select one or more shortcut target locations. Choose target\\nlocations by clicking the checkbox next a folder in the left navigation view.\\n5. Select Next\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The review page allows you to verify all of your selections. Here you can see each\\nshortcut that will be created. In the action column, you can click the pencil icon to\\nedit the shortcut name. You can click the trash can icon to delete shortcut.\\n6. Select Create.\\n7. The lakehouse automatically refreshes. The shortcut appears in the left Explorer\\npane.\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nCreate a OneLake shortcut\\nCreate an Amazon S3 shortcut\\nUse OneLake shortcuts REST APIs\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Create an Azure Blob Storage shortcut\\n(preview)\\nArticle• 05/19/2025\\nIn this article, you learn how to create an Azure Blob Storage shortcut inside a Microsoft Fabric\\nlakehouse.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts programmatically, see\\nOneLake shortcuts REST APIs.\\nA lakehouse in Microsoft Fabric. If you don't have a lakehouse, create one by following\\nthese steps: Create a lakehouse with OneLake.\\nAn Azure Storage account with data in a container.\\n1. Open a lakehouse in Fabric.\\n2. Right-click on a directory in the Explorer pane of the lakehouse.\\n3. Select New shortcut.\\n７ Note\\nAzure Blob Storage shortcuts are currently in public preview.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='When you create a shortcut in a lakehouse, the New shortcut window opens to walk you\\nthrough the configuration details.\\n1. On the New shortcut window, under External sources, select Azure Blob Storage\\n(preview).\\n2. Select Existing connection or Create new connection, depending on whether this\\nStorage account is already connected in your OneLake.\\nFor an Existing connection, select the connection from the drop-down menu.\\nTo Create new connection, provide the following connection settings.\\nSelect a source'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Field Description\\nAccount name\\nor URL\\nThe name of your blob storage account.\\nConnection The default value, Create new connection.\\nConnection\\nname\\nA name for your Azure Blob Storage connection. The service generates a\\nsuggested connection name based on the storage account name, but you\\ncan overwrite with a preferred name.\\nAuthentication\\nkind\\nSelect the authorization model from the drop-down menu that you want\\nto use to connect to the Storage account. The supported models are:\\naccount key, organizational account, Shared Access Signature (SAS),\\nservice principal, and workspace identity. Once you select a model, fill in\\nthe required credentials. For more information, see Azure Blob Storage\\nshortcuts authorization.\\n3. Select Next.\\n4. Browse to the target location for the shortcut.\\nIf you provided the storage account name in the connection details, all of your available\\ncontainers appear in the navigation view. If you specified a container in connection URL,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='containers appear in the navigation view. If you specified a container in connection URL,\\nonly the specified container and its contents appear in the navigation view.\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Navigate the storage account by selecting a folder or expanding a folder to view its child\\nitems.\\nChoose one or more target locations by selecting the checkbox next a folder in the\\nnavigation view. Then, select Next.\\n5. On the review page, verify your selections. Here you can see each shortcut to be created.\\nIn the Actions column, you can select the pencil icon to edit the shortcut name. You can\\nselect the trash can icon to delete the shortcut.\\n6. Select Create.\\n7. The lakehouse automatically refreshes. The shortcut or shortcuts appear in the Explorer\\npane.\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Create a OneLake shortcut\\nCreate an Amazon S3 shortcut\\nUse OneLake shortcuts REST APIs'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Create a OneDrive or SharePoint shortcut\\n(preview)\\nIn this article, you learn how to create a OneDrive or SharePoint shortcut inside a Microsoft\\nFabric lakehouse.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts programmatically, see\\nOneLake shortcuts REST APIs.\\nA lakehouse in Microsoft Fabric. If you don't have a lakehouse, create one by following\\nthese steps: Create a lakehouse with OneLake.\\nData in a OneDrive or SharePoint folder.\\n1. Open a lakehouse in Fabric.\\n2. Right-click on a directory in the Explorer pane of the lakehouse.\\n3. Select New shortcut.\\n７ Note\\nOneDrive and SharePoint shortcuts are currently in public preview.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='When you create a shortcut in a lakehouse, the New shortcut window opens to walk you\\nthrough the configuration details.\\n1. On the New shortcut window, under External sources, select OneDrive (preview) or\\nSharePoint Folder (preview).\\n2. Select Existing connection or New connection, depending on whether this account is\\nalready connected in your OneLake.\\nFor an Existing connection, select the connection from the drop-down menu.\\nTo create a New connection, provide the following connection settings:\\nField Description\\nSite URL The root URL of your SharePoint account.\\nTo retrieve your URL, sign in to OneDrive. Select the settings gear icon, then\\nOneDrive settings > More settings. Copy the OneDrive web URL from the more\\nSelect a source\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Field Description\\nsettings page and remove anything after _onmicrosoft_com. For example,\\nhttps://mytenant-my.sharepoint.com/personal/user01_mytenant_onmicrosoft_com.\\nConnection The default value, Create new connection.\\nConnection\\nname\\nA name for your connection. The service generates a suggested connection name\\nbased on the storage account name, but you can overwrite with a preferred\\nname.\\nAuthentication\\nkind\\nThe supported authentication type for this shortcut is Organizational account.\\n3. Select Next.\\n4. Browse to the target location for the shortcut.\\nNavigate by selecting a folder or expanding a folder to view its child items.\\nChoose one or more target locations by selecting the checkbox next a folder in the\\nnavigation view. Then, select Next.\\n5. On the Transform page, select a transformation option if you want to transform the data\\nin your shortcut or select Skip.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='For more information, see AI-powered transforms.\\n6. On the review page, verify your selections. Here you can see each shortcut to be created.\\nIn the Actions column, you can select the pencil icon to edit the shortcut name. You can\\nselect the trash can icon to delete the shortcut.\\n7. Select Create.\\n8. The lakehouse automatically refreshes. The shortcut or shortcuts appear in the Explorer\\npane.\\nCreate a OneLake shortcut\\nUse OneLake shortcuts REST APIs\\nLast updated on 11/18/2025\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Create an Amazon S3 shortcut\\nArticle• 03/31/2025\\nIn this article, you learn how to create an Amazon S3 shortcut inside a Fabric lakehouse.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts\\nprogrammatically, see OneLake shortcuts REST APIs.\\nYou can secure your S3 buckets using customer-managed KMS keys. As long as the IAM\\nuser has encrypt/decrypt permissions for the bucket key, OneLake can access the\\nencrypted data in the S3 bucket. For more information, see Configuring your bucket to\\nuse an S3 Bucket Key with SSE-KMS for new objects.\\nS3 shortcuts can take advantage of file caching to reduce egress costs associated with\\ncross-cloud data access. For more information, see OneLake shortcuts > Caching.\\nIf you don't have a lakehouse, create one by following these steps: Create a\\nlakehouse with OneLake.\\nEnsure your chosen S3 bucket and IAM user meet the access and authorization\\nrequirements for S3 shortcuts.\\n1. Open a lakehouse.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='requirements for S3 shortcuts.\\n1. Open a lakehouse.\\n2. Right-click on the Tables directory within the lakehouse.\\n3. Select New shortcut.\\nPrerequisites\\nCreate a shortcut'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Under External sources, select Amazon S3.\\n2. Enter the Connection settings according to the following table:\\nSelect a source\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Field Description Value\\nURL The connection\\nstring for your\\nAmazon S3\\nbucket.\\nhttps://BucketName.s3.RegionCode.amazonaws.com\\nConnection Previously\\ndefined\\nconnections for\\nthe specified\\nstorage location\\nappear in the\\ndrop-down. If no\\nconnections\\nexist, create a\\nnew connection.\\nCreate new connection\\nConnection\\nname\\nThe Amazon S3\\nconnection\\nname.\\nA name for your connection.\\nAuthentication\\nkind\\nThe Identity and\\nAccess\\nManagement\\n(IAM) policy. The\\npolicy must have\\nread and list\\npermissions. For\\nmore\\ninformation, see\\nIAM users.\\nDependent on the bucket policy.\\n\\uf80a\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Field Description Value\\nAccess Key ID The Identity and\\nAccess\\nManagement\\n(IAM) user key.\\nFor more\\ninformation, see\\nManage access\\nkeys for IAM\\nusers .\\nYour access key.\\nSecret Access\\nKey\\nThe Identity and\\nAccess\\nManagement\\n(IAM) secret key.\\nYour secret key.\\n3. Select Next.\\n4. Browse to the target location for the shortcut.\\nIf you used the global endpoint in the connection URL, all of your available buckets\\nappear in the left navigation view. If you used a bucket specific endpoint in the\\nconnection URL, only the specified bucket and its contents appear in the\\nnavigation view.\\nNavigate the storage account by selecting a folder or clicking on the expansion\\narrow next to a folder.\\nIn this view, you can select one or more shortcut target locations. Choose target\\nlocations by clicking the checkbox next a folder in the left navigation view.\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='5. Select Next\\nThe review page allows you to verify all of your selections. Here you can see each\\nshortcut that will be created. In the action column, you can click the pencil icon to\\nedit the shortcut name. You can click the trash can icon to delete shortcut.\\n6. Select Create.\\nThe lakehouse automatically refreshes. The shortcut appears in the left Explorer pane\\nunder the Tables section.\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nCreate a OneLake shortcut\\nCreate an Azure Data Lake Storage Gen2 shortcut\\nUse OneLake shortcuts REST APIs\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Create an Amazon S3 compatible shortcut\\n07/28/2025\\nIn this article, you learn how to create an S3 compatible shortcut inside a Fabric lakehouse. For\\nan overview of shortcuts, see OneLake shortcuts.\\nS3 compatible shortcuts can take advantage of file caching to reduce egress costs associated\\nwith cross-cloud data access. For more information, see OneLake shortcuts Caching. Currently\\nonly key or secret authentication is supported for S3-compatible sources. Entra-based OAuth,\\nService Principal, and RoleArn are not yet supported.\\nIf you don't have a lakehouse, create one by following these steps: Create a lakehouse\\nwith OneLake.\\nEnsure your chosen S3 compatible bucket and secret key credentials meet the access and\\nauthorization requirements for S3 shortcuts.\\n1. Open a lakehouse.\\n2. Right-click on a directory within the Lake view of the lakehouse.\\n3. Select New shortcut.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Under External sources, select Amazon S3 compatible.\\n2. Enter the Connection settings according to the following table:\\nSelect a source\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Field Description Value\\nURL The connection string for your S3 compatible endpoint.\\nFor this shortcut type, you must provide a non-bucket\\nspecific URL. This URL must allow path style bucket\\naddressing, not just virtual hosted style.\\nhttps://s3.contoso.com\\nConnectionPreviously defined connections for the specified storage\\nlocation appear in the drop-down. If no connections\\nexist, create a new connection.\\nCreate new connection\\nConnection\\nname\\nThe S3 compatible connection name. A name for your\\nconnection.\\nAccess Key\\nID\\nThe access key ID to be used when accessing the S3\\ncompatible endpoint.\\nYour access key.\\nSecret\\nAccess Key\\nThe secret key associated with the access key ID.Your secret key.\\n3. Select Next.\\n4. Enter a name for your shortcut.\\nOptionally, you can enter a sub path to select a specific folder in your S3 bucket.\\n\\uf80a\\nﾉ Expand table\\n７ Note'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='5. Select Create.\\nThe lakehouse automatically refreshes. The shortcut appears under Files in the Explorer pane.\\nCreate a OneLake shortcut\\nCreate an Azure Data Lake Storage Gen2 shortcut\\nShortcut paths are case sensitive.\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Integrate Microsoft Entra with AWS S3\\nshortcuts using service principal\\nauthentication\\n07/10/2025\\nYou can integrate Microsoft Entra with AWS S3 using the Service Principal Name (SPN)\\napproach. This integration enables seamless, secure access to S3 buckets using Microsoft Entra\\ncredentials, simplifying identity management and enhancing security.\\nUnified identity management: Use Microsoft Entra credentials to access Amazon S3. No\\nneed to manage AWS IAM users.\\nOIDC-based authentication: Uses OpenID Connect for secure authentication with AWS\\nIAM roles.\\nAuditing support: Full traceability through AWS CloudTrail to monitor role assumptions.\\nSeamless integration: Designed to integrate with existing AWS deployments with minimal\\nconfiguration changes.\\nThe Entra-AWS integration is built on a federated identity model that uses OpenID Connect\\n(OIDC) to enable secure, temporary access to AWS resources. The architecture consists of the'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"(OIDC) to enable secure, temporary access to AWS resources. The architecture consists of the\\nfollowing three main components that work together to establish trust, authenticate users, and\\nauthorize access to Amazon S3 from Microsoft Fabric:\\n1. A Service Principal (SPN) registered in Microsoft Entra.\\n2. An OIDC trust relationship between AWS and Microsoft Entra.\\n3. A Fabric connection that uses temporary credentials from AWS Security Token Service\\n(STS).\\nIn the following sections you'll configure Microsoft Entra ID, AWS IAM, and Microsoft Fabric for\\nsecure access to Amazon S3 using the service principal-based integration. This setup\\nestablishes the necessary trust relationships and connection details required for the integration\\nto work.\\nKey benefits\\nArchitecture\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Sign in to Azure portal and navigate to Microsoft Entra ID.\\nFrom the left-hand menu, expand Manage > App registrations > New registration. Fill\\nout the following details:\\nName: Enter a name for your application such as S3AccessServicePrincipal.\\nRedirect URL: Leave it blank or set to https://localhost if necessary.\\nSelect Register to register your application.\\nOpen the Microsoft Entra application you created above.\\nFrom the left-hand menu, expand Manage > Certificates and secrets > New client secret\\nto add a new secret\\nNote down the generated secret and its expiration date\\nClient Secret: Get this value from the previous step\\n*Tenant ID: From the Azure portal, navigate to Microsoft Entra ID and open the Overview\\ntab and get the Tenant ID value.\\n７ Note\\nOnly key or secret authentication is supported for S3-compatible sources; Entra-based\\nOAuth, Service Principal, and RoleArn are not supported.\\nConfigure Microsoft Entra ID\\nStep1: Register a Microsoft Entra application\\n７ Note'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Configure Microsoft Entra ID\\nStep1: Register a Microsoft Entra application\\n７ Note\\nIt's recommended to use a unique Service Principal per AWS role for enhanced security\\nStep2: Create a client secret\\nStep3: Get the application details\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='From the Azure portal, navigate to Microsoft Entra ID. From the left-hand navigation,\\nexpand the Manage tab and open Enterprise applications. Search for the application you\\ncreated in the previous step. Copy the following values\\nApplication ID (also known as Client ID)\\nObject ID\\nThe following screenshot shows you how to get the application/client ID and object ID.\\nSign into the AWS IAM portal.\\nNavigate to AWS IAM → Identity providers → Add provider\\nSelect Provider type as Open ID Connect\\nProvider URL: https://sts.windows.net/<your-tenant-id>\\nAudience: https://analysis.windows.net/powerbi/connector/AmazonS3\\n７ Note\\nThese values are from Microsoft Entra ID > Enterprise applications tab and NOT from\\nMicrosoft Entra ID > App registrations tab.\\n\\uf80a\\nAWS IAM configuration\\nStep 1: Create OIDC identity provider'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Navigate to AWS IAM → Roles → Create Role\\nTrusted entity type: Web identity\\nIdentity provider: Select the Open ID Connect provider created in Step 1\\nAudience: https://analysis.windows.net/powerbi/connector/AmazonS3\\nAssign appropriate S3 access policies to the role\\nEnsure that the Trust policy has the service principal as one of the conditions\\nJSON\\n\\uf80a\\nStep 2: Create IAM roles\\n{\\n  \"Version\": \"2012-10-17\",\\n  \"Statement\": [\\n    {\\n      \"Effect\": \"Allow\",\\n      \"Principal\": {\\n        \"Federated\": \"arn:aws:iam::<aws-account>:oidc-\\nprovider/sts.windows.net/<tenant-id>/\" // (1)\\n      },\\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",// (2)\\n      \"Condition\": {\\n        \"StringEquals\": {\\n          \"sts.windows.net/<tenant-id>/:sub\": \"<Object ID of the SPN that \\nwill assume this role>\", // (3)\\n          \"sts.windows.net/<tenant-id>/:aud\": \\n\"https://analysis.windows.net/powerbi/connector/AmazonS3\" // (4)\\n        }\\n      }\\n    }'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Description of key fields:\\n1. Principal.Federated – Specifies the external identity provider(OIDC from Microsoft Entra\\nID).\\n2. Action – Grants permission to assume the role using a web identity token.\\n3. Condition > :sub – Limits which Microsoft Entra ID service can assume the role. This is the\\nObject ID that you noted in Step3: Get the application details\\n4. Condition > :aud – Ensures the request is from Power BI's S3 connector.\\nUse Microsoft Fabric OneLake's shortcut creation interface to create the shortcut as described\\nin the create an S3 shortcut article. Follow the same steps, but set RoleARN to the Amazon\\nResource Name (ARN) for the IAM role, and set the Authentication Kind to Service Principal\\nand fill in the following details:\\nTenant ID: Tenant ID of the Microsoft Entra application\\nService principal client ID: The Application ID you got in the previous step.\\nService principal key: The client secret of the Microsoft Entra application\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Service principal key: The client secret of the Microsoft Entra application\\nUse a separate service principal per AWS role for better isolation and auditability\\n  ]\\n}\\n\\uf80a\\nCreate an S3 connection in Fabric\\nSecurity recommendations'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Rotate secrets periodically and store them securely\\nMonitor AWS CloudTrail for STS-related activity\\nThis feature currently supports only the service principal-based approach; OAuth and\\nWorkspace Identity aren't yet supported.\\nAccess to S3 buckets behind a firewall via on-premises data gateway isn't currently\\nsupported with service principal or OAuth.\\nCreate an S3 shortcut\\nCreate an Amazon S3 compatible shortcut\\nCurrent limitations\\nRelated content\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Create a Google Cloud Storage (GCS)\\nshortcut\\nArticle• 03/31/2025\\nIn this article, you learn how to create a Google Cloud Storage (GCS) shortcut inside a\\nFabric lakehouse.\\nFor an overview of shortcuts, see OneLake shortcuts. To create shortcuts\\nprogrammatically, see OneLake shortcuts REST APIs.\\nGCS shortcuts can take advantage of file caching to reduce egress costs associated with\\ncross-cloud data access. For more information, see OneLake shortcuts > Caching.\\nIf you don't have a lakehouse, create one by following these steps: Creating a\\nlakehouse with OneLake.\\nEnsure your chosen GCS bucket and user meet the access and authorization\\nrequirements for GCS shortcuts.\\n1. Open a lakehouse.\\n2. Right-click on a directory within the Lake view of the lakehouse.\\n3. Select New shortcut.\\nPrerequisites\\nCreate a shortcut\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Under External sources, select Google Cloud Storage.\\n2. Enter the Connection settings according to the following table:\\n\\uf80a\\nSelect a source\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Field Description Value\\nURL The connection string\\nfor your GCS bucket.\\nThe bucket name is\\noptional.\\nhttps://BucketName.storage.googleapis.com\\nhttps://storage.googleapis.com\\nConnection Previously defined\\nconnections for the\\nspecified storage\\nlocation appear in the\\ndrop-down. If no\\nconnections exist, create\\na new connection.\\nCreate new connection\\nConnection\\nname\\nThe user defined name\\nfor the connection.\\nA name for your connection.\\nAuthentication\\nkind\\nFabric uses Hash-based\\nMessage Authentication\\nCode (HMAC) keys to\\naccess Google Cloud\\nstorage. These keys are\\nassociated with a user\\nor service account. The\\naccount must have\\npermission to access the\\ndata within the GCS\\nbucket. If the bucket\\nspecific endpoint was\\nused in the connection\\nHMAC Key\\n\\uf80a\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Field Description Value\\nURL, the account must\\nhave the\\nstorage.objects.get\\nand\\nstorage.objects.list\\npermissions. If the\\nglobal endpoint was\\nused in the connection\\nURL, the account must\\nalso have the\\nstorage.buckets.list\\npermission.\\nAccess ID The access key\\nassociated with a user\\nor service account. For\\nmore on creating HMAC\\nkeys, see Manage\\nHMAC Keys .\\nYour access key.\\nSecret The secret for the access\\nkey.\\nYour secret key.\\n3. Select Next.\\n4. Browse to the target location for the shortcut.\\nIf you used the global endpoint in the connection URL, all of your available buckets\\nappear in the left navigation view. If you used a bucket specific endpoint in the\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='connection URL, only the specified bucket and its contents appear in the\\nnavigation view.\\nNavigate the storage account by selecting a folder or clicking on the expansion\\narrow next to a folder.\\nIn this view, you can select one or more shortcut target locations. Choose target\\nlocations by clicking the checkbox next a folder in the left navigation view.\\n5. Select Next\\nThe review page allows you to verify all of your selections. Here you can see each\\nshortcut that will be created. In the action column, you can click the pencil icon to\\nedit the shortcut name. You can click the trash can icon to delete shortcut.\\n6. Select Create.\\nThe lakehouse automatically refreshes. The shortcut appears in the left Explorer pane.\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nCreate a OneLake shortcut\\nCreate an Azure Data Lake Storage Gen2 shortcut\\nUse OneLake shortcuts REST APIs\\n\\uf80a\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Provide product feedback | Ask the community'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Create shortcuts to on-premises data\\nArticle• 03/31/2025\\nWith OneLake Shortcuts, you can create virtual references to bring together data from a\\nvariety sources across clouds, regions, systems, and domains – all with no data\\nmovement or duplication. By using a Fabric on-premises data gateway (OPDG), you can\\nalso create shortcuts to on-premises data sources, such as S3 compatible storage\\nhosted on-premises. With this feature, you can also create shortcuts to other network-\\nrestricted data sources, such as Amazon S3 or Google Cloud Storage buckets configured\\nbehind a firewall or Virtual Private Cloud (VPC).\\nOn-premises data gateways are software agents that you install on a Windows machine\\nand configure to connect to your data endpoints. By selecting an OPDG when creating a\\nshortcut, you can establish network connectivity between OneLake and your data\\nsource.\\nThis feature is available for the following shortcut types:\\nAmazon S3\\nS3 compatible\\nGoogle Cloud Storage'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Amazon S3\\nS3 compatible\\nGoogle Cloud Storage\\nYou can use this feature in any Fabric-enabled workspace.\\nOn-premises data gateway shortcuts can take advantage of file caching to reduce\\negress costs associated with cross-cloud data access. For more information, see\\nOneLake shortcuts > Caching.\\nIn this document, we show you how to install and use these on-premises data gateways\\nto create shortcuts to on-premises or network-restricted data.\\nCreate or identify a Fabric lakehouse that will contain your shortcut(s).\\nIdentify the endpoint URL associated with your Amazon S3, Google Cloud Storage,\\nor S3 compatible location.\\n） Important\\nThis feature is in preview.\\nPrerequisites'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='For S3 compatible, the endpoint is the URL for the service, not a specific bucket.\\nFor example:\\nhttps://mys3api.contoso.com\\nhttp://10.0.1.4:9000\\nFor Amazon S3, the endpoint is the URL for a specific bucket. For example:\\nhttps://BucketName.s3.us-east.amazonaws.com\\nFor Google Cloud Storage, the endpoint is either the URL for the bucket or the\\nservice. For example:\\nhttps://storage.googleapis.com\\nhttps://bucketname.storage.googleapis.com\\nIdentify the user or identity credentials that meet the necessary access and\\nauthorization requirements for your data source. Your credentials generally need to\\nbe able to list buckets, list objects, and read data.\\nIdentify a physical or virtual machine that:\\nHas network connectivity to your storage endpoint. This article explains how\\nyou can confirm this connectivity before creating your shortcut.\\nAllows you to install software.\\nFollow the instructions to install a standard on-premises data gateway on the'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Follow the instructions to install a standard on-premises data gateway on the\\nmachine you identified. Be sure to install the latest version.\\nIf your storage endpoint uses a self-signed certificate for HTTPS connections, be\\nsure to trust this certificate on the machine hosting your gateway.\\nBefore setting up your shortcut, follow these steps to confirm that your gateway can\\nconnect to your storage endpoint.\\n1. Log in to the machine hosting the gateway.\\n2. Install a client application that can query S3 compatible data sources, such as the\\nAmazon Web Services Command Line Interface, WinSCP, or another tool of choice.\\n3. Connect to your endpoint URL and provide the credentials you identified in the\\nprerequisite steps.\\n4. Ensure you can explore and read data from your storage location.\\nReview the instructions for creating an Amazon S3, Google Cloud Storage, or S3\\ncompatible shortcut.\\nCheck connectivity from gateway host\\nCreate a shortcut'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nDuring shortcut creation, select your on-premises data gateway (OPDG) in the Data\\ngateway dropdown field.\\nIf you encounter any connectivity issues during shortcut creation, try the following\\ntroubleshooting steps.\\nAs needed, ensure the machine hosting your gateway can connect to your storage\\nendpoint. Follow the steps to check connectivity.\\nIf you're using HTTPS and need to use a self-signed certificate, ensure the machine\\nhosting your gateway trusts the certificate. You may need to install the self-signed\\ncertificate on the machine.\\nCreate an Amazon S3 shortcut\\nCreate a Google Cloud Storage shortcut\\nCreate an S3 compatible shortcut\\n７ Note\\nIf you do not see your OPDG in the Data gateway dropdown field and someone\\nelse created the gateway, ask them to share the gateway with you from the\\nManage connections and gateways interface.\\nTroubleshooting\\nRelated content\\n\\ue8e1Yes \\ue8e0No\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Use Iceberg tables with OneLake\\n07/01/2025\\nIn Microsoft OneLake, you can seamlessly work with tables in both Delta Lake and Apache\\nIceberg formats.\\nThis flexibility is enabled through metadata virtualization, a feature that allows Iceberg tables\\nto be interpreted as Delta Lake tables, and vice versa. You can directly write Iceberg tables or\\ncreate shortcuts to them, making these tables accessible across various Fabric workloads.\\nSimilarly, Fabric tables written in the Delta Lake format can be read using Iceberg readers.\\nWhen you write or create a shortcut to an Iceberg table folder, OneLake automatically\\ngenerates virtual Delta Lake metadata (Delta log) for the table, enabling its use with Fabric\\nworkloads. Conversely, Delta Lake tables now include virtual Iceberg metadata, allowing\\ncompatibility with Iceberg readers.\\nWhile this article includes guidance for using Iceberg tables with Snowflake, this feature is'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='While this article includes guidance for using Iceberg tables with Snowflake, this feature is\\nintended to work with any Iceberg tables with Parquet-formatted data files in storage.\\n） Important\\nThis feature is in preview.\\nVirtualize Delta Lake tables as Iceberg'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='To set up the automatic conversion and virtualization of tables from Delta Lake format to\\nIceberg format, follow these steps.\\n1. Enable automatic table virtualization of Delta Lake tables to the Iceberg format by turning\\non the delegated OneLake setting named Enable Delta Lake to Apache Iceberg table\\nformat virtualization in your workspace settings.\\n2. Make sure your Delta Lake table, or a shortcut to it, is located in the Tables section of\\nyour data item. The data item may be a lakehouse or another Fabric data item.\\n3. Confirm that your Delta Lake table has converted successfully to the virtual Iceberg\\nformat. You can do this by examining the directory behind the table.\\nTo view the directory if your table is in a lakehouse, you can right-click the table in the\\nFabric UI and select View files.\\nIf your table is in another data item type, such as a warehouse, a database, or a mirrored\\ndatabase, you will need to use a client like Azure Storage Explorer or OneLake File'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='database, you will need to use a client like Azure Storage Explorer or OneLake File\\nExplorer, rather than the Fabric UI, to view the files behind the table.\\n4. You should see a directory named metadata inside the table folder, and it should contain\\nmultiple files, including the conversion log file. Open the conversion log file to see more\\ninfo about the Delta Lake to Iceberg conversion, including the timestamp of the most\\nrecent conversion and any error details.\\n5. If the conversion log file shows that the table was successfully converted, read the Iceberg\\ntable using your service, app, or library of choice.\\nDepending on what Iceberg reader you use, you will need to know either the the path to\\nthe table directory or to the most recent .metadata.json file shown in the metadata\\n７ Note\\nThis setting controls a feature that is currently in preview. This setting will be\\nremoved in a future update when the feature is enabled for all users and is no longer\\nin preview.\\n\\uea80 Tip'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='in preview.\\n\\uea80 Tip\\nIf your lakehouse is schema-enabled, then your table directory will be located\\ndirectly within a schema such as dbo. If your lakehouse is not schema-enabled, then\\nyour table directory will be directly within the Tables directory.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"directory.\\nYou can see the HTTP path to the latest metadata file of your table by opening the\\nProperties view for the *.metadata.json file with the highest version number. Take note\\nof this path.\\nThe path to your data item's Tables folder may look like this:\\nWithin that folder, the relative path to the latest metadata file may look like\\ndbo/MyTable/metadata/321.metadata.json.\\nTo read your virtual Iceberg table using Snowflake, follow the steps in this guide.\\nIf you already have an Iceberg table in a storage location supported by OneLake shortcuts,\\nfollow these steps to create a shortcut and have your Iceberg table appear with the Delta Lake\\nformat.\\n1. Locate your Iceberg table. Find where your Iceberg table is stored, which could be in\\nAzure Data Lake Storage, OneLake, Amazon S3, Google Cloud Storage, or an S3\\ncompatible storage service.\\nhttps://onelake.dfs.fabric.microsoft.com/83896315-c5ba-4777-8d1c-\\ne4ab3a7016bc/a95f62fa-2826-49f8-b561-a163ba537828/Tables/\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"e4ab3a7016bc/a95f62fa-2826-49f8-b561-a163ba537828/Tables/\\nCreate a table shortcut to an Iceberg table\\n７ Note\\nIf you're using Snowflake and aren't sure where your Iceberg table is stored, you can\\nrun the following statement to see the storage location of your Iceberg table.\\nSELECT SYSTEM$GET_ICEBERG_TABLE_INFORMATION('<table_name>');\\nRunning this statement returns a path to the metadata file for the Iceberg table. This\\npath tells you which storage account contains the Iceberg table. For example, here's\\nthe relevant info to find the path of an Iceberg table stored in Azure Data Lake\\nStorage:\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Your Iceberg table folder needs to contain a metadata folder, which itself contains at least\\none file ending in .metadata.json.\\n2. In your Fabric lakehouse, create a new table shortcut in the Tables area of a lakehouse.\\n3. For the target path of your shortcut, select the Iceberg table folder. The Iceberg table\\nfolder contains the metadata and data folders.\\n4. Once your shortcut is created, you should automatically see this table reflected as a Delta\\nLake table in your lakehouse, ready for you to use throughout Fabric.\\n{\"metadataLocation\":\"azure://<storage_account_path>/<path_within_storage>/<tabl\\ne_name>/metadata/00001-389700a2-977f-47a2-9f5f-\\n7fd80a0d41b2.metadata.json\",\"status\":\"success\"}\\n\\uea80 Tip\\nIf you see schemas such as dbo under the Tables folder of your lakehouse, then the\\nlakehouse is schema-enabled. In this case, right-click on the schema and create a\\ntable shortcut under the schema.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"If your new Iceberg table shortcut doesn't appear as a usable table, check the\\nTroubleshooting section.\\nThe following tips can help make sure your Iceberg tables are compatible with this feature:\\nOpen your Iceberg folder in your preferred storage explorer tool, and check the directory\\nlisting of your Iceberg folder in its original location. You should see a folder structure like the\\nfollowing example.\\nIf you don't see the metadata folder, or if you don't see files with the extensions shown in this\\nexample, then you might not have a properly generated Iceberg table.\\nTroubleshooting\\nCheck the folder structure of your Iceberg table\\n../\\n|-- MyIcebergTable123/\\n    |-- data/\\n        |-- A5WYPKGO_2o_APgwTeNOAxg_0_1_002.parquet\\n        |-- A5WYPKGO_2o_AAIBON_h9Rc_0_1_003.parquet\\n    |-- metadata/\\n        |-- 00000-1bdf7d4c-dc90-488e-9dd9-2e44de30a465.metadata.json\\n        |-- 00001-08bf3227-b5d2-40e2-a8c7-2934ea97e6da.metadata.json\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='|-- 00001-08bf3227-b5d2-40e2-a8c7-2934ea97e6da.metadata.json\\n        |-- 00002-0f6303de-382e-4ebc-b9ed-6195bd0fb0e7.metadata.json\\n        |-- 1730313479898000000-Kws8nlgCX2QxoDHYHm4uMQ.avro\\n        |-- 1730313479898000000-OdsKRrRogW_PVK9njHIqAA.avro\\n        |-- snap-1730313479898000000-9029d7a2-b3cc-46af-96c1-ac92356e93e9.avro\\n        |-- snap-1730313479898000000-913546ba-bb04-4c8e-81be-342b0cbc5b50.avro'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"When an Iceberg table is virtualized as a Delta Lake table, a folder named _delta_log/ can be\\nfound inside the shortcut folder. This folder contains the Delta Lake format's metadata (the\\nDelta log) after successful conversion.\\nThis folder also includes the latest_conversion_log.txt file, which contains the latest\\nattempted conversion's success or failure details.\\nTo see the contents of this file after creating your shortcut, open the menu for the Iceberg table\\nshortcut under Tables area of your lakehouse and select View files.\\nYou should see a structure like the following example:\\nOpen the conversion log file to see the latest conversion time or failure details. If you don't see\\na conversion log file, conversion wasn't attempted.\\nIf you don't see a conversion log file, then the conversion wasn't attempted. Here are two\\ncommon reasons why conversion isn't attempted:\\nThe shortcut wasn't created in the right place.\\nCheck the conversion log\\nTables/\\n|-- MyIcebergTable123/\\n    |-- data/\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Check the conversion log\\nTables/\\n|-- MyIcebergTable123/\\n    |-- data/\\n        |-- <data files>\\n    |-- metadata/\\n        |-- <metadata files>\\n    |-- _delta_log/   <-- Virtual folder. This folder doesn't exist in the \\noriginal location.\\n        |-- 00000000000000000000.json\\n        |-- latest_conversion_log.txt   <-- Conversion log with latest \\nsuccess/failure details.\\nIf conversion wasn't attempted\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"In order for a shortcut to an Iceberg table to get converted to the Delta Lake format, the\\nshortcut must be placed directly under the Tables folder of a non-schema-enabled\\nlakehouse. You shouldn't place the shortcut in the Files section or under another folder if\\nyou want the table to be automatically virtualized as a Delta Lake table.\\nThe shortcut's target path is not the Iceberg folder path.\\nWhen you create the shortcut, the folder path you select in the target storage location\\nmust only be the Iceberg table folder. This folder contains the metadata and data folders.\\nIf you are using Snowflake to write a new Iceberg table to OneLake, you might see the\\nfollowing error message:\\nFabric capacity region cannot be validated. Reason: 'Invalid access token. This may be due\\nto authentication and scoping. Please verify delegated scopes.'\\nIf you see this error, have your Fabric tenant admin double-check that you've enabled both\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='If you see this error, have your Fabric tenant admin double-check that you\\'ve enabled both\\ntenant settings mentioned in the Write an Iceberg table to OneLake using Snowflake section:\\n\"Fabric capacity region cannot be validated\" error message in\\nSnowflake'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"1. In the upper-right corner of the Fabric UI, open Settings, and select Admin portal.\\n2. Under Tenant settings, in the Developer settings section, enable the setting labeled\\nService principals can use Fabric APIs.\\n3. In the same area, in the OneLake settings section, enable the setting labeled Users can\\naccess data stored in OneLake with apps external to Fabric.\\nKeep in mind the following temporary limitations when you use this feature:\\nSupported data types\\nThe following Iceberg column data types map to their corresponding Delta Lake types\\nusing this feature.\\nIceberg\\ncolumn type\\nDelta Lake\\ncolumn type\\nComments\\nint integer\\nlong long See Type width issue.\\nfloat float\\ndouble double See Type width issue.\\ndecimal(P, S) decimal(P, S) See Type width issue.\\nboolean boolean\\ndate date\\ntimestamp timestamp_ntz The timestamp Iceberg data type doesn't contain time zone\\ninformation. The timestamp_ntz Delta Lake type isn't fully\\nsupported across Fabric workloads. We recommend the use of\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='supported across Fabric workloads. We recommend the use of\\ntimestamps with time zones included.\\ntimestamptz timestamp In Snowflake, to use this type, specify timestamp_ltz as the\\ncolumn type during Iceberg table creation. More info on\\nIceberg data types supported in Snowflake can be found here.\\nstring string\\nbinary binary\\ntime N/A Not supported\\nLimitations and considerations\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Type width issue\\nIf you use Snowflake to write your Iceberg table and the table contains column types\\nINT64, double, or Decimal with precision >= 10, then the resulting virtual Delta Lake\\ntable may not be consumable by all Fabric engines. You may see errors such as:\\nWe're working on a fix for this issue.\\nWorkaround: If you're using the Lakehouse table preview UI and see this issue, you can\\nresolve this error by switching to the SQL Endpoint view (top right corner, select\\nLakehouse view, switch to SQL Endpoint) and previewing the table from there. If you then\\nswitch back to the Lakehouse view, the table preview should display properly.\\nIf you're running a Spark notebook or job and encounter this issue, you can resolve this\\nerror by setting the spark.sql.parquet.enableVectorizedReader Spark configuration to\\nfalse. Here's an example PySpark command to run in a Spark notebook:\\nIceberg table metadata storage isn't portable\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Iceberg table metadata storage isn't portable\\nThe metadata files of an Iceberg table refer to each other using absolute path references.\\nIf you copy or move an Iceberg table's folder contents to another location without\\nrewriting the Iceberg metadata files, the table becomes unreadable by Iceberg readers,\\nincluding this OneLake feature.\\nWorkaround:\\nIf you need to move your Iceberg table to another location to use this feature, use the\\ntool that originally wrote the Iceberg table to write a new Iceberg table in the desired\\nlocation.\\nIceberg table folders must contain only one set of metadata files\\nIf you drop and recreate an Iceberg table in Snowflake, the metadata files aren't cleaned\\nup. This behavior is by design, in support of the UNDROP feature in Snowflake. However,\\nbecause your shortcut points directly to a folder and that folder now has multiple sets of\\nParquet column cannot be converted in file ... Column: [ColumnA], Expected: \\ndecimal(18,4), Found: INT32.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='decimal(18,4), Found: INT32.\\nspark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"metadata files within it, we can't convert the table until you remove the old table’s\\nmetadata files.\\nConversion will fail if more than one set of metadata files are found in the Iceberg table's\\nmetadata folder.\\nWorkaround:\\nTo ensure the converted table reflects the correct version of the table:\\nEnsure you aren’t storing more than one Iceberg table in the same folder.\\nClean up any contents of an Iceberg table folder after dropping it, before recreating\\nthe table.\\nMetadata changes not immediately reflected\\nIf you make metadata changes to your Iceberg table, such as adding a column, deleting a\\ncolumn, renaming a column, or changing a column type, the table may not be\\nreconverted until a data change is made, such as adding a row of data.\\nWe're working on a fix that picks up the correct latest metadata file that includes the\\nlatest metadata change.\\nWorkaround:\\nAfter making the schema change to your Iceberg table, add a row of data or make any\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Workaround:\\nAfter making the schema change to your Iceberg table, add a row of data or make any\\nother change to the data. After that change, you should be able to refresh and see the\\nlatest view of your table in Fabric.\\nRegion availability limitation\\nThe feature isn't yet available in the following regions:\\nQatar Central\\nNorway West\\nWorkaround:\\nWorkspaces attached to Fabric capacities in other regions can use this feature. See the full\\nlist of regions where Microsoft Fabric is available.\\nPrivate links not supported\\nThis feature isn't currently supported for tenants or workspaces that have private links\\nenabled.\\nWe're working on an improvement to remove this limitation.\\nOneLake shortcuts must be same-region\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"We have a temporary limitation on the use of this feature with shortcuts that point to\\nOneLake locations: the target location of the shortcut must be in the same region as the\\nshortcut itself.\\nWe're working on an improvement to remove this requirement.\\nWorkaround:\\nIf you have a OneLake shortcut to an Iceberg table in another lakehouse, be sure that the\\nother lakehouse is associated with a capacity in the same region.\\nCertain Iceberg partition transform types are not supported\\nCurrently, the Iceberg partition types bucket[N], truncate[W], and void are not\\nsupported.\\nIf the Iceberg table being converted contains these partition transform types,\\nvirtualization to the Delta Lake format will not succeed.\\nWe're working on an improvement to remove this limitation.\\nUse Snowflake to write or read Iceberg tables in OneLake.\\nLearn more about Fabric and OneLake security.\\nLearn more about OneLake shortcuts.\\nRelated content\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Use Snowflake with Iceberg tables in\\nOneLake\\nMicrosoft OneLake can be used with Snowflake for storage and access of Apache Iceberg\\ntables.\\nFollow this guide to use Snowflake on Azure to:\\nwrite Iceberg tables directly to OneLake\\nread virtual Iceberg tables converted from the Delta Lake format\\nBefore getting started, follow the pre-requisite steps shown below.\\nTo use Snowflake on Azure to write or read Iceberg tables with OneLake, your Snowflake\\naccount's identity in Entra ID needs to be able to communicate with Fabric. Enable the Fabric\\ntenant-level settings that allow service principals to call Fabric APIs and to call OneLake APIs.\\nIf you use Snowflake on Azure, you can write Iceberg tables to OneLake by following these\\nsteps:\\n1. Make sure your Fabric capacity is in the same Azure location as your Snowflake instance.\\nIdentify the location of the Fabric capacity associated with your Fabric lakehouse. Open\\nthe settings of the Fabric workspace that contains your lakehouse.\\n） Important\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='the settings of the Fabric workspace that contains your lakehouse.\\n） Important\\nThis feature is in preview.\\nPrerequisite\\nWrite an Iceberg table to OneLake using Snowflake\\non Azure'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='In the bottom-left corner of your Snowflake on Azure account interface, check the Azure\\nregion of the Snowflake account.\\nIf these regions are different, you need to use a different Fabric capacity in the same\\nregion as your Snowflake account.\\n2. Open the menu for the Files area of your lakehouse, select Properties, and copy the URL\\n(the HTTPS path) of that folder.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='3. Identify your Fabric tenant ID. Select your user profile in the top-right corner of the Fabric\\nUI, and hover over the info bubble next to your Tenant Name. Copy the Tenant ID.\\n4. In Snowflake, set up your EXTERNAL VOLUME using the path to the Files folder in your\\nlakehouse. More info on setting up Snowflake external volumes can be found here.\\nSQL\\n７ Note\\nSnowflake requires the URL scheme to be azure://, so be sure to change the path\\nfrom https:// to azure://.\\nCREATE OR REPLACE EXTERNAL VOLUME onelake_write_exvol\\nSTORAGE_LOCATIONS =\\n('),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"In this sample, any tables created using this external volume are stored in the Fabric\\nlakehouse, within the Files/icebergtables folder.\\n5. Now that your external volume is created, run the following command to retrieve the\\nconsent URL and name of the application that Snowflake uses to write to OneLake. This\\napplication is used by any other external volume in your Snowflake account.\\nSQL\\nThe output of this command returns the AZURE_CONSENT_URL and\\nAZURE_MULTI_TENANT_APP_NAME properties. Take note of both values. The Azure multitenant\\napp name looks like <name>_<number>, but you only need to capture the <name> portion.\\n6. Open the consent URL from the previous step in a new browser tab, if you haven't done\\nthis previously. If you would like to proceed, consent to the required application\\npermissions, if prompted. You may be redirected to the main Snowflake website.\\n7. Back in Fabric, open your workspace and select Manage access, then Add people or\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"7. Back in Fabric, open your workspace and select Manage access, then Add people or\\ngroups. Grant the application used by your Snowflake external volume the permissions\\nneeded to write data to lakehouses in your workspace. We recommend granting the\\nContributor role.\\n8. Back in Snowflake, use your new external volume to create an Iceberg table.\\nSQL\\nAfter running this statement, a new Iceberg table folder named Inventory has been\\ncreated within the folder path defined in the external volume.\\n    (\\n        NAME = 'onelake_write_exvol'\\n        STORAGE_PROVIDER = 'AZURE'\\n        STORAGE_BASE_URL = 'azure://<path_to_lakehouse>/Files/icebergtables'\\n        AZURE_TENANT_ID = '<Tenant_ID>'\\n    )\\n);\\nDESC EXTERNAL VOLUME onelake_write_exvol;\\nCREATE OR REPLACE ICEBERG TABLE MYDATABASE.PUBLIC.Inventory (\\n    InventoryId int,\\n    ItemName STRING\\n)\\nEXTERNAL_VOLUME = 'onelake_write_exvol'\\nCATALOG = 'SNOWFLAKE'\\nBASE_LOCATION = 'Inventory/';\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"9. Add some data to your Iceberg table.\\nSQL\\n10. Finally, in the Tables area of the same lakehouse, you can create a OneLake shortcut to\\nyour Iceberg table. Through that shortcut, your Iceberg table appears as a Delta Lake\\ntable for consumption across Fabric workloads.\\nTo use Snowflake on Azure to read a virtual Iceberg table based on a Delta Lake table in Fabric,\\nfollow these steps.\\n1. Follow the guide to confirm your Delta Lake table has converted successfully to Iceberg,\\nand take note of the path to the data item containing your table, as well as your table's\\nmost recent *.metadata.json file.\\n2. Identify your Fabric tenant ID. Select your user profile in the top-right corner of the Fabric\\nUI, and hover over the info bubble next to your Tenant Name. Copy the Tenant ID.\\n3. In Snowflake, set up your EXTERNAL VOLUME using the path to the Tables folder of the data\\nitem that contains your table. More info on setting up Snowflake external volumes can be\\nfound here.\\nSQL\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"found here.\\nSQL\\nINSERT INTO MYDATABASE.PUBLIC.Inventory\\nVALUES\\n(123456,'Amatriciana');\\nRead a virtual Iceberg table from OneLake using\\nSnowflake on Azure\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"4. Now that your external volume is created, run the following command to retrieve the\\nconsent URL and name of the application that Snowflake uses to write to OneLake. This\\napplication is used by any other external volume in your Snowflake account.\\nSQL\\nThe output of this command returns the AZURE_CONSENT_URL and\\nAZURE_MULTI_TENANT_APP_NAME properties. Take note of both values. The Azure multitenant\\napp name looks like <name>_<number>, but you only need to capture the <name> portion.\\n5. Open the consent URL from the previous step in a new browser tab, if you haven't done\\nthis previously. If you would like to proceed, consent to the required application\\npermissions, if prompted. You may be redirected to the main Snowflake website.\\n6. Back in Fabric, open your workspace and select Manage access, then Add people or\\ngroups. Grant the application used by your Snowflake external volume the permissions\\nneeded to read data from data items in your workspace.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"needed to read data from data items in your workspace.\\nCREATE OR REPLACE EXTERNAL VOLUME onelake_read_exvol\\nSTORAGE_LOCATIONS =\\n(\\n    (\\n        NAME = 'onelake_read_exvol'\\n        STORAGE_PROVIDER = 'AZURE'\\n        STORAGE_BASE_URL = 'azure://<path_to_data_item>/Tables/'\\n        AZURE_TENANT_ID = '<Tenant_ID>'\\n    )\\n)\\nALLOW_WRITES = false;\\n７ Note\\nSnowflake requires the URL scheme to be azure://, so be sure to change https://\\nto azure://.\\nReplace <path_to_data_item> with the path to your data item, such as\\nhttps://onelake.dfs.fabric.microsoft.com/83896315-c5ba-4777-8d1c-\\ne4ab3a7016bc/a95f62fa-2826-49f8-b561-a163ba537828.\\nDESC EXTERNAL VOLUME onelake_read_exvol;\\n\\uea80 Tip\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"7. Create the CATALOG INTEGRATION object in Snowflake, if you haven't done this previously.\\nThis is required by Snowflake to reference existing Iceberg tables in storage.\\nSQL\\n8. Back in Snowflake, create an Iceberg table referencing the latest metadata file for the\\nvirtualized Iceberg table in OneLake.\\nSQL\\nAfter running this statement, you now have a reference to your virtualized Iceberg table\\nthat you can now query using Snowflake.\\n9. Query your virtualized Iceberg table by running the following statement.\\nSQL\\nSee the troubleshooting and limitations and considerations sections of our documentation of\\nOneLake table format virtualization and conversion between Delta Lake and Apache Iceberg\\ntable formats.\\nYou may instead choose to grant permissions at the data item level, if you wish.\\nLearn more about OneLake data access.\\nCREATE CATALOG INTEGRATION onelake_catalog_integration\\nCATALOG_SOURCE = OBJECT_STORE\\nTABLE_FORMAT = ICEBERG\\nENABLED = TRUE;\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"CATALOG_SOURCE = OBJECT_STORE\\nTABLE_FORMAT = ICEBERG\\nENABLED = TRUE;\\nCREATE OR REPLACE ICEBERG TABLE MYDATABASE.PUBLIC.<TABLE_NAME>\\nEXTERNAL_VOLUME = 'onelake_read_exvol'\\nCATALOG = onelake_catalog_integration\\nMETADATA_FILE_PATH = '<metadata_file_path>';\\n７ Note\\nReplace <TABLE_NAME> with your table name, and <metadata_file_path> with your\\nIceberg table's metadata file path, such as dbo/MyTable/metadata/321.metadata.json.\\nSELECT TOP 10 * FROM MYDATABASE.PUBLIC.<TABLE_NAME>;\\nTroubleshooting\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Last updated on 07/01/2025'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake shortcut security\\nOneLake shortcuts serve as pointers to data residing in various storage accounts, whether\\nwithin OneLake itself or in external systems like Azure Data Lake Storage (ADLS). This article\\nlooks at the permissions required to create shortcuts and access data using them.\\nTo ensure clarity around the components of a shortcut this document uses the following terms:\\nTarget path: The location that a shortcut points to.\\nShortcut path: The location where the shortcut appears.\\nTo create a shortcut a user needs to have Write permission on the Fabric Item where the\\nshortcut is being created. In addition, the user needs Read access to the data the shortcut is\\npointing to. Shortcuts to external sources might require certain permissions in the external\\nsystem. The What are shortcuts? article has the full list of shortcut types and required\\npermissions.\\nCapability Permission on shortcut path Permission on target path\\nCreate a shortcut Write ReadAll\\nDelete a shortcut Write N/A'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Create a shortcut Write ReadAll\\nDelete a shortcut Write N/A\\n If OneLake security is enabled, the user needs to be in a role that grants access to the target\\npath.  If OneLake data access roles is enabled, the user needs to be in a role that grants access\\nto the target path.\\nA combination of the permissions in the shortcut path and the target path governs the\\npermissions for shortcuts. When a user accesses a shortcut, the most restrictive permission of\\nthe two locations is applied. Therefore, a user that has read/write permissions in the lakehouse\\nbut only read permissions in the target path can't write to the target path. Likewise, a user that\\nonly has read permissions in the lakehouse but read/write in the target path also can't write to\\nthe target path.\\nThis table shows the permissions needed for each shortcut action.\\nCreate and delete shortcuts\\nﾉ Expand table\\n2 1\\n2\\n1\\n2\\nAccessing shortcuts\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Capability Permission on\\nshortcut path\\nPermission on target\\npath\\nRead file/folder content of shortcut ReadAll ReadAll\\nWrite to shortcut target location Write Write\\nRead data from shortcuts in table section of the\\nlakehouse via TDS endpoint\\nRead ReadAll\\n If OneLake security is enabled the user needs to be in a role that grants access to the target\\npath.\\n Alternatively, OneLake security with ReadWrite permission on the shortcut path.\\nOneLake security (preview) is a feature that enables you to apply role-based access control\\n(RBAC) to your data stored in OneLake. You can define security roles that grant read access to\\nspecific tables and folders within a Fabric item, and assign them to users or groups. The access\\npermissions determine what users will across all engines in Fabric, ensuring consistent access\\ncontrol.\\nﾉ Expand table\\n1 1\\n2 2\\n3\\n1\\n2\\n） Important\\n Exception to identity passthrough: While OneLake security typically passes through the'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"） Important\\n Exception to identity passthrough: While OneLake security typically passes through the\\ncalling user's identity to enforce permissions, certain query engines operate differently.\\nWhen accessing shortcut data through Power BI semantic models using DirectLake over\\nSQL or T-SQL engines configured for Delegated identity mode, these engines don't pass\\nthrough the calling user's identity to the shortcut target. Instead, they use the item\\nowner's identity to access the data, and then apply OneLake security roles to filter what\\nthe calling user can see.\\nThis means:\\nThe shortcut target is accessed using the item owner's permissions (not the end\\nuser's)\\nOneLake security roles still determine what data the end user can read\\nAny permissions configured directly at the shortcut target path for the end user are\\nbypassed\\n3\\nOneLake security\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Users in the Admin, Member, and Contributor roles have full access to read data from a\\nshortcut regardless of the OneLake data access roles defined. However they still need access\\non both the shortcut path and target path as mentioned in Workspace roles.\\nUsers in the Viewer role or that had a lakehouse shared with them directly have access\\nrestricted based on if the user has access through a OneLake data access role. For more\\ninformation on the access control model with shortcuts, see Data Access Control Model in\\nOneLake.\\nUsers in Viewer roles can create shortcuts if they have ReadWrite permissions on the path\\nwhere the shortcut is created.\\nThe following table illustrates the necessary permissions to perform shortcut operations.\\nShortcut operation Permission on shortcut path Permission on target path\\nCreate Fabric Read and OneLake security\\nReadWrite\\nOneLake security Read\\nRead (GET/LIST\\nshortcuts)\\nFabric Read and OneLake security Read N/A\\nUpdate Fabric Read and OneLake security'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"shortcuts)\\nFabric Read and OneLake security Read N/A\\nUpdate Fabric Read and OneLake security\\nReadWrited\\nOneLake security Read (on the new\\ntarget)\\nDelete Fabric Read and OneLake security\\nReadWrite\\nN/A\\nShortcuts use two authentication models with OneLake security: passthrough and delegated.\\nIn the passthrough model, the shortcut accesses data in the target location by 'passing' the\\nuser’s identity to the target system. This ensures that any user accessing the shortcut is only\\nable to see whatever they have access to in the target.\\nWith OneLake to OneLake shortcuts, only passthrough mode is supported. This design ensures\\nthat the source system retains full control over its data. Organizations benefit from enhanced\\nsecurity because there’s no need to replicate or redefine access controls for the shortcut.\\nHowever, it’s important to understand that security for OneLake shortcuts can't be modified\\ndirectly from the downstream item. Any changes to access permissions must be made at the\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='directly from the downstream item. Any changes to access permissions must be made at the\\nsource location.\\nﾉ Expand table\\nShortcut auth models'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Delegated shortcuts access data by using some intermediate credential, such as another user\\nor an account key. These shortcuts allow for permission management to be separated or\\n'delegated' to another team or downstream user to manage. Delegated shortcuts always break\\nthe flow of security from one system to another. All delegated shortcuts in OneLake can have\\nOneLake security roles defined for them.\\nAll shortcuts from OneLake to external systems (multicloud shortcuts) like AWS S3 or Google\\nCloud Storage are delegated. This allows users to connect to the external system without being\\ngiven direct access. OneLake security can then be configured on the shortcut to limit what data\\nin the external system can be accessed\\nIn addition to OneLake security access to the target path, accessing external shortcuts via\\nSpark or direct API calls also require read permissions on the item containing the external\\nshortcut path.\\n\\uf80a\\n\\uf80a\\nOneLake security limitations\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='What are shortcuts?\\nCreate a OneLake shortcut\\nUse OneLake shortcuts REST APIs\\nData Access Control Model in OneLake.\\nLast updated on 11/18/2025\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Manage connections for shortcuts\\nArticle• 04/25/2025\\nShortcuts in OneLake use shared cloud connections to access the cloud resources where your\\ndata is stored. These connections can be managed on a per-shortcut basis, but you can also\\nview and update connections in bulk to keep all of your shortcuts working efficiently.\\nYou can view and manage all existing cloud connections for shortcuts in a single lakehouse.\\n1. In the Microsoft Fabric portal, navigate to your lakehouse.\\n2. Select Settings.\\n3. Select Shortcut connections.\\nView shortcut connections'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"4. On the Manage OneLake shortcut connections page, you can view all connections. The\\nAction required section highlights any broken connections that need attention. You can\\nalso see how many shortcuts share each connection.\\nThere are many reasons that you might have to replace a cloud connection. Maybe the\\nconnection is broken, maybe the user that created that connection left your organization and\\nyou can't access the connection anymore, or maybe you want to switch to a different\\nconnection that uses a different authentication method.\\n1. On the Manage OneLake shortcut connections page, select Replace for the connection\\nthat you want to update.\\n2. Select either Existing connection or Create new connection.\\n3. Provide the new connection information.\\nFor an existing connection, use the drop-down menu to select the connection, then\\nselect Save.\\nFor a new connection, provide the connection settings and credentials, then select\\nSave.\\nReplace shortcut connections\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Once the new cloud connection is established, all of the shortcuts that used the old connection\\nare updated to use the new connection.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Access Fabric OneLake shortcuts in an\\nApache Spark notebook\\nArticle• 06/05/2024\\nFor an overview of shortcuts, see OneLake shortcuts.\\nShortcuts appear as folders in OneLake, and Apache Spark can read from them just like\\nany other folder in OneLake.\\nTo access a shortcut as a folder:\\n1. From a lakehouse containing shortcuts, select Open notebook and then select\\nNew notebook.\\n2. Select a shortcut and right-click on a file from the shortcut.\\n3. In the right-click menu, select Load data and then select Spark.\\n4. Run the automatically generated code cell.\\nAccess shortcuts as folders in an Apache Spark\\nnotebook'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Microsoft Fabric automatically recognizes shortcuts in the Tables section of the\\nlakehouse that have data in the Delta\\\\Parquet format as tables. You can reference these\\ntables directly from a Spark notebook.\\nTo access a shortcut as a table:\\n1. From a Lakehouse containing shortcuts, select Open notebook and then select\\nNew notebook.\\n2. Select the Table view in the notebook.\\nAccess shortcuts as tables in a Spark notebook'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='3. Right-click on the table, then select Load data and Spark.\\n4. Run the automatically generated code cell.\\nYou can also access shortcuts through the Azure Blob Filesystem (ABFS) driver or REST\\nendpoint directly. Copy these paths from the lakehouse.\\n1. Open a Lakehouse containing shortcuts.\\n2. Right-click on a shortcut and select Properties.\\n\\uf80a\\nAccess the HTTPS and ABFS paths of a shortcut'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\n3. Select the copy icon next to the ABFS path or URL in the Properties screen.\\nOneLake access and APIs\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Assign variables to shortcuts (preview)\\n06/13/2025\\nFabric lifecycle management tools allow for the simple collaboration and continuous\\ndevelopment of analytical solutions across multiple environments like testing and production.\\nTo lean more about these tools and processes see: Introduction to CI/CD in Microsoft Fabric\\nWhen deploying solutions across environments, you may want to configure properties that are\\nunique to each environment so your testing environment points to test data and your\\nproduction environment points to production data. Workspace variables make this possible.\\nYou can use workspace variables within individual shortcut properties. This allows you to have\\nunique values for properties like connection ID or target location for each environment.\\nWorkspace variables and variable values sets can be defined within a variable library. See: Learn\\nhow to use Variable libraries .\\nOnce a variable is defined within a variable library, it can be assigned to a shortcut property'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Once a variable is defined within a variable library, it can be assigned to a shortcut property\\nusing the manage shortcuts UX.\\n1. Open a lakehouse and select an existing shortcut\\n2. Right click on the shortcut and choose Manage Shortcut\\n） Important\\nThis feature is in preview.\\nAssign a variable through the UX'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='3. Select Edit variables and choose the desired property to assign the variable to.\\n4. Assign a variable from the variable library\\n5. Once the variable is assigned, the variable name and variable value appear below the\\nshortcut property'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='７ Note\\nOnly variables of type string are supported. Selecting a variable of any other type results\\nin an error.\\n７ Note\\nAssignment of workspace variables through the shortcuts REST API is not currently\\nsupported.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake Shortcuts\\nService:Core\\nAPI Version:v1\\nCreate Shortcut Creates a new shortcut or updates an existing shortcut.\\nCreates Shortcuts In\\nBulk\\nCreates bulk shortcuts.\\nDelete Shortcut Deletes the shortcut but does not delete the destination storage folder.\\nGet Shortcut Returns shortcut properties.\\nList Shortcuts Returns a list of shortcuts for the item, including all the subfolders\\nexhaustively.\\nReset Shortcut CacheDeletes any cached files that were stored while reading from shortcuts.\\nOperations\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Connecting to Microsoft OneLake\\n09/19/2025\\nMicrosoft OneLake provides open access to all of your Fabric items through existing Azure\\nData Lake Storage (ADLS) and Blob APIs and SDKs. You can access your data in OneLake\\nthrough any API, SDK, or tool compatible with ADLS or Azure Blob Storage just by using a\\nOneLake URI instead. You can upload data to a lakehouse through Azure Storage Explorer, or\\nread a delta table through a shortcut from Azure Databricks.\\nAs OneLake is software as a service (SaaS), some operations, such as managing permissions or\\nupdating items, must be done through Fabric experiences, and can't be done via ADLS APIs.\\nFor a full list of changes to these APIs, see OneLake API parity.\\nBecause OneLake exists across your entire Microsoft Fabric tenant, you can refer to anything in\\nyour tenant by its workspace, item, and path:\\nHTTP\\nOneLake also supports referencing workspaces and items with globally unique identifiers\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"HTTP\\nOneLake also supports referencing workspaces and items with globally unique identifiers\\n(GUIDs). OneLake assigns GUIDs and GUIDs don't change, even if the workspace or item name\\nchanges. You can find the associated GUID for your workspace or item in the URL on the Fabric\\nportal. You must use GUIDs for both the workspace and the item, and don't need the item type.\\nHTTP\\nWhen adopting a tool for use over OneLake instead of ADLS, use the following mapping:\\nURI syntax\\nhttps://onelake.dfs.fabric.microsoft.com/<workspace>/<item>.\\n<itemtype>/<path>/<fileName>\\n７ Note\\nBecause you can reuse item names across multiple item types, you must specify the item\\ntype in the extension. For example, .lakehouse for a lakehouse and .warehouse for a\\nwarehouse.\\nhttps://onelake.dfs.fabric.microsoft.com/<workspaceGUID>/<itemGUID>/<path>/<fileNa\\nme>\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"The account name is always onelake.\\nThe container name is your workspace name.\\nThe data path starts at the item. For example: /mylakehouse.lakehouse/Files/.\\nOneLake also supports the Azure Blob Filesystem driver (ABFS) for more compatibility with\\nADLS and Azure Blob Storage. The ABFS driver uses its own scheme identifier abfs and a\\ndifferent URI format to address files and directories in ADLS accounts. To use this URI format\\nover OneLake, swap workspace for filesystem and include the item and item type.\\nHTTP\\nThe abfs driver URI doesn't allow special characters, such as spaces, in the workspace name. In\\nthese cases, you can reference workspaces and items with the globally unique identifiers\\n(GUIDs) as described earlier in this section.\\nYou can authenticate OneLake APIs using Microsoft Entra ID by passing through an\\nauthorization header. If a tool supports logging into your Azure account to enable token\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"authorization header. If a tool supports logging into your Azure account to enable token\\npassthrough, you can select any subscription. OneLake only requires your user token and\\ndoesn't care about your Azure subscription.\\nWhen calling OneLake via DFS APIs directly, you can authenticate with a bearer token for your\\nMicrosoft Entra account. To learn more about requesting and managing bearer tokens for your\\norganization, check out the Microsoft Authentication Library.\\nFor quick, ad-hoc testing of OneLake using direct API calls, here's a simple example using\\nPowerShell to sign in to your Azure account, retrieve a storage-scoped token, and copy it to\\nyour clipboard for easy use elsewhere. For more information about retrieving access tokens\\nusing PowerShell, see Get-AzAccessToken.\\nPowerShell\\nabfs[s]://<workspace>@onelake.dfs.fabric.microsoft.com/<item>.\\n<itemtype>/<path>/<fileName>\\nAuthorization\\n７ Note\\nOneLake only supports tokens in the Storage audience. In the following example, we set\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='７ Note\\nOneLake only supports tokens in the Storage audience. In the following example, we set\\nthe audience through the ResourceTypeName parameter.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"If you use the global endpoint ('https://onelake.dfs.fabric.microsoft.com`) to query data in a\\nregion different than your workspace's region, there's a possibility that data could leave your\\nregion during the endpoint resolution process. If you're concerned about data residency, using\\nthe correct regional endpoint for your workspace ensures your data stays within its current\\nregion and doesn't cross any regional boundaries. You can discover the correct regional\\nendpoint by checking the region of the capacity that the workspace is attached to.\\nOneLake regional endpoints all follow the same format: https://<region>-\\nonelake.dfs.fabric.microsoft.com. For example, a workspace attached to a capacity in the\\nWest US region would be accessible through the regional endpoint https://westus-\\nonelake.dfs.fabric.microsoft.com.\\nIf a tool or package compatible with ADLS isn't working over OneLake, the most common issue\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='If a tool or package compatible with ADLS isn\\'t working over OneLake, the most common issue\\nis URL validation. As OneLake uses a different endpoint (dfs.fabric.microsoft.com) than ADLS\\n(dfs.core.windows.net), some tools don\\'t recognize the OneLake endpoint and block it. Some\\ntools allow you to use custom endpoints (such as PowerShell). Otherwise, it\\'s often a simple fix\\nto add OneLake\\'s endpoint as a supported endpoint. If you find a URL validation issue or have\\nany other issues connecting to OneLake, let us know.\\nOneLake is accessible through the same APIs and SDKs as ADLS. To learn more about using\\nADLS APIs, please see the following pages:\\nADLS Gen2 API Reference\\nADLS Gen2 Filesystem SDKs\\n.NET\\nPython\\nJava\\nConnect-AzAccount\\n$testToken = Get-AzAccessToken -AsSecureString -ResourceTypeName Storage\\n# Retrieved token is of string type which you can validate with the \\n\"$testToken.Token.GetTypeCode()\" command.\\n$testToken.Token | Set-Clipboard\\nData residency\\nCommon issues\\nResources'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Create file\\nRequest PUT https://onelake.dfs.fabric.microsoft.com/{workspace}/{item}.\\n{itemtype}/Files/sample?resource=file\\nHeaders Authorization: Bearer <userAADToken>\\nResponseResponseCode: 201 Created\\nHeaders:\\nx-ms-version : 2021-06-08\\nx-ms-request-id : 272526c7-0995-4cc4-b04a-8ea3477bc67b\\nx-ms-content-crc64 : OAJ6r0dQWP0=\\nx-ms-request-server-encrypted : true\\nETag : 0x8DA58EE365\\nBody:\\nOneLake parity and integration\\nConnect to OneLake with Python\\nOneLake integration with Azure Synapse Analytics\\nSamples\\nﾉ Expand table\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake and Azure Data Lake Storage\\n(ADLS) API parity\\nArticle• 02/14/2025\\nOneLake supports the same APIs as Azure Data Lake Storage (ADLS) and Azure Blob\\nStorage, enabling users to read, write, and manage their data in OneLake with the tools\\nthey already use today. Because OneLake is a managed, logical data lake, some features\\nare managed differently than in Azure Storage, and not all behaviors are supported over\\nOneLake. This page details these differences, including OneLake managed folders, API\\ndifferences, and open source compatibility.\\nThe workspaces and data items in your Fabric tenant define the structure of OneLake.\\nManaging workspaces and items is done through Fabric experiences - OneLake doesn't\\nsupport creating, updating, or deleting workspaces or items through the ADLS APIs.\\nOneLake only allows HEAD calls at the workspace (container) level and tenant (account)\\nlevel, as you must make changes to the tenant and workspaces in the Fabric\\nadministration portal.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='level, as you must make changes to the tenant and workspaces in the Fabric\\nadministration portal.\\nOneLake also enforces a folder structure for Fabric items, protecting items and their\\nmanaged subfolders from creation, deletion, or renaming through ADLS and Blob APIs.\\nFabric-managed folders include the top-level folder in an item (for example,\\n/MyLakehouse.lakehouse) and the first level of folders within it (for example,\\n/MyLakehouse.lakehouse/Files and /MyLakehouse.lakehouse/Tables).\\nYou can perform CRUD operations on any folder or file created within these managed\\nfolders, and perform read-only operations on workspace and item folders.\\nEven in user-created files and folders, OneLake restricts some Fabric management\\noperations through ADLS APIs. You must use Fabric experiences to update permissions\\nor edit items and workspaces, and Fabric manages other options such as access tiers.\\nOneLake accepts almost all of the same headers as Storage, ignoring only some headers'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake accepts almost all of the same headers as Storage, ignoring only some headers\\nthat relate to unpermitted actions on OneLake. Since these headers don't alter the\\nbehavior of the entire call, OneLake ignores the banned headers, returns them in a new\\n'x-ms-rejected-headers' response header, and permits the rest of the call. For example,\\nManaged OneLake folders\\nUnsupported request headers and parameters\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake ignores the 'x-ms-owner' parameter in a PUT call since Fabric and OneLake\\ndon't have the same concept of owning users as Azure Storage.\\nOneLake rejects requests containing unallowed query parameters since query\\nparameters change the behavior of the entire call. For example, UPDATE calls with the\\n'setAccessControl' parameter are blocked since OneLake never supports setting access\\ncontrol via Azure Storage APIs.\\nOneLake doesn’t allow the following behaviors and their associated request headers and\\nURI parameters:\\nSet access control\\nURI Parameter:\\naction: setAccessControl (Request rejected)\\naction: setAccessControlRecursive (Request rejected)\\nRequest headers:\\nx-ms-owner (Header ignored)\\nx-ms-group (Header ignored)\\nx-ms-permissions (Header ignored)\\nx-ms-group (Header ignored)\\nx-ms-acls (Header ignored)\\nSet encryption scope\\nRequest headers:\\nx-ms-encryption-key (Header ignored)\\nx-ms-encryption-key (Header ignored)\\nx-ms-encryption-algorithm:AES256 (Header ignored)\\nSet access tier\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"x-ms-encryption-algorithm:AES256 (Header ignored)\\nSet access tier\\nRequest headers:\\nx-ms-access-tier (Header ignored)\\nSince OneLake uses a different permission model than ADLS, response headers related\\nto permissions are handled differently:\\n'x-ms-owner' and 'x-ms-group' always returns '$superuser' as OneLake doesn't\\nhave owning users or groups\\n'x-ms-permissions' always returns '---------' as OneLake doesn't have owning\\nusers, groups, or public access permissions\\n'x-ms-acl' returns the Fabric permissions for the calling user converted to a POSIX\\naccess control list (ACL), in the form 'rwx'\\nResponse header differences\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Since OneLake supports the same APIs as ADLS and Blob Storage, many open source\\nlibraries and packages compatible with ADLS and Blob Storage work seamlessly with\\nOneLake (for example, Azure Storage Explorer). Other libraries may require small\\nupdates to accommodate OneLake endpoints or other compatibility issues. The\\nfollowing libraries are confirmed to be compatible with OneLake due to recent changes.\\nThis list isn't exhaustive:\\nDelta-RS\\nRust Object Store\\nHTTP\\nHTTP\\nHTTP\\nHTTP\\nOpen Source Integration\\nExamples\\nList items within a workspace (ADLS)\\nGET https://onelake.dfs.fabric.microsoft.com/myWorkspace?\\nresource=filesystem&recursive=false\\nList items within a workspace (Blob)\\nGET  https://onelake.blob.fabric.microsoft.com/myWorkspace?\\nrestype=container&comp=list&delimiter=%2F\\nCreate a folder within a lakehouse (ADLS)\\nPUT \\nhttps://onelake.dfs.fabric.microsoft.com/myWorkspace/myLakehouse.Lakehouse/F\\niles/newFolder/?resource=directory\\nGet blob properties (Blob)\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nConnect to OneLake using Python\\nUse Azure Storage Explorer to manage OneLake\\nHEAD  \\nhttps://onelake.blob.fabric.microsoft.com/myWorkspace/myLakehouse.Lakehouse/\\nFiles/file.txt\\nRelated content\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Use Python to manage files and folders\\nin Microsoft OneLake\\nArticle• 11/21/2023\\nThis article shows how you can use the Azure Storage Python SDK to manage files and\\ndirectories in OneLake. This walkthrough covers the same content as Use Python to\\nmanage directories and files in ADLS Gen2 and highlights the differences when\\nconnecting to OneLake.\\nBefore starting your project, make sure you have the following prerequisites:\\nA workspace in your Fabric tenant with Contributor permissions.\\nA lakehouse in the workspace. Optionally, have data preloaded to read using\\nPython.\\nFrom your project directory, install packages for the Azure Data Lake Storage and Azure\\nIdentity client libraries. OneLake supports the same SDKs as Azure Data Lake Storage\\n(ADLS) Gen2 and supports Microsoft Entra ID authentication, which is provided by the\\nazure-identity package.\\nConsole\\nNext, add the necessary import statements to your code file:\\nPython\\nPrerequisites\\nSet up your project'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Python\\nPrerequisites\\nSet up your project\\npip install azure-storage-file-datalake azure-identity\\nimport os\\nfrom azure.storage.filedatalake import (\\n    DataLakeServiceClient,\\n    DataLakeDirectoryClient,\\n    FileSystemClient\\n)\\nfrom azure.identity import DefaultAzureCredential'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"The following example creates a service client connected to OneLake that you can use to\\ncreate filesystem clients for other operations. To authenticate to OneLake, this example\\nuses the DefaultAzureCredential to automatically detect credentials and obtain the\\ncorrect authentication token. Common methods of providing credentials for the Azure\\nSDK include using the 'az login' command in the Azure Command Line Interface or the\\n'Connect-AzAccount' cmdlet from Azure PowerShell.\\nPython\\nTo learn more about using DefaultAzureCredential to authorize access to data, see\\nOverview: Authenticate Python apps to Azure using the Azure SDK.\\nTo work with a directory in OneLake, create a filesystem client and directory client. You\\ncan use this directory client to perform various operations, including renaming, moving,\\nor listing paths (as seen in the following example). You can also create a directory client\\nwhen creating a directory, using the FileSystemClient.create_directory method.\\nPython\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='when creating a directory, using the FileSystemClient.create_directory method.\\nPython\\nAuthorize access to OneLake\\ndef get_service_client_token_credential(self, account_name) -> \\nDataLakeServiceClient:\\n    account_url = f\"https://{account_name}.dfs.fabric.microsoft.com\"\\n    token_credential = DefaultAzureCredential()\\n    service_client = DataLakeServiceClient(account_url, \\ncredential=token_credential)\\n    return service_client\\nWorking with directories\\ndef create_file_system_client(self, service_client, file_system_name: str) : \\nDataLakeServiceClient) -> FileSystemClient:\\n    file_system_client = service_client.get_file_system_client(file_system = \\nfile_system_name)\\n    return file_system_client\\ndef create_directory_client(self, file_system_client : FileSystemClient, \\npath: str) -> DataLakeDirectoryClient: directory_client \\n    directory_client = file_system_client.GetDirectoryClient(path)\\n    return directory_client'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='directory_client = file_system_client.GetDirectoryClient(path)\\n    return directory_client\\ndef list_directory_contents(self, file_system_client: FileSystemClient, \\ndirectory_name: str):'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You can upload content to a new or existing file by using the\\nDataLakeFileClient.upload_data method.\\nPython\\nThe following code sample lists the directory contents of any folder in OneLake.\\nPython\\n    paths = file_system_client.get_paths(path=directory_name)\\n    for path in paths:\\n        print(path.name + \\'\\\\n\\')\\nUpload a file\\ndef upload_file_to_directory(self, directory_client: \\nDataLakeDirectoryClient, local_path: str, file_name: str):\\n    file_client = directory_client.get_file_client(file_name)\\n    with open(file=os.path.join(local_path, file_name), mode=\"rb\") as data:\\n        file_client.upload_data(dataW, overwrite=True)\\nSample\\n#Install the correct packages first in the same folder as this file. \\n#pip install azure-storage-file-datalake azure-identity\\nfrom azure.storage.filedatalake import (\\n    DataLakeServiceClient,\\n    DataLakeDirectoryClient,\\n    FileSystemClient\\n)\\nfrom azure.identity import DefaultAzureCredential\\n# Set your account, workspace, and item path here'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='from azure.identity import DefaultAzureCredential\\n# Set your account, workspace, and item path here\\nACCOUNT_NAME = \"onelake\"\\nWORKSPACE_NAME = \"<myWorkspace>\"\\nDATA_PATH = \"<myLakehouse>.Lakehouse/Files/<path>\"\\ndef main():\\n    #Create a service client using the default Azure credential\\n    account_url = f\"https://{ACCOUNT_NAME}.dfs.fabric.microsoft.com\"\\n    token_credential = DefaultAzureCredential()\\n    service_client = DataLakeServiceClient(account_url, \\ncredential=token_credential)'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nTo run this sample, save the preceding code into a file listOneLakeDirectory.py and run\\nthe following command in the same directory. Remember to replace the workspace and\\npath with your own values in the example.\\nterminal\\nUse Python to manage ADLS Gen2\\nOneLake parity and integration\\nSync OneLake with your Windows File Explorer\\n    #Create a file system client for the workspace\\n    file_system_client = \\nservice_client.get_file_system_client(WORKSPACE_NAME)\\n    \\n    #List a directory within the filesystem\\n    paths = file_system_client.get_paths(path=DATA_PATH)\\n    for path in paths:\\n        print(path.name + \\'\\\\n\\')\\nif __name__ == \"__main__\":\\n    main()\\npython listOneLakeDirectory.py\\nLearn more\\n\\ue8e1 Yes \\ue8e0 No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Use Blob and ADLS APIs to mirror data into\\nOneLake\\n10/10/2025\\nIf your application uses Azure Data Lake Storage (ADLS) or Blob Storage APIs and needs to\\nconnect to OneLake, you can continue using the existing APIs.\\nWe demonstrate how Blob and ADLS APIs are used with OneLake through a real-world\\nmirroring example and share developer insights from OneLake. We explore when and why you\\nmight choose one API over another, and how to get the most out of each. All the patterns we\\ncover apply to Azure Storage storage as well.\\nIn this scenario, we cover:\\nWhat is open mirroring\\nHow to use the .NET Azure Blob Storage and Distributed File System (DFS) clients to write\\ndata into the open mirror landing zone.\\nHow to combine the Blob Storage and DFS clients for uploading data and managing\\nfolders in OneLake, especially when performance matters\\nHow to handle scenarios that crop up with block blobs when writing parquet data to blob\\nstorage from .NET'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"storage from .NET\\nHow to test everything locally using the Azurite emulator. (Yes—code you write against\\nOneLake works with the storage emulator too!)\\nIn this section, we demonstrate how to efficiently stream parquet data into OneLake,\\nparticularly the open mirroring landing zone. Open mirroring is a powerful way to bring data\\nfrom proprietary systems, where shortcuts shortcuts can't be used, into Microsoft Fabric. It\\nhandles the heavy lifting, converting raw data into Delta Lake format, managing upserts,\\ndelete vectors, optimize, vacuum, and more. All you need to do is upload your data into\\nthe landing zone, include a row marker, and mirroring takes it from there.\\nIt's common that teams write custom code to extract data from proprietary systems and output\\nit in an open format. While open mirroring ingests both CSV and Parquet into Delta tables, if\\nyou’re already writing code, you might as well go with Parquet, it’s more efficient to upload\\nand process.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='and process.\\nParquet is a storage file format designed for analytics. Delta, on the other hand, is a table\\nprotocol built on top of Parquet. It adds transactional guarantees, schema enforcement, and\\nsupport for updates and deletes. When you upload Parquet files to the open mirroring landing\\nStreaming parquet into OneLake with Blob APIs'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='zone, those files are ingested into Delta tables—bringing ACID semantics and query\\nperformance optimizations without requiring you to manage those complexities yourself. The\\nrow marker indicates how each record should be merged into the table, which enables the\\nmirroring process to know when to insert, update, or delete rows.\\nLet’s walk through a concrete (but fictional) example.\\nImagine you’ve got a .NET application that pulls UK house price data from an on-premises\\nInland Revenue system. (Just to be clear, this is a made-up scenario. The UK Inland Revenue\\nisn’t doing this to my knowledge, but the dataset is publicly available and makes for a good\\nexample.) This app runs monthly as an Azure Function, and to keep costs low, it needs to be\\nfast and avoid using local disk. So, the goal is to stream the data directly from the source into\\nthe open mirroring landing zone in Parquet format. The Price Paid dataset from the UK Inland'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='the open mirroring landing zone in Parquet format. The Price Paid dataset from the UK Inland\\nRevenue includes a RecordStatus  field, which must be mapped to the row marker required by\\nthe open mirroring format.\\nThis scenario aligns well with the Blob Storage API. It supports streaming writes, allowing you\\nto push data directly into OneLake without staging it locally. That makes it simple, efficient, and\\ncost-effective, especially for serverless workloads like Azure Functions. It’s also fast: the Blob\\nAPI supports parallel block uploads, which can significantly boost throughput when writing\\nlarge files—and isn’t supported when using the DFS endpoint.\\nFor this, we’re using the open source Parquet.NET library, a fully managed .NET assembly\\nthat makes it easy to write Parquet data on the fly. Also, working with an object store like Blob\\nStorage introduces a few nuances, especially around streaming and buffering, which gives us'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Storage introduces a few nuances, especially around streaming and buffering, which gives us\\nan opportunity to explore some nuances when working with blob storage.\\nThis approach keeps your Azure Function lightweight, fast, and cost-efficient, no local disk, no\\nstaging, just stream, serialize, and upload.\\nThe open mirroring landing zone acts like an inbox for your mirrored tables, add files then\\nFabric takes care of the ingestion. But behind that simplicity is a clear protocol your application\\nneeds to follow to ensure data is correctly discovered and processed.\\nEach mirrored table has a dedicated path in OneLake:\\n<workspace>/mirrored-database/Files/LandingZone/<table-name>/\\nOpen mirroring landing zone summary\\nFolder structure'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='This is where you’ll write both metadata and data files. You don’t need to explicitly create\\nfolders, just write blobs with the appropriate prefix, and the structure is inferred.\\nBefore writing any data, you must create a _metadata.json file in the table’s landing zone\\nfolder. This file defines the key columns used for upserts and deletes:\\nC#\\nThis metadata file tells Fabric how to uniquely identify rows. Without it, Fabric won’t ingest your\\ndata.\\nOnce the metadata is in place, you can start writing data. Files must be named sequentially\\nusing zero-padded numbers like 00000000000000000001.parquet,\\n00000000000000000002.parquet, etc. This ensures deterministic ordering and avoids collisions.\\nList APIs return blobs alphabetically, so our logic can quickly find the next sequence number by\\nprocessing the landing zone folder with a flat listing. Open mirroring moves processed files to\\nfolders prefixed with _, which are sorted below numeric values. Exiting the loop after seeing all'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='folders prefixed with _, which are sorted below numeric values. Exiting the loop after seeing all\\nparquet files improves performance when you’re using the Azure Storage List Blob API – which\\nwould enumerate blobs in sub folders as it matches the prefix. If you’re using the ADLS Path\\nList API, you can choose to perform a recursive list, which allows control over whether or not to\\nlist the contents of sub folders – this is a clear advantage of the hierarchical namespace.\\nThe logic to determine the next file name looks like this:\\nC#\\nStep 1: Declare the table keys\\npublic async Task CreateTableAsync(OpenMirroredTableId table, params string[] \\nkeyColumns)\\n{\\n    await using var metadataFile = await OpenWriteAsync(table, \"_metadata.json\");\\n    var json = new { keyColumns };\\n    await JsonSerializer.SerializeAsync(metadataFile, json);\\n}\\nStep 2: Create data files with the correct name\\npublic async Task<MirrorDataFile> CreateNextTableDataFileAsync(OpenMirroredTableId \\ntable)\\n{'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='public async Task<MirrorDataFile> CreateNextTableDataFileAsync(OpenMirroredTableId \\ntable)\\n{\\n    var (containerClient, path) = GetTableLocation(table);\\n    var listBlobs = containerClient.GetBlobsAsync(prefix: path);\\n    var tableFound = false;'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You might be wondering what the MirrorDataFile class is for, we’ll come back to that shortly\\nwhen we cover how to work reliably with block blobs.\\nThe actual write is handled by opening a stream to the blob:\\nC#\\n    BlobItem? lastDataFile = null;\\n    // Parquet files will be first in the folder because other files and folders \\nall start with an underscore.\\n    // So we can just take the last Parquet file to get our sequence number.\\n    await foreach (var blob in listBlobs)\\n    {\\n        tableFound = true;\\n        if (blob.Name.EndsWith(\".parquet\") && blob.Properties.ContentLength > 0)\\n        {\\n            lastDataFile = blob;\\n        }\\n        else\\n        {\\n            break;\\n        }\\n    }\\n    if (!tableFound)\\n    {\\n        throw new ArgumentException($\"Table not found.\", nameof(table));\\n    }\\n    long lastFileNumber = 0;\\n    \\n    if (lastDataFile is not null)\\n    {\\n        var dataFileName = Path.GetFileName(lastDataFile.Name);'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='{\\n        var dataFileName = Path.GetFileName(lastDataFile.Name);\\n        lastFileNumber = long.Parse(dataFileName.Split(\\'.\\')[0]);\\n    }\\n    \\n    var stream = await OpenWriteAsync(table, $\"{++lastFileNumber:D20}.parquet\");\\n    return new MirrorDataFile(stream)\\n    {\\n        FileSequenceNumber = lastFileNumber\\n    };\\n}\\nStep 3: Upload the contents of the file\\nprivate async Task<Stream> OpenWriteAsync(OpenMirroredTableId table, string \\nfileName)\\n{\\n    var (containerClient, path) = GetTableLocation(table);\\n    path += fileName;\\n    var blobClient = containerClient.GetBlobClient(path);'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='C#\\nThe business logic that reads Price Paid data from the Inland Revenue system converts each\\nrecord into Parquet format as it is streamed from the source. Each row is written directly into a\\nParquet file, which is simultaneously streamed byte-by-byte into Azure Storage using the\\nstream returned by CreateNextTableDataFileAsync. This approach avoids local staging and\\nsupports efficient, serverless ingestion.\\nHere’s how the code works:\\nC#\\n    var stream = await blobClient.OpenWriteAsync(true);\\n    return stream;\\n}\\n７ Note\\nMirrored databases support schemas, so the landing zone can contain an optional\\nschema folder to denote that the table is within a schema. Also, in OneLake, Fabric\\nworkspaces are mapped to storage Containers and ADLS Filesystems.\\npublic record OpenMirroredTableId(string WorkspaceName, string \\nMirroredDatabaseName, string TableName)\\n{\\n    public string? Schema { get; init; } = null;\\n    public string GetTablePath() => Schema == null'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='public string? Schema { get; init; } = null;\\n    public string GetTablePath() => Schema == null\\n            ? $\"{MirroredDatabaseName}/Files/LandingZone/{TableName}/\"\\n            : $\"\\n{MirroredDatabaseName}/Files/LandingZone/{Schema}.schema/{TableName}/\";\\n}\\nprivate (BlobContainerClient ContainerClient, string TablePath) \\nGetTableLocation(OpenMirroredTableId table)\\n{\\n    var containerClient = \\nblobServiceClient.GetBlobContainerClient(table.WorkspaceName);\\n    var path = table.GetTablePath();\\n    \\n    return (containerClient, path);\\n}\\npublic async Task SeedMirrorAsync(OpenMirroredTableId tableId, CancellationToken \\ncancellationToken = default)\\n{\\n    await openMirroringWriter.CreateTableAsync(tableId,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The WriteAsync method serializes the data into Parquet format, row group by row group:\\nC#\\nEach PricePaid record is transformed into a PricePaidMirroredDataFormat object, which\\nincludes the required __rowMarker__ field:\\nC#\\nPricePaidMirroredDataFormat.KeyColumns);\\n    var data = pricePaidDataReader.ReadCompleteData(cancellationToken);\\n    await using var mirrorDataFile = await \\nopenMirror.CreateNextTableDataFileAsync(tableId);\\n    await mirrorDataFile.WriteData(async stream => await data.WriteAsync(stream, \\nSettings.RowsPerRowGroup, cancellationToken));\\n}\\npublic static async Task WriteAsync(this IAsyncEnumerable<PricePaid> data, Stream \\nresultStream, int rowsPerRowGroup = 10000, CancellationToken cancellationToken = \\ndefault)\\n{\\n    await using var parquetWriter = await ParquetWriter.CreateAsync(\\n        PricePaidMirroredDataFormat.CreateSchema(), \\n        resultStream, \\n        cancellationToken: cancellationToken);\\n    await foreach (var chunk in data'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='cancellationToken: cancellationToken);\\n    await foreach (var chunk in data\\n        .Select(PricePaidMirroredDataFormat.Create)\\n        .ChunkAsync(rowsPerRowGroup, cancellationToken))\\n    {\\n        await ParquetSerializer.SerializeRowGroupAsync(parquetWriter, chunk, \\ncancellationToken);\\n    }\\n}\\npublic static PricePaidMirroredDataFormat Create(PricePaid pricePaid)\\n{\\n    var recordMarker = pricePaid.RecordStatus.Value switch\\n    {\\n        RecordStatus.Added => 0,\\n        RecordStatus.Changed => 1,\\n        RecordStatus.Deleted => 2,\\n        _ => throw new InvalidEnumArgumentException(\"Unexpected RecordStatus \\nvalue\")\\n    };\\n    return new PricePaidMirroredDataFormat\\n    {\\n        TransactionId = pricePaid.TransactionId,\\n        Price = pricePaid.Price,\\n        ...'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"This protocol is simple and deterministic. It avoids unnecessary API calls, works seamlessly with\\nboth batch and streaming pipelines, and integrates smoothly with serverless environments like\\nAzure Functions. Uploading a blob to a prefix automatically creates the parent folders and\\nremains compatible with the storage emulator—more on that in the testing section.\\nOnce open mirroring has successfully processed your data, it moves the original files into\\nspecial folders, _ProcessedFiles and _FilesReadyToDelete, and adds a\\n_FilesReadyToDelete.json file. While Fabric will automatically delete these files after seven\\ndays, that retention window can lead to significant storage costs if you're mirroring large\\nvolumes of data.\\nTo reduce costs, you can proactively delete these folders once you're confident the data has\\nbeen ingested. This is a great use case for the ADLS API, which supports atomic directory\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='been ingested. This is a great use case for the ADLS API, which supports atomic directory\\ndeletion—far more efficient than enumerating and deleting individual blobs and updating the\\n_FilesReadyToDelete.json file.\\nHere’s how to do it:\\nC#\\n        __rowMarker__ = recordMarker\\n    };\\n}\\nStep 4: Clean up after yourself\\npublic async Task CleanUpTableAsync(OpenMirroredTableId tableId)\\n{\\n    var (fileSystemClient, tablePath) = GetTableLocation(tableId);\\n    var foldersToDelete = new []\\n    {\\n        \"_ProcessedFiles\",\\n        \"_FilesReadyToDelete\"\\n    };\\n    await Parallel.ForEachAsync(foldersToDelete, async (path, _) =>\\n    {\\n        var fullPath = $\"{tablePath}{path}/\";\\n        var directoryClient = fileSystemClient.GetDirectoryClient(fullPath);\\n        if (await directoryClient.ExistsAsync())\\n        {\\n            await directoryClient.DeleteAsync();\\n        }\\n    });\\n}\\nprivate (DataLakeFileSystemClient FileSystemClient, string TablePath) \\nGetTableLocation(OpenMirroredTableId table)\\n{'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='I chose this example because it opens the door to talk about some nuances with block blobs.\\nThese aren\\'t OneLake specific, but because OneLake is built on Azure Storage, OneLake\\nexposes these nuances too.\\nOne such case: the .NET Storage SDK exposes an OpenWrite API that returns a Stream. Super\\nhandy. As shown in the example above, that stream fits nicely with the Parquet.NET APIs. It also\\nmakes testing a breeze—you can easily substitute the stream in unit tests without needing to\\nbuild extra abstractions just for testability.\\nC#\\n    var containerClient = \\ndataLakeServiceClient.GetFileSystemClient(table.WorkspaceName);\\n    var path = $\"\\n{table.MirroredDatabaseName}/Files/LandingZone/{table.TableName}/\";\\n    return (containerClient, path);\\n}\\nReliably working with block blobs\\n[Test]\\npublic async Task when_writing_price_paid_data_to_parquet()\\n{\\n    var row = new PricePaid\\n    {\\n        TransactionId = \"{34222872-B554-4D2B-E063-4704A8C07853}\",\\n        Price = 375000,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='{\\n        TransactionId = \"{34222872-B554-4D2B-E063-4704A8C07853}\",\\n        Price = 375000,\\n        DateOfTransfer = new DateTime(2004, 4, 27),\\n        Postcode = \"SW13 0NP\",\\n        PropertyType = PropertyType.Detached,\\n        IsNew = true,\\n        DurationType = DurationType.Freehold,\\n        PrimaryAddressableObjectName = \"10A\",\\n        SecondaryAddressableObjectName = string.Empty,\\n        Street = \"THE TERRACE\",\\n        Locality = string.Empty,\\n        TownCity = \"LONDON\",\\n        District = \"RICHMOND UPON THAMES\",\\n        County = \"GREATER LONDON\",\\n        CategoryType = CategoryType.AdditionalPricePaid,\\n        RecordStatus = RecordStatus.Added\\n    };\\n    using var memoryStream = new MemoryStream();\\n    await new[] { row }.ToAsyncEnumerable().WriteAsync(memoryStream);\\n    var readData = await \\nPricePaidMirroredDataFormat.Read(memoryStream).SingleAsync();\\n    Assert.Multiple(() =>'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"But here’s the catch: Blob Storage isn’t the same as a local file system.\\nTo understand why the Flush() call in Parquet.NET matters, we need to take a quick detour\\ninto how Parquet files are structured—and how that interacts with the Blob Storage block blob\\nAPI.\\nParquet is a columnar storage format designed for efficient analytics. A Parquet file is made up\\nof:\\nRow groups: These are the core building blocks. Each row group contains a chunk of\\nrows, organized by column. Row groups are written sequentially and independently.\\nColumn chunks: Within each row group, data is stored column-by-column.\\nMetadata: At the end of the file, Parquet writes a footer that includes schema and offset\\ninformation for fast reads. In Parquet.NET, each time a row group is completed, the library\\ncalls Flush() on the output stream. This is where things get interesting when you're\\nwriting to Blob Storage.\\nBlob Storage, and therefore OneLake, uses a block blob model, which works like this:\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Blob Storage, and therefore OneLake, uses a block blob model, which works like this:\\nYou upload blocks: Each block can be up to 4,000 MiB in size (default is 4 MiB). You can\\nupload blocks in parallel to maximize throughput—and the .NET Blob client does this for\\nyou automatically, which is great. This is one reason to use the Blob client (not the DFS\\nclient) for uploads. The DFS client doesn’t support parallel block uploads, which can be a\\nperformance bottleneck. That said, the DFS client can still read the resulting files just fine.\\nYou commit the blocks: Once all blocks are uploaded, you call CommitBlockList() to\\nfinalize the blob. You can upload up to 50,000 blocks per blob, which means—if you’re\\nusing 4,000-MiB blocks—you could theoretically write a single 190.7-TiB file. (Not that I’d\\n    {\\n        Assert.That(readData.TransactionId, Is.EqualTo(row.TransactionId));\\n        Assert.That(readData.Price, Is.EqualTo(row.Price));'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Assert.That(readData.Price, Is.EqualTo(row.Price));\\n        Assert.That(readData.DateOfTransfer, Is.EqualTo(row.DateOfTransfer));\\n        // more assertions\\n        Assert.That(readData.__rowMarker__, Is.EqualTo(0)); // RecordStatus.Added\\n    });\\n}\\nCalling flush – commits the blocks\\nParquet file basics\\nBlock blob model'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"recommend it.) If you’d like to learn more, read this article Understanding block blobs,\\nappend blobs, and page blobs.\\nHere's the catch\\nWhen Parquet.NET calls Flush() after each row group, and you're writing to a stream backed\\nby Blob Storage, that flush triggers a BlockList commit. As the file grows, each new row group\\ncauses the library to recommit all previously uploaded blocks.\\nLet’s say you’re writing a 1-GB file with 100MB row groups, using 4MB blocks. That gives you\\n250 blocks in total. On the first flush, you commit 25 blocks. On the second, 50. By the final\\nflush, you’re committing all 250 blocks. Add that up across all 10 row groups, and you’ve\\ncommitted a total of 1,375 blocks—even though the final file only needs one commit.\\nThis is inefficient, and it introduces two key problems:\\nTimeouts and retries. Large BlockLists can lead to timeouts. Remember, Blob Storage is\\nbuilt on HTTP. It’s reliable—but not perfect. A timeout might succeed on the server, but\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"built on HTTP. It’s reliable—but not perfect. A timeout might succeed on the server, but\\nyour client doesn’t know that, so it retries. That retry can result in a 409 Conflict. Doing\\nsomething expensive 250 times instead of once increases the chance of this happening.\\nFewer commits = fewer retries = fewer headaches.\\nPremature blob visibility. One of the nice things about the Blob API is that the blocks\\ndon't become visible until you explicitly commit the BlockList. This aligns well for\\nscenarios like Parquet, where the file isn’t valid until the footer metadata is written. It\\nmeans downstream processes won’t accidentally pick up a half-written file. But here’s the\\ntwist: because Parquet.NET calls Flush() after each row group, and that flush triggers a\\ncommit, the blob becomes visible before the file is complete. So even though the Blob\\nAPI is designed to help you avoid this problem, the way Parquet.NET works with a Stream\\nintroduces an issue—unless you take steps to prevent it.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"introduces an issue—unless you take steps to prevent it.\\nTo bridge the gap between Parquet.NET and the nuances of Blob Storage, don't implement the\\nFlush() call in the BlobFile.BlobStream implementation. That way, even though Parquet.NET\\ncalls Flush after each row group, the underlying stream doesn’t flush to storage.\\nThe storage clients (DFS and Blob) will create an empty file when calling OpenWrite. The open\\nmirroring processor logs an error when encountering a 0-byte file, which is possible if the\\nreplicator process interleaves with file creation. To avoid this write a file to another location and\\nmove it to the correct path, which the ADLS APIs supports through a rename operation, like so:\\nC#\\npublic async Task<BlobFile> CreateFileAsync(string filePath)\\n{\\n    if (filePath is null)\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Here’s another subtle but important behavior to be aware of: when you use the Azure Blob\\nstream, calling .Dispose() (or letting it be disposed implicitly) will commit all uploaded blocks.\\nThat’s usually what you want—but not always.\\nLet’s say your source system is streaming data using an IAsyncEnumerable, as it does in the\\nexample to illustrate the bug. If that source fails partway through, for example, the database\\nconnection times out or the network drops, you might only have a partially written Parquet file.\\nBut if the stream gets disposed (which it will, due to await using or a using block), those partial\\nblocks get committed.\\nTo avoid this, the example code explicitly commits once only when the entire operation\\ncompletes successfully. That way, if the producer fails mid-stream, you’re not left with a half-\\nwritten file.\\nThis is why the example returns a BlobFile object from CreateNextTableDataFileAsync, to'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='This is why the example returns a BlobFile object from CreateNextTableDataFileAsync, to\\nencapsulate the stream to prevent the zero-byte file, and issues described with Flush and\\nDispose committing partial parquet files.\\nC#\\n    {\\n        throw new ArgumentNullException(nameof(filePath), \"File path cannot be \\nnull.\");\\n    }\\n    var containerClient = \\nclient.blobServiceClient.GetBlobContainerClient(containerName);\\n    var temporaryPath = $\"_{filePath}.temp\"!;\\n    var blobClient = containerClient.GetBlobClient(Combine(temporaryPath));\\n    var blobStream = await blobClient.OpenWriteAsync(overwrite: true);\\n    return new BlobFile(blobStream, GetChildPath(temporaryPath), \\nCombine(filePath)!);\\n}\\nCalling dispose – commits the blocks and move the file\\npublic class BlobFile(Stream stream, IStoragePath temporaryFilePath, string \\nfinalFilePath) : IAsyncDisposable\\n{\\n    private readonly BlobStream stream = new(stream, temporaryFilePath, \\nfinalFilePath);'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='{\\n    private readonly BlobStream stream = new(stream, temporaryFilePath, \\nfinalFilePath);\\n    \\n    public async Task WriteData(Func<Stream, Task> writeOperation)\\n    {\\n        try\\n        {\\n            await writeOperation(stream);\\n        }'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='catch (Exception)\\n        {\\n            stream.Failed();\\n            throw;\\n        }\\n    }\\n    \\n    public async ValueTask DisposeAsync()\\n    {\\n        await stream.DisposeAsync();\\n    }\\n    \\n    private class BlobStream(Stream innerStream, IStoragePath temporaryFilePath, \\nstring finalFilePath) : Stream\\n    {\\n        private bool disposed = false;\\n        private bool success = true;\\n        public override bool CanRead => innerStream.CanRead;\\n        public override bool CanSeek => innerStream.CanSeek;\\n        public override bool CanWrite => innerStream.CanWrite;\\n        public override long Length => innerStream.Length;\\n        public override long Position\\n        {\\n            get => innerStream.Position;\\n            set => innerStream.Position = value;\\n        }\\n        public override void Flush()\\n        {\\n            // no-op\\n        }\\n        public override int Read(byte[] buffer, int offset, int count) => \\ninnerStream.Read(buffer, offset, count);'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='innerStream.Read(buffer, offset, count);\\n        public override long Seek(long offset, SeekOrigin origin) => \\ninnerStream.Seek(offset, origin);\\n        public override void SetLength(long value) => \\ninnerStream.SetLength(value);\\n        public override void Write(byte[] buffer, int offset, int count) => \\ninnerStream.Write(buffer, offset, count);\\n        \\n        public void Failed()\\n        {\\n            success = false;\\n        }\\n        public override async ValueTask DisposeAsync()\\n        {\\n            if (disposed)'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='One of the great things about OneLake is that it’s built on Azure Storage—which means you\\ncan test your integration code locally using the Azurite emulator. This makes it easy to write\\nreliable tests, that emulate the behavior of OneLake / Azure Storage, without needing a live\\nFabric environment or cloud resources.\\nAzurite emulates the Blob Storage API, which is exactly what OneLake exposes. That means the\\nsame code you use in production can run unchanged in your test suite. You can spin up Azurite\\nas a local process or container, point your BlobServiceClient at it, and go.\\nThis is especially useful for unit and integration tests. You can:\\nValidate that your _metadata.json is written correctly.\\nCheck that your file naming logic produces the expected sequence.\\nSimulate partial writes resulting from calling Flush and Dispose on failure. This allows\\ntesting of the nuances described above and that the implementation handles them\\nappropriately.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"testing of the nuances described above and that the implementation handles them\\nappropriately.\\nAssert that your Parquet serialization round-trips cleanly.\\nAzurite doesn't support the ADFS APIs. This is why BlobFile above uses an IStoragePath\\ninterface to implement ADFS functionality using the blob APIs so they can work with the\\nemulator under testing. Here’s an example:\\nC#\\n            {\\n                return;\\n            }\\n            if (success)\\n            {\\n                await innerStream.DisposeAsync();\\n                await temporaryFilePath.RenameAsync(finalFilePath);\\n            }\\n            disposed = true;\\n        }\\n    }\\n}\\nWriting tests with OneLake using the Azurite\\nemulator\\npublic async Task RenameAsync(string newPath)\\n{\\n    async Task RenameDirectoryAsync(DataLakeServiceClient dataLakeServiceClient)\\n    {\\n        var fileSystemClient =\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Earlier, we talked about how Parquet.NET’s use of Flush() and Dispose() can lead to\\npremature or partial blob commits when writing to OneLake. These behaviors are subtle—but\\ntestable.\\nHere are a few tests to validate that the mirroring logic handles these scenarios correctly:\\nC#\\ndataLakeServiceClient.GetFileSystemClient(containerName);\\n        var directoryClient = fileSystemClient.GetDirectoryClient(path);\\n        await directoryClient.RenameAsync(newPath);\\n    }\\n    async Task CopyThenDeleteAsync(BlobServiceClient blobServiceClient)\\n    {\\n        var sourceBlob = \\nblobServiceClient.GetBlobContainerClient(containerName).GetBlobClient(path);\\n        var destinationBlob = \\nblobServiceClient.GetBlobContainerClient(containerName).GetBlobClient(newPath);\\n        \\n        await destinationBlob.StartCopyFromUriAsync(sourceBlob.Uri);\\n        await sourceBlob.DeleteIfExistsAsync();\\n    }\\n    StorageOperation operation = new()\\n    {\\n        WithFlatNamespace = CopyThenDeleteAsync,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='}\\n    StorageOperation operation = new()\\n    {\\n        WithFlatNamespace = CopyThenDeleteAsync,\\n        WithHierarchicalNamespace = RenameDirectoryAsync\\n    };\\n    await operation.Execute(client);\\n}\\nTesting corner cases: flush and dispose\\n[Test]\\npublic async Task it_should_write_data_to_table()\\n{\\n    await setup.FabricPricePaidMirror.SeedMirrorAsync(setup.TableId);\\n    var mirroredData = await GetMirroredBlobItem();\\n    var mirroredDataClient = \\nsetup.WorkspaceContainer.GetBlobClient(mirroredData!.Name);\\n    var mirroredDataContents = await mirroredDataClient.DownloadContentAsync();\\n    var readData = await \\nPricePaidMirroredDataFormat.Read(mirroredDataContents.Value.Content.ToStream()).Si\\nngleAsync();\\n    Assert.Multiple(() =>\\n    {\\n        Assert.That(mirroredData, Is.Not.Null);'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='This test confirms that a successful write results in a valid, nonempty Parquet file that round-\\ntrips correctly.\\nNow for the failure cases:\\nC#\\nThis test simulates a mid-stream exception and verifies that no partial data is visible while the\\nwrite is in progress because of the no-op Flush() override.\\nAnd finally, the failure path:\\nC#\\n        Assert.That(mirroredData!.Properties.ContentLength, Is.GreaterThan(0));\\n        Assert.That(readData.TransactionId, \\nIs.EqualTo(setup.PricePaidReader.TransactionId));\\n        // ... more assertions ...\\n    });\\n}\\n[Test]\\npublic async Task it_should_not_commit_partially_written_data()\\n{\\n    long? lengthDuringWrite = null;\\n    setup.PricePaidReader.ActionBetweenRowGroups = async () =>\\n    {\\n        var mirroredData = await GetMirroredBlobItem();\\n        lengthDuringWrite = mirroredData!.Properties.ContentLength;\\n    };\\n    await setup.FabricPricePaidMirror.SeedMirrorAsync(setup.TableId);\\n    Assert.That(lengthDuringWrite, Is.EqualTo(0));\\n}\\n[Test]'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Assert.That(lengthDuringWrite, Is.EqualTo(0));\\n}\\n[Test]\\npublic async Task \\nafter_a_row_group_is_written_it_should_not_leave_a_partially_complete_blob()\\n{\\n    setup.PricePaidReader.ThrowsAfterFirstRowGroup = true;\\n    var previouslyMirroredFile = await GetMirroredBlobItem();\\n    var threw = true;\\n    try\\n    {\\n        await setup.FabricPricePaidMirror.SeedMirrorAsync(setup.TableId);\\n    }\\n    catch (Exception)\\n    {\\n        threw = true;'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"This test confirms that even if the stream is disposed due to an exception a new mirrored file\\nisn't partially written.\\nThese tests provide confidence that the mirroring logic is robust, even under failure conditions.\\nThey demonstrate how the emulator can be used to simulate real-world behavior without\\nneeding a full Fabric (or Azure) environment.\\nAnd just to prove compatibility, tweaking the test setup to point at a Fabric workspace, here’s a\\ntable full of UK house price data.\\nC#\\n    }\\n    var mirroredData = await GetMirroredBlobItem();\\n    var mirroredTemporaryData = await GetMirroredBlobTemporaryItem();\\n    Assert.Multiple(() =>\\n    {\\n        Assert.That(threw, Is.True);\\n        if (previouslyMirroredFile == null)\\n        { \\n            Assert.That(mirroredData, Is.Null);\\n        }\\n        else\\n        {\\n            Assert.That(mirroredData!.Name, \\nIs.EqualTo(previouslyMirroredFile.Name));\\n        }\\n        Assert.That(mirroredTemporaryData, Is.Not.Null);\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='}\\n        Assert.That(mirroredTemporaryData, Is.Not.Null);\\n        Assert.That(mirroredTemporaryData!.Properties.ContentLength, \\nIs.EqualTo(0));\\n    });\\n}\\npublic class when_using_fabric\\n{\\n    public class in_success_cases : when_copying_to_mirror_successfully\\n    {\\n        [SetUp]\\n        public void UseFabric() => setup = TestSetup.UsingFabric();\\n    }\\n    public class in_failure_cases : when_copying_to_mirror_fails\\n    {\\n        [SetUp]\\n        public void UseFabric() => setup = TestSetup.UsingFabric();\\n    }\\n}\\npublic static TestSetup UsingFabric()\\n{'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='If you’re building new pipelines on OneLake, especially for streaming or serverless workloads,\\nthe ADLS and Blob storage APIs work as expected. They\\'re fast, flexible, and work seamlessly\\nwith open mirroring. By following the landing zone protocol and handling block blob and file\\nsystem differences, you can build robust, testable integrations that work just as well in\\nproduction as they do in your local emulator. And best of all—you don’t need to rewrite your\\napp or fight the filesystem. Just stream, serialize, and upload to OneLake.\\n    var blobServiceClient = new BlobServiceClient(new \\nUri(\"https://onelake.blob.fabric.microsoft.com/\"), new DefaultAzureCredential());\\n    var pricePaidReader = new TestPricePaidReader();\\n    var tableId = new OpenMirroredTableId($\"TestWorkspace\", \\n\"HousePriceOpenMirror.MountedRelationalDatabase\", \"PricePaid\");\\n    var workspaceContainer = \\nblobServiceClient.GetBlobContainerClient(tableId.WorkspaceName);'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='var workspaceContainer = \\nblobServiceClient.GetBlobContainerClient(tableId.WorkspaceName);\\n    var fabricPricePaidMirror = new FabricPricePaidMirror(new \\nFabricOpenMirroringWriter(blobServiceClient), pricePaidReader)\\n    {\\n        Settings = new FabricPricePaidMirrorSettings { RowsPerRowGroup = 1 }\\n    };\\n    return new TestSetup\\n    {\\n        BlobServiceClient = blobServiceClient,\\n        PricePaidReader = pricePaidReader,\\n        TableId = tableId,\\n        WorkspaceContainer = workspaceContainer,\\n        FabricPricePaidMirror = fabricPricePaidMirror\\n    };\\n}\\n\\uf80a\\nWrapping up'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Integrate OneLake with Azure Synapse\\nAnalytics\\nArticle• 11/29/2023\\nAzure Synapse is a limitless analytics service that brings together enterprise data\\nwarehousing and Big Data analytics. This tutorial shows how to connect to OneLake\\nusing Azure Synapse Analytics.\\nFollow these steps to use Apache Spark to write sample data to OneLake from Azure\\nSynapse Analytics.\\n1. Open your Synapse workspace and create an Apache Spark pool with your\\npreferred parameters.\\n2. Create a new Apache Spark notebook.\\n3. Open the notebook, set the language to PySpark (Python), and connect it to your\\nnewly created Spark pool.\\n4. In a separate tab, navigate to your Microsoft Fabric lakehouse and find the top-\\nlevel Tables folder.\\n5. Right-click on the Tables folder and select Properties.\\nWrite data from Synapse using Apache Spark'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"6. Copy the ABFS path from the properties pane.\\n7. Back in the Azure Synapse notebook, in the first new code cell, provide the\\nlakehouse path. This lakehouse is where your data is written later. Run the cell.\\nPython\\n8. In a new code cell, load data from an Azure open dataset into a dataframe. This\\ndataset is the one you load into your lakehouse. Run the cell.\\nPython\\n# Replace the path below with the ABFS path to your lakehouse Tables \\nfolder. \\noneLakePath = \\n'abfss://WorkspaceName@onelake.dfs.fabric.microsoft.com/LakehouseName.l\\nakehouse/Tables'\\nyellowTaxiDf = \\nspark.read.parquet('wasbs://nyctlc@azureopendatastorage.blob.core.windo\\nws.net/yellow/puYear=2018/puMonth=2/*.parquet')\\ndisplay(yellowTaxiDf.limit(10))\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"9. In a new code cell, filter, transform, or prep your data. For this scenario, you can\\ntrim down your dataset for faster loading, join with other datasets, or filter down to\\nspecific results. Run the cell.\\nPython\\n10. In a new code cell, using your OneLake path, write your filtered dataframe to a new\\nDelta-Parquet table in your Fabric lakehouse. Run the cell.\\nPython\\n11. Finally, in a new code cell, test that your data was successfully written by reading\\nyour newly loaded file from OneLake. Run the cell.\\nPython\\nCongratulations. You can now read and write data in OneLake using Apache Spark in\\nAzure Synapse Analytics.\\nFollow these steps to use SQL serverless to read data from OneLake from Azure Synapse\\nAnalytics.\\n1. Open a Fabric lakehouse and identify a table that you'd like to query from Synapse.\\n2. Right-click on the table and select Properties.\\n3. Copy the ABFS path for the table.\\nfilteredTaxiDf = \\nyellowTaxiDf.where(yellowTaxiDf.tripDistance>2).where(yellowTaxiDf.pass\\nengerCount==1)\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='yellowTaxiDf.where(yellowTaxiDf.tripDistance>2).where(yellowTaxiDf.pass\\nengerCount==1)\\ndisplay(filteredTaxiDf.limit(10))\\nfilteredTaxiDf.write.format(\"delta\").mode(\"overwrite\").save(oneLakePath \\n+ \\'/Taxi/\\')\\nlakehouseRead = spark.read.format(\\'delta\\').load(oneLakePath + \\'/Taxi/\\')\\ndisplay(lakehouseRead.limit(10))\\nRead data from Synapse using SQL'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\n4. Open your Synapse workspace in Synapse Studio .\\n5. Create a new SQL script.\\n6. In the SQL query editor, enter the following query, replacing ABFS_PATH_HERE with\\nthe path you copied earlier.\\nSQL\\n7. Run the query to view the top 10 rows of your table.\\nCongratulations. You can now read data from OneLake using SQL serverless in Azure\\nSynapse Analytics.\\nIntegrate OneLake with Azure Storage Explorer\\nSELECT TOP 10 *\\nFROM OPENROWSET(\\nBULK 'ABFS_PATH_HERE',\\nFORMAT = 'delta') as rows;\\nRelated content\\n\\ue8e1 Yes \\ue8e0 No\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Integrate OneLake with Azure Storage\\nExplorer\\nArticle• 11/29/2023\\nThis article demonstrates OneLake integration with Azure Storage Explorer. Azure\\nStorage Explorer allows you to view and manage your cloud storage account’s contents.\\nYou can upload, download, or move files from one location to another.\\n1. Install the latest version of Azure Storage Explorer from the product webpage.\\n2. Check to ensure the version installed is 1.29.0 or higher. (Check the version by\\nselecting Help > About.)\\n3. Select the Open connect dialog icon.\\n4. Azure Storage Explorer requires you to sign in to connect to Azure resources.\\nSelect Subscription and follow the instructions to sign in.\\nConnect and use Azure Storage Explorer\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='5. Connect to OneLake by selecting the Open connect dialog icon again and select\\nADLS Gen2 container or directory.\\n6. Enter URL details of the workspace or item you would like to connect to, in this\\nformat: https://onelake.dfs.fabric.microsoft.com/{workspace-\\nName}/{itemName.itemType}/. You can find the workspace name and item name in\\nthe Properties pane of a file in the Microsoft Fabric portal.\\n\\uf80a \\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You can choose a Display name for convenience, then select Next.\\n7. Storage Explorer browses to the location of the OneLake you entered.\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='8. To view the contents, select the OneLake folder you connected.\\n9. Select Upload. In the Select files to upload dialog box, select the files that you\\nwant to upload.\\n\\uf80a \\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='10. To download, select the folders or files that you want to download and then select\\nDownload.\\n11. To copy data across locations, select the folders you want to copy and select Copy,\\nthen navigate to the destination location and select Paste.\\nIf a workspace name has capital letters, deletion of files or folders fails due to a\\nrestriction from the storage service. We recommend using your workspace name in\\nlowercase letters.\\n\\uf80a \\n\\uf80a \\nLimitations'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\nIntegrate OneLake with Azure Databricks\\nRelated content\\n\\ue8e1 Yes \\ue8e0 No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Integrate OneLake with Azure Databricks\\n10/10/2025\\nThis scenario shows how to connect to OneLake via Azure Databricks. After completing this\\ntutorial, you'll be able to read and write to a Microsoft Fabric lakehouse from your Azure\\nDatabricks workspace.\\nBefore you connect, you must have:\\nA Fabric workspace and lakehouse.\\nA premium Azure Databricks workspace. Only premium Azure Databricks workspaces\\nsupport Microsoft Entra credential passthrough, which you need for this scenario.\\n1. Open your Azure Databricks workspace and select Create > Cluster.\\n2. To authenticate to OneLake with your Microsoft Entra identity, you must enable Azure\\nData Lake Storage (ADLS) credential passthrough on your cluster in the Advanced\\nOptions.\\nPrerequisites\\nSet up your Databricks workspace\\n７  Note\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='3. Create the cluster with your preferred parameters. For more information on creating a\\nDatabricks cluster, see Configure clusters - Azure Databricks.\\n4. Open a notebook and connect it to your newly created cluster.\\n1. Navigate to your Fabric lakehouse and copy the Azure Blob Filesystem (ABFS) path to\\nyour lakehouse. You can find it in the Properties pane.\\n2. Save the path to your lakehouse in your Databricks notebook. This lakehouse is where\\nyou write your processed data later:\\nPython\\n3. Load data from a Databricks public dataset into a dataframe. You can also read a file from\\nelsewhere in Fabric or choose a file from another ADLS Gen2 account you already own.\\nPython\\n4. Filter, transform, or prep your data. For this scenario, you can trim down your dataset for\\nfaster loading, join with other datasets, or filter down to specific results.\\nPython\\nYou can also connect Databricks to OneLake using a service principal. For more'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Python\\nYou can also connect Databricks to OneLake using a service principal. For more\\ninformation about authenticating Azure Databricks using a service principal, see\\nManage service principals.\\nAuthor your notebook\\n７  Note\\nAzure Databricks only supports the Azure Blob Filesystem (ABFS) driver when\\nreading and writing to ADLS Gen2 and OneLake:\\nabfss://myWorkspace@onelake.dfs.fabric.microsoft.com/.\\noneLakePath = \\n\\'abfss://myWorkspace@onelake.dfs.fabric.microsoft.com/myLakehouse.lakehouse/F\\niles/\\'\\nyellowTaxiDF = spark.read.format(\"csv\").option(\"header\", \\n\"true\").option(\"inferSchema\", \"true\").load(\"/databricks-\\ndatasets/nyctaxi/tripdata/yellow/yellow_tripdata_2019-12.csv.gz\")'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='5. Write your filtered dataframe to your Fabric lakehouse using your OneLake path.\\nPython\\n6. Test that your data was successfully written by reading your newly loaded file.\\nPython\\nThis completes the setup and now you can now read and write data in Fabric using Azure\\nDatabricks.\\n[Databricks serverless compute]/azure/databricks/compute/serverless/) allows you to run\\nworkloads without provisioning a cluster. As per Databricks serverless limitations, to automate\\nthe configuration of Spark on serverless compute, Databricks doesn\\'t allow configuring Spark\\nproperties outside supported properties that are listed here.\\nIf you attempt to modify or set an unsupported Spark configuration in a notebook linked to\\nDatabricks serverless compute, the system returns a CONFIG_NOT_AVAILABLE error.\\nfilteredTaxiDF = \\nyellowTaxiDF.where(yellowTaxiDF.fare_amount<4).where(yellowTaxiDF.passenger_c\\nount==4)\\ndisplay(filteredTaxiDF)\\nfilteredTaxiDF.write.format(\"csv\").option(\"header\",'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='ount==4)\\ndisplay(filteredTaxiDF)\\nfilteredTaxiDF.write.format(\"csv\").option(\"header\", \\n\"true\").mode(\"overwrite\").csv(oneLakePath)\\nlakehouseRead = spark.read.format(\\'csv\\').option(\"header\", \\n\"true\").load(oneLakePath)\\ndisplay(lakehouseRead.limit(10))\\nConnecting to OneLake using Databricks serverless\\ncompute\\n７  Note\\nThis limitation isn\\'t unique to Azure Databricks. Databricks Serverless implementations on\\nAmazon Web Services (AWS)  and Google Cloud exhibit the same behavior.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake supports inbound connectivity from Databricks serverless compute. You can connect\\nto OneLake as provided you have successfully authenticated and there's network path between\\nDatabricks serverless compute and OneLake. With Databricks serverless, you must ensure that\\nyour code doesn't modify any unsupported Spark properties.\\nBefore you connect, you must have:\\nA Fabric workspace and lakehouse.\\nA premium Azure Databricks workspace.\\nA service principal with a minimum of Contributor workspace role assignment.\\nDatabase secrets or Azure Key Vault (AKV) to store and retrieve secrets. This example uses\\nDatabricks secrets.\\n1. Create a notebook in Databricks workspace and attach it to serverless compute.\\n2. Import Python modules - in this sample, you're using three modules:\\nmsal is Microsoft Authentication Library (MSAL) and it is designed to help\\ndevelopers integrate Microsoft identity platform authentication into their\\napplications.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='developers integrate Microsoft identity platform authentication into their\\napplications.\\nrequests module is used to make HTTP requests using Python.\\ndelta lake is used to read and write Delta Lake tables using Python.\\nPython\\nPrerequisites\\nAuthor your notebook'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='3. Declare variables for Microsoft Entra tenant including application ID. Use the tenant ID of\\nthe tenant where Microsoft Fabric is deployed.\\nPython\\n4. Declare Fabric workspace variables.\\nPython\\n5. Initialize client to acquire token.\\nPython\\nfrom msal import ConfidentialClientApplication\\nimport requests\\nfrom deltalake import DeltaTable\\n# Fetch from Databricks secrets.\\ntenant_id = dbutils.secrets.get(scope=\"<replace-scope-name>\",key=\"<replace \\nvalue with key value for tenant _id>\")\\nclient_id = dbutils.secrets.get(scope=\"<replace-scope-name>\",key=\"<replace \\nvalue with key value for client _id>\") \\nclient_secret = dbutils.secrets.get(scope=\"<replace-scope-name>\",key=\"\\n<replace value with key value for secret>\")\\nworkspace_id = \"<replace with workspace name>\"\\nlakehouse_id = \"<replace with lakehouse name>\"\\ntable_to_read = \"<name of lakehouse table to read>\"\\nstorage_account_name = workspace_id\\nonelake_uri = \\nf\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}.lake'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='onelake_uri = \\nf\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}.lake\\nhouse/Tables/{table_to_read}\"\\nauthority = f\"https://login.microsoftonline.com/{tenant_id}\"\\napp = ConfidentialClientApplication(\\n client_id,\\n authority=authority,\\n client_credential=client_secret\\n )\\n result = app.acquire_token_for_client(scopes=\\n[\"https://onelake.fabric.microsoft.com/.default\"])\\n if \"access_token\" in result:\\n   access_token = result[\"access_token\"]\\n   print(\"Access token acquired.\")\\n   token_val = result[\\'access_token\\']'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='6. Read a delta table from OneLake\\nPython\\nThis completes the setup and you can now read data from OneLake using Databricks a\\nnotebook attached to serverless compute.\\nIntegrate OneLake with Azure HDInsight\\ndt = DeltaTable(onelake_uri, storage_options={\"bearer_token\": f\"{token_val}\", \\n\"use_fabric_endpoint\": \"true\"})\\ndf = dt.to_pandas()\\nprint(df.head())\\n７  Note\\nThe service principal has Contributor workspace role assignment and you can use it\\nto write data back to OneLake.\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Integrate Databricks Unity Catalog with\\nOneLake\\nArticle• 04/09/2024\\nThis scenario shows how to integrate Unity Catalog external Delta tables to OneLake\\nusing shortcuts. After completing this tutorial, you’ll be able to automatically sync your\\nUnity Catalog external Delta tables to a Microsoft Fabric lakehouse.\\nBefore you connect, you must have:\\nA Fabric workspace.\\nA Fabric lakehouse in your workspace.\\nExternal Unity Catalog Delta tables created within your Azure Databricks\\nworkspace.\\nFirst, examine which storage locations in Azure Data Lake Storage Gen2 (ADLS Gen2)\\nyour Unity Catalog tables are using. This Cloud storage connection is used by OneLake\\nshortcuts. To create a Cloud connection to the appropriate Unity Catalog storage\\nlocation:\\n1. Create a Cloud storage connection used by your Unity Catalog tables. See how to\\nset up a ADLS Gen2 connection.\\n2. Once you create the connection, obtain the connection ID by selecting Settings'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='2. Once you create the connection, obtain the connection ID by selecting Settings \\n> Manage connections and gateways > Connections > Settings.\\nPrerequisites\\nSet up your Cloud storage connection'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Once the Cloud connection ID is obtained, integrate Unity Catalog tables to Fabric\\nlakehouse as follows:\\n７ Note\\nGranting users direct storage level access to external location storage in ADLS Gen2\\ndoes not honor any permissions granted or audits maintained by Unity Catalog.\\nDirect access will bypass auditing, lineage, and other security/monitoring features\\nof Unity Catalog including access control and permissions. You are responsible for\\nmanaging direct storage access through ADLS Gen2 and ensuring that users have\\nthe appropriate permissions granted via Fabric. Avoid all scenarios granting direct\\nstorage level write access for buckets storing Databricks managed tables.\\nModifying, deleting, or evolving any objects directly through storage which were\\noriginally managed by Unity Catalog can result in data corruption.\\nRun the notebook'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Import sync notebook to your Fabric workspace. This notebook exports all Unity\\nCatalog tables metadata from a given catalog and schemas in your metastore.\\n2. Configure the parameters in the first cell of the notebook to integrate Unity\\nCatalog tables. The Databricks API, authenticated through PAT token, is utilized for\\nexporting Unity Catalog tables. The following snippet is used to configure the\\nsource (Unity Catalog) and destination (OneLake) parameters. Ensure to replace\\nthem with your own values.\\nPython\\n3. Run all cells of the notebook to start synchronizing Unity Catalog Delta tables to\\nOneLake using shortcuts. Once notebook is completed, shortcuts to Unity Catalog\\nDelta tables are available in the lakehouse, SQL endpoint, and semantic model.\\nIf you want to execute the notebook at regular intervals to integrate Unity Catalog Delta\\ntables into OneLake without manual resync / rerun, you can either schedule the'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='tables into OneLake without manual resync / rerun, you can either schedule the\\nnotebook or utilize a notebook activity in a data pipeline within Fabric Data Factory.\\n# Databricks workspace\\ndbx_workspace = \"<databricks_workspace_url>\"\\ndbx_token = \"<pat_token>\"\\n# Unity Catalog\\ndbx_uc_catalog = \"catalog1\"\\ndbx_uc_schemas = \\'[\"schema1\", \"schema2\"]\\'\\n# Fabric\\nfab_workspace_id = \"<workspace_id>\"\\nfab_lakehouse_id = \"<lakehouse_id>\"\\nfab_shortcut_connection_id = \"<connection_id>\"\\n# If True, UC table renames and deletes will be considered\\nfab_consider_dbx_uc_table_changes = True\\nSchedule the notebook'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nIn the latter scenario, if you intend to pass parameters from the data pipeline, designate\\nthe first cell of the notebook as a toggle parameter cell and provide the appropriate\\nparameters in the pipeline.\\nFor production scenarios, we recommend using Databricks OAuth for\\nauthentication and Azure Key Vault to manage secrets. For instance, you can use\\nthe MSSparkUtils credentials utilities to access Key Vault secrets.\\nThe notebook works with Unity Catalog external Delta tables. If you’re using\\nmultiple Cloud storage locations for your Unity Catalog tables, i.e. more than one\\nADLS Gen2, the recommendation is to run the notebook separately by each Cloud\\nconnection.\\nUnity Catalog managed Delta tables, views, materialized views, streaming tables\\nand non-Delta tables are not supported.\\nChanges to Unity Catalog table schemas like add / delete columns are reflected\\nautomatically in the shortcuts. However, some updates like Unity Catalog table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='automatically in the shortcuts. However, some updates like Unity Catalog table\\nrename and deletion require a notebook resync / rerun. This is considered by\\nfab_consider_dbx_uc_table_changes parameter.\\nFor writing scenarios, using the same storage layer across different compute\\nengines can result in unintended consequences. Be sure to grasp the implications\\nwhen using different Spark compute engines and runtime versions.\\nIntegrate OneLake with Azure Databricks\\nOneLake shortcuts\\n\\uf80a\\nOther considerations\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Was this page helpful?\\nProvide product feedback | Ask the community\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Write Iceberg tables from Snowflake to\\nOneLake (Preview)\\nYou can configure a new or existing Snowflake database to automatically store Iceberg tables\\nin Microsoft OneLake. This feature creates a new data item in Microsoft Fabric, and Iceberg\\ntables that you create in Snowflake are stored there by default. With this capability, both\\nMicrosoft Fabric and Snowflake can work with a single copy of Iceberg data without duplication\\nor movement.\\nThis article shows you how to:\\nAllow connectivity between Snowflake and Fabric\\nConfigure a new Snowflake database to write Iceberg tables to OneLake by default\\n1. Because this feature is in a Preview state, you first need to enable this setting at the\\ntenant or capacity level.\\nYour tenant admin can enable the setting tenant-wide using the \"Enable Snowflake\\ndatabase item (preview)\" setting seen in the Admin portal.\\nAlternatively, your capacity admin can enable this delegated tenant setting in the\\ncapacity settings area.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='capacity settings area.\\n2. Select (or create) a Fabric workspace for the Snowflake database item.\\nTo keep things simple, use alphanumeric characters only for your workspace name.\\nIf workspace name has special characters, copy the workspace ID from the browser\\nURL seen when the workspace is open.\\nYou must have the Admin or Member role to proceed with this feature.\\n3. Find your Fabric tenant ID (a GUID) and your Snowflake account identifier.\\n） Important\\nThis feature is currently in Public Preview. Behavior may change before General\\nAvailability.\\nOverview\\nPrerequisites'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You can find your Fabric tenant ID by selecting your profile in the top-right corner of\\nthe Fabric UI and hovering over the tenant information.\\nYou can find your Snowflake account identifier in the lower-left corner of the\\nSnowflake UI and selecting Account details.\\n4. Identify the Snowflake warehouse that should be used when Fabric communicates with\\nSnowflake (for example, COMPUTE_WH).\\n5. Pick a strong password to assign to the new Snowflake user for Fabric to use when\\ncommunicating with Snowflake.\\nThis article assumes the connection to Snowflake will use a user account with a\\npassword. You may use the KeyPair authentication method alternatively.\\nIn your Snowflake account, log in with a user that has an administrative role.\\n1. Run the following SQL statements in Snowflake:\\nSQL\\n2. Go to Ingestion > Add data.\\n3. Select Microsoft OneLake.\\n4. Enter your Fabric tenant ID (a GUID).\\n5. If prompted, click Provide consent.\\nCreate least-privileged user and role in Snowflake'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"5. If prompted, click Provide consent.\\nCreate least-privileged user and role in Snowflake\\n-- Use a privileged role\\nUSE ROLE ACCOUNTADMIN;\\n-- Create least-privilege role (if not exists)\\nCREATE ROLE IF NOT EXISTS R_ICEBERG_METADATA;\\n-- Create service user (adjust LOGIN_NAME / PASSWORD as needed)\\nCREATE USER IF NOT EXISTS SVC_FABRIC_ICEBERG_METADATA\\n  TYPE = LEGACY_SERVICE\\n  LOGIN_NAME = 'SVC_FABRIC_ICEBERG_METADATA'\\n  DISPLAY_NAME = 'Service - Fabric Iceberg Metadata'\\n  PASSWORD = '<STRONG_PASSWORD_HERE>'\\n  MUST_CHANGE_PASSWORD = FALSE\\n  DEFAULT_ROLE = R_ICEBERG_METADATA;\\n-- Grant role to user\\nGRANT ROLE R_ICEBERG_METADATA TO USER SVC_FABRIC_ICEBERG_METADATA;\\n-- Allow role to use an existing warehouse (adjust COMPUTE_WH as needed)\\nGRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE R_ICEBERG_METADATA;\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='If prompted for permissions, review and accept (may require Entra admin). You may\\nclose the popup once complete.\\n6. Copy the multitenant app name displayed (used to write Iceberg tables to OneLake).\\n7. Keep this browser tab open.\\nIn a new browser tab:\\n1. Open the Fabric web UI.\\n2. Go to Settings (top right) > Manage connections and gateways.\\n3. Select + New.\\n4. Choose Cloud connection. Enter the following details:\\nConnection type: Snowflake\\nServer: https://<accountIDpart1>-<accountIDpart2>.snowflakecomputing.com\\nWarehouse: COMPUTE_WH (or your choice)\\nAuthentication method: Snowflake\\nUsername: SVC_FABRIC_ICEBERG_METADATA (unless customized)\\nPassword: Your chosen password\\n5. Create the connection. If it fails, recheck the information from the previous section.\\n6. Copy the connection ID (GUID) from the connection page.\\n7. Filter the list of connections to see your new connection.\\n8. On that connection, select Manage users.\\nPrepare connection in Fabric'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='9. Grant access to the Snowflake multitenant app, then select Share.\\n10. Navigate to your Fabric workspace.\\n11. Select Manage access, Add people or groups.\\n12. Paste the multitenant app name. Grant at least Contributor role to ensure the app can be\\nused by Snowflake to create the data item and write data.\\n13. Keep this browser tab open.\\n７ Note\\nIf you granted consent in the previous section, it may take a few minutes for the\\nmultitenant app to appear in your search.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Continue the setup flow in Snowflake:\\n1. Enter the Fabric workspace name (if alphanumeric) or workspace ID.\\n2. Enter the Fabric connection ID that you copied in the previous section.\\n3. Provide a new Snowflake database name (for example, SnowflakeFabricIcebergDB).\\nIf you choose to have the item name in Fabric differ, keep track of that item name.\\n4. Confirm that you see the message \"Fabric item and database successfully created.\"\\n5. Continue, and acknowledge the configuration when prompted.\\nIf you enter a custom external volume name, keep track of that name.\\n6. Select Create volume, then View database.\\nConfirm you see the new database.\\n7. Run the following SQL statements:\\nReplace SnowflakeFabricIcebergDB with your database name.\\nSQL\\nEstablish connectivity in Snowflake\\n-- Grant Iceberg metadata role permissions on the new database\\nBEGIN\\n  LET db STRING := \\'SnowflakeFabricIcebergDB\\';\\n  EXECUTE IMMEDIATE \\'GRANT USAGE ON DATABASE \\' || db || \\' TO ROLE \\nR_ICEBERG_METADATA\\';'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"EXECUTE IMMEDIATE 'GRANT USAGE ON DATABASE ' || db || ' TO ROLE \\nR_ICEBERG_METADATA';\\n  EXECUTE IMMEDIATE 'GRANT MONITOR ON DATABASE ' || db || ' TO ROLE \\nR_ICEBERG_METADATA';\\n  EXECUTE IMMEDIATE 'GRANT USAGE ON ALL SCHEMAS IN DATABASE ' || db || ' TO \\nROLE R_ICEBERG_METADATA';\\n  EXECUTE IMMEDIATE 'GRANT USAGE ON FUTURE SCHEMAS IN DATABASE ' || db || ' TO \\nROLE R_ICEBERG_METADATA';\\n  EXECUTE IMMEDIATE 'GRANT SELECT ON ALL ICEBERG TABLES IN DATABASE ' || db || \\n' TO ROLE R_ICEBERG_METADATA';\\n  EXECUTE IMMEDIATE 'GRANT SELECT ON FUTURE ICEBERG TABLES IN DATABASE ' || db \\n|| ' TO ROLE R_ICEBERG_METADATA';\\nEND;\\nGRANT USAGE ON EXTERNAL VOLUME SnowflakeFabricIcebergDB TO ROLE \\nR_ICEBERG_METADATA;\\nGRANT OWNERSHIP ON EXTERNAL VOLUME SnowflakeFabricIcebergDB TO ROLE \\nR_ICEBERG_METADATA COPY CURRENT GRANTS;\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"You're done with setup! Proceed to create an Iceberg table and read it in Fabric.\\nIn Snowflake:\\n1. Run the following SQL statements to create an Iceberg table and populate it with data:\\nSQL\\n2. Run the following SQL statement to read the Iceberg table in Snowflake:\\nSQL\\nIn the Fabric web UI:\\n1. Navigate to your workspace.\\n2. Locate the Snowflake database item. Refresh if needed.\\n3. Open it to see the Iceberg table from Part 4.\\n4. Select SQL analytics endpoint (top right) to query this table using SQL:\\nSQL\\nCreate an Iceberg table in Snowflake\\n-- Create a sample Iceberg table\\nCREATE ICEBERG TABLE SnowflakeFabricIcebergDB.PUBLIC.SampleIcebergTable (\\n  id   INT,\\n  name STRING\\n)\\nCATALOG = 'SNOWFLAKE';\\n-- Insert sample rows\\nINSERT INTO SnowflakeFabricIcebergDB.PUBLIC.SampleIcebergTable VALUES\\n  (1, 'Alpha'),\\n  (2, 'Beta'),\\n  (3, 'Gamma');\\n-- Display all rows in the Iceberg table\\nSELECT * FROM SnowflakeFabricIcebergDB.PUBLIC.SampleIcebergTable;\\nRead the Iceberg table in Fabric\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='SELECT * FROM SnowflakeFabricIcebergDB.PUBLIC.SampleIcebergTable;\\nRead the Iceberg table in Fabric\\n-- Display all rows in the Iceberg table\\nSELECT * FROM [SnowflakeFabricIcebergDB].[PUBLIC].[SampleIcebergTable];'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You should see the same data displayed in both Snowflake and Fabric.\\nThis data resides in OneLake, and both Fabric and Snowflake can work with the same copy of\\ndata, no data movement or duplication required!\\nLast updated on 11/18/2025'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Integrate OneLake with Azure HDInsight\\nArticle• 06/05/2024\\nAzure HDInsight is a managed cloud-based service for big data analytics that helps\\norganizations process large amounts data. This tutorial shows how to connect to\\nOneLake with a Jupyter notebook from an Azure HDInsight cluster.\\nTo connect to OneLake with a Jupyter notebook from an HDInsight cluster:\\n1. Create an HDInsight (HDI) Apache Spark cluster. Follow these instructions: Set up\\nclusters in HDInsight.\\na. While providing cluster information, remember your Cluster login Username\\nand Password, as you need them to access the cluster later.\\nb. Create a user assigned managed identity (UAMI): Create for Azure HDInsight -\\nUAMI and choose it as the identity in the Storage screen.\\n2. Give this UAMI access to the Fabric workspace that contains your items. For help\\ndeciding what role is best, see Workspace roles.\\nUsing Azure HDInsight\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='3. Navigate to your lakehouse and find the name for your workspace and lakehouse.\\nYou can find them in the URL of your lakehouse or the Properties pane for a file.\\n4. In the Azure portal, look for your cluster and select the notebook.\\n5. Enter the credential information you provided while creating the cluster.\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\n6. Create a new Apache Spark notebook.\\n7. Copy the workspace and lakehouse names into your notebook and build the\\nOneLake URL for your lakehouse. Now you can read any file from this file path.\\nPython\\n8. Try writing some data into the lakehouse.\\nPython\\n9. Test that your data was successfully written by checking your lakehouse or by\\nreading your newly loaded file.\\nYou can now read and write data in OneLake using your Jupyter notebook in an HDI\\nSpark cluster.\\nOneLake security\\n\\uf80a\\nfp = \\'abfss://\\' + \\'Workspace Name\\' + \\n\\'@onelake.dfs.fabric.microsoft.com/\\' + \\'Lakehouse Name\\' + \\'/Files/\\' \\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").load(fp + \\n\"test1.csv\") \\ndf.show()\\nwritecsvdf = df.write.format(\"csv\").save(fp + \"out.csv\") \\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Was this page helpful?\\nProvide product feedback | Ask the community\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Use OneLake files in Microsoft Foundry\\nUse Microsoft OneLake as a knowledge source for Microsoft Foundry. You can connect directly\\nand securely to OneLake from Foundry, index unstructured and semi structured files stored in\\nOneLake (including files that arrive through shortcuts), and then use that indexed content as a\\nknowledge source inside agents in Foundry.\\nWith this integration, you can ground your agents on the same enterprise data that already\\nlives in OneLake, instead of creating new copies of files in separate AI specific stores.\\nPermissions and governance are enforced through the same OneLake and Fabric controls that\\nyou use for analytics workloads.\\nA lakehouse in Fabric. If you don't have a lakehouse, follow the steps in Create a\\nlakehouse with OneLake.\\nFiles in the Files folder of the lakehouse.\\nA Foundry project. If you don't have one, follow the steps in Create a project.\\nAn Azure AI Search service at the Basic tier or higher. If you don't have one, follow the\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"An Azure AI Search service at the Basic tier or higher. If you don't have one, follow the\\nsteps in Create an Azure AI Search service.\\nThe search service must be in the same tenant as your Fabric workspace.\\nIn this article, you create an assign a managed identity for the search service. To create\\na managed identity, you must be an Owner or User Access Administrator roles. To\\nassign roles, you must be an Owner, User Access Administrator, Role-based Access\\nControl Administrator, or a member of a custom role with\\nMicrosoft.Authorization/roleAssignments/write permissions.\\nUse Azure AI Search to configure a OneLake files indexer to make your lakehouse data\\nsearchable as a knowledge source.\\nReview the prerequisites in Index data from OneLake files and shortcuts > Prerequisites.\\nThen, follow the steps for system managed identity in Index data from OneLake files and\\nshortcuts > Grant permissions.\\nPrerequisites\\nIndex data from OneLake files\\nCreate a OneLake connection in Foundry\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Sign in to Microsoft Foundry.\\nMake sure the New Foundry toggle is On. The steps in this article refer to Microsoft\\nFoundry (new).\\n2. Open the project that you want to work in.\\n3. Select Build from the navigation menu, then select Knowledge from the left pane.\\n4. Select your AI Search resource.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='5. Select Create a knowledge base.\\n6. Select Microsoft OneLake as the knowledge type. Select Connect.\\n7. Provide your Fabric workspace ID and lakehouse ID.\\nYou can retrieve both of these IDs from your lakehouse URL:\\nhttps://app.powerbi.com/groups/<WORKSPACE_ID>/lakehouses/<LAKEHOUSE_ID>.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='8. Select Create.\\n9. Select Save knowledge base.\\nLast updated on 11/18/2025'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Manage OneLake with PowerShell\\nMicrosoft OneLake integrates with the Azure PowerShell module for data reading, writing, and\\nmanagement.\\nConnect to OneLake from PowerShell by following these steps:\\n1. Install the Azure Storage PowerShell module.\\nPowerShell\\n2. Sign in to your Azure account.\\nPowerShell\\n3. Create the storage account context.\\nStorage account name is onelake.\\nSet -UseConnectedAccount to passthrough your Azure credentials.\\nSet -endpoint as fabric.microsoft.com.\\n4. Run the same commands used for Azure Data Lake Storage (ADLS) Gen2. For more\\ninformation about ADLS Gen2 and the Azure Storage PowerShell module, see Use\\nPowerShell to manage ADLS Gen2.\\nPowerShell\\nConnect to OneLake with Azure PowerShell\\nInstall-Module Az.Storage -Repository PSGallery -Force\\nConnect-AzAccount\\nExample: Get the size of an item or directory\\nInstall-Module Az.Storage -Repository PSGallery -Force\\nConnect-AzAccount\\n$ctx = New-AzStorageContext -StorageAccountName 'onelake' -UseConnectedAccount -\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Connect-AzAccount\\n$ctx = New-AzStorageContext -StorageAccountName 'onelake' -UseConnectedAccount -\\nendpoint 'fabric.microsoft.com' \\n# This example uses the workspace and item name. If the workspace name does not meet \\nAzure Storage naming criteria (no special characters), you can use GUIDs instead.\\n$workspaceName = 'myworkspace'\\n$itemPath = 'mylakehouse.lakehouse/Files'\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Integrate OneLake with Azure Synapse Analytics\\nLast updated on 11/18/2025\\n# Recursively get the length of all files within your lakehouse, sum, and convert to \\nGB.\\n$colitems = Get-AzDataLakeGen2ChildItem -Context $ctx -FileSystem $workspaceName -\\nPath $itemPath -Recurse -FetchProperty | Measure-Object -property Length -sum\\n\"Total file size: \" + ($colitems.sum / 1GB) + \" GB\"\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"AzCopy\\n08/28/2025\\nAzCopy is a powerful command-line utility designed to facilitate the transfer of data between\\nAzure Storage accounts. Because Microsoft OneLake supports the same APIs, SDKs, and tools\\nas Azure Storage, you can also use AzCopy to load data to and from OneLake. This article helps\\nyou use AzCopy with OneLake, from copying data between artifacts to uploading or\\ndownloading data.\\nAzCopy is optimized for data plane operations at scale and large scale data movement. When\\nyou copy data between storage accounts (including OneLake), data moves directly from\\nstorage server to storage server, minimizing performance bottlenecks. AzCopy is also easy-to-\\nuse and reliable, with built-in mechanisms to handle network interruptions and retries. With\\nAzCopy, it's easy to upload data to OneLake, or load data from existing sources directly into\\nyour items in Fabric!\\nTrusted workspace access lets you access firewall-enabled Azure Storage accounts securely by\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Trusted workspace access lets you access firewall-enabled Azure Storage accounts securely by\\nconfiguring a resource instance rule on an Azure Storage account. This rule lets your specific\\nFabric workspace access the storage account\\'s firewall from select Fabric experiences, like\\nshortcuts, pipelines, and AzCopy. By configuring trusted workspace access, AzCopy can copy\\ndata from a firewall-enabled Azure Storage account into OneLake without affecting the firewall\\nprotections. Learn more at trusted workspace access.\\nIf you\\'re new to AzCopy, you can learn how to download and get started with AzCopy at Get\\nstarted with AzCopy.\\nWhen you use AzCopy with OneLake, there\\'s a few key points to remember:\\n1. Add \"fabric.microsoft.com\" as a trusted domain using the--trusted-microsoft-suffixes\\nparameter.\\n2. Select the subscription of your source Azure Storage account when logging in with your\\nMicrosoft Entra ID, as OneLake only cares about the tenant.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Microsoft Entra ID, as OneLake only cares about the tenant.\\n3. Use double quotes when using AzCopy in the command prompt, and single quotes when\\nin PowerShell.\\nWhy use AzCopy and OneLake?\\nTrusted workspace access and AzCopy\\nGetting Started'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The samples in this article also assume that your Microsoft Entra ID has appropriate\\npermissions to access both the source and destinations.\\nFinally, you need at least one source and destination for your data movement - the samples in\\nthis page use two Fabric lakehouses and one ADLS account.\\nUse this sample to copy a file from a lakehouse in one workspace to a different workspace by\\nusing the azcopy copy command. Remember to authenticate first by running azcopy login\\nfirst.\\nSyntax\\nAzCopy\\nThe copy operation is synchronous so when the command returns, all files are copied.\\nA shared access signature (SAS) provides short-term, delegated access to Azure Storage and\\nOneLake, and is a great option to provide tools or users temporary access to storage for one-\\ntime upload or downloads. A SAS is also a great option if the Azure Storage account is in a\\ndifferent tenant than your OneLake, as Entra authorization will not work if the tenants are\\ndifferent.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='different.\\nThis sample uses a unique SAS token to authenticate to both Azure Storage and OneLake. To\\nlearn more about generating and using SAS tokens with Azure Storage and OneLake, check out\\nthe following pages:\\nHow to create a OneLake shared access signature (SAS)\\nGrant limited access to Azure storage resources using shared access signatures (SAS)\\nSample: Copying data between Fabric workspaces\\nazcopy copy \"https://onelake.dfs.fabric.microsoft.com/<source-workspace-\\nname>/<source-item-name>/Files/<source-file-path>\" \\n\"https://onelake.dfs.fabric.microsoft.com/<destination-workspace-\\nname>/<destination-item-name>/Files/<destination-file-path>\" --trusted-microsoft-\\nsuffixes \"fabric.microsoft.com\" \\nSample: Copying data from ADLS to OneLake with\\na shared access signatures (SAS)'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"AzCopy\\nSince OneLake is a managed data lake, some operations aren't supported with AzCopy. For\\nexample, you can't use AzCopy to move or copy entire items or workspaces. Instead, create the\\nnew item in your destination location using a Fabric experience (like the portal), and then use\\nAzCopy to move the contents of the existing item into the new item.\\nWhen attempting to perform operations directly between two Fabric tenants, you must use\\nexternal data sharing. This means you cannot currently use AzCopy to directly load data\\nbetween two Fabric tenants, as that results in a direct cross-tenant operation. Other methods\\nto load data, such as downloading the data locally or to a Spark cluster and then re-uploading\\nthe data to the new tenant, will function.\\nCopy blobs between Azure Storage accounts by using AzCopy\\n７  Note\\nWhen using a SAS token to authenticate to OneLake in AzCopy, you must set the '``-s2s-\\npreserve-access-tier' parameter to false.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='preserve-access-tier\\' parameter to false.\\nazcopy copy \"https://<account-name>.blob.core.windows.net/<source-container-\\nname>/<source-file-path>?<blob-sas-token>\" \\n\"https://onelake.dfs.fabric.microsoft.com/<destination-workspace-\\nname>/<destination-item-name>/Files/<destination-file-path>?<onelake-sas-token>\" -\\n-trusted-microsoft-suffixes \"fabric.microsoft.com\" --s2s-preserve-access-\\ntier=false\\nLimitations\\nCross-tenant operations\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Overview of OneLake table APIs\\n10/27/2025\\nOneLake offers a REST API endpoint for interacting with tables in Microsoft Fabric. This\\nendpoint can be used with clients and libraries that are compatible with the Iceberg REST\\nCatalog (IRC) API open standard or the Unity Catalog API open standard..\\nUsing these APIs is straightforward once you identify a few pieces of information and select\\nyour preferred Microsoft Entra ID authentication flow.\\nTo use these APIs, you first need to gather the following pieces of information:\\nYour Fabric tenant ID.\\nThe tenant ID is a GUID, and it can be found in the Profile card or the **Help, About\\nFabric menu in Fabric.\\nThe workspace and data item ID of the data item (such as a lakehouse) with a top-level\\nTables directory.\\nThese IDs are GUIDs. They can be found within the OneLake URL of any table in OneLake.\\nThey can alternatively be found within the URL seen in your browser when you have a\\ndata item open in Fabric.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='data item open in Fabric.\\nThe user or service principal identity in Microsoft Entra ID that has permissions to read\\ntables in your chosen data item.\\n1. Decide how you would like to authenticate with Microsoft Entra ID to obtain an access\\ntoken for your chosen Microsoft Entra identity.\\n） Important\\nThis feature is in preview.\\nPrerequisites\\nGathering basic information\\nPreparing for authentication'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You can check this guide to learn about the different ways to obtain an access token with\\nMicrosoft Entra ID. Microsoft offers convenient authentication libraries in several\\nlanguages.\\n2. If you are developing a new application that will either allow users to sign in or sign in as\\na standalone application, register your application with Microsoft Entra ID.\\n3. Grant API permission for the Azure Storage (https://storage.azure.com/) token audience,\\nto your Microsoft Entra ID application. Granting this permission ensures that your\\napplication can obtain tokens for use with the OneLake table endpoint.\\nLearn how to get started with the OneLake table API endpoint to interact with Iceberg tables in\\nOneLake. Initially, read-only metadata table operations are supported, and we plan to add\\nmore operations soon.\\nLearn how to get started with the OneLake table API endpoint to interact with Delta tables in\\nOneLake.\\n７ Note\\nThe OneLake table API endpoint accepts the same token audience as the OneLake'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake.\\n７ Note\\nThe OneLake table API endpoint accepts the same token audience as the OneLake\\nfilesystem endpoints.\\nIf you are developing an application, you might already know how to authenticate\\nwith Microsoft Entra ID to interact with OneLake filesystem REST APIs. If so, you can\\nuse the same approach to authenticate with the new OneLake table endpoint.\\nIceberg REST Catalog (IRC) API operations on\\nOneLake\\n７ Note\\nBefore using the Iceberg APIs, be sure you have Delta Lake to Iceberg metadata\\nconversion enabled for your tenant or workspace. Review the instructions to learn how to\\nenable automatic Delta Lake to Iceberg table format conversion.\\nDelta Lake REST API operations on OneLake\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Learn more about OneLake table APIs for Iceberg.\\nLearn more about OneLake table APIs for Delta.\\nSet up automatic Delta Lake to Iceberg format conversion.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake table APIs for Iceberg\\n10/27/2025\\nOneLake offers a REST API endpoint for interacting with tables in Microsoft Fabric. This article\\ndescribes how to get started using this endpoint to interact with Apache Iceberg REST Catalog\\n(IRC) APIs available at this endpoint for metadata read operations.\\nFor overall OneLake table API guidance and prerequisite guidance, see the OneLake table API\\noverview.\\nFor detailed API documentation, see the Getting started guide.\\nThe OneLake table API endpoint is:\\nAt the OneLake table API endpoint, the Iceberg REST Catalog (IRC) APIs are available under the\\nfollowing <BaseUrl>. You can generally provide this path when initializing existing IRC clients or\\nlibraries.\\nExamples of IRC client configuration with the OneLake table endpoint are covered in the\\nGetting started guide.\\n） Important\\nThis feature is in preview.\\nIceberg table API endpoint\\nhttps://onelake.table.fabric.microsoft.com\\nhttps://onelake.table.fabric.microsoft.com/iceberg\\n７ Note'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='https://onelake.table.fabric.microsoft.com/iceberg\\n７ Note\\nBefore using the Iceberg APIs, be sure you have Delta Lake to Iceberg metadata\\nconversion enabled for your tenant or workspace. See the instructions to learn how to\\nenable automatic Delta Lake to Iceberg metadata conversion.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"The following IRC operations are currently supported at this endpoint. Detailed guidance for\\nthese operations is available in the Getting started guide.\\nGet configuration\\nGET <BaseUrl>/v1/config?warehouse=<Warehouse>\\nThis operation accepts the workspace ID and data item ID (or their equivalent friendly\\nnames if they don’t contain any special characters). <Warehouse> is typically\\n<WorkspaceID>/<dataItemID>.\\nThis operation returns the Prefix string that is used in subsequent requests.\\nList namespaces\\nGET <BaseUrl>/v1/<Prefix>/namespaces\\nThis operation returns the list of schemas within a data item. If the data item doesn't\\nsupport schemas, a fixed schema named dbo is returned.\\nGet namespace\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>\\nThis operation returns information about a schema within a data item, if the schema is\\nfound. If the data item doesn't support schemas, a fixed schema named dbo is supported\\nhere.\\nList tables\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>/tables\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='here.\\nList tables\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>/tables\\nThis operation returns the list of tables found within a given schema.\\nGet table\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>/tables/<TableName>\\nThis operation returns metadata details for a table within a schema, if the table is found.\\nIceberg table API operations\\nCurrent limitations, considerations'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"The use of the OneLake table APIs for Iceberg is subject to the following limitations and\\nconsiderations:\\nCertain data items may not support schemas\\nDepending on the type of data item you use, such as non-schema-enabled Fabric\\nlakehouses, there may not be schemas within the Tables directory. In such cases, for\\ncompatibility with API clients, the OneLake table APIs provide a default, fixed dbo schema\\n(or namespace) to contain all tables within a data item.\\nCurrent namespace scope\\nIn Fabric, data items contain a flat list of schemas, which each contains a flat list of tables.\\nToday, the top-level namespaces listed by the Iceberg APIs are schemas, so although the\\nIceberg REST Catalog (IRC) standard supports multi-level namespaces, the OneLake\\nimplementation offers one level, mapping to schemas.\\nBecause of this limitation, we don't yet support the parent query parameter for the list\\nnamespaces operation.\\nMetadata write operations, other operations\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"namespaces operation.\\nMetadata write operations, other operations\\nOnly the operations listed in Iceberg table API operations are supported today.\\nOperations that handle metadata write operations aren't yet supported by the OneLake\\ntable API endpoint. We plan to add support for more operations at a later time.\\nLearn more about OneLake table APIs.\\nSee detailed guidance and API details.\\nSet up automatic Delta Lake to Iceberg format conversion.\\nRelated content\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Getting started with OneLake table APIs for\\nIceberg\\n10/27/2025\\nOneLake offers a REST API endpoint for interacting with tables in Microsoft Fabric. This\\nendpoint supports read-only metadata operations for Apache Iceberg tables in Fabric. These\\noperations are compatible with the Iceberg REST Catalog (IRC) API open standard.\\nLearn more about OneLake table APIs for Iceberg and make sure to review the prerequisite\\ninformation.\\nReview these samples to learn how to set up existing Iceberg REST Catalog (IRC) clients or\\nlibraries for use with the new OneLake table endpoint.\\nUse the following sample Python code to configure PyIceberg to use the OneLake table API\\nendpoint. Then, list schemas and tables within a data item.\\nThis code assumes there is a default AzureCredential available for a currently signed-in user.\\nAlternatively, you can use the MSAL Python library to obtain a token.\\nPython\\n）  Important\\nThis feature is in preview.\\nPrerequisites\\nClient quickstart examples\\nPyIceberg'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Python\\n）  Important\\nThis feature is in preview.\\nPrerequisites\\nClient quickstart examples\\nPyIceberg\\nfrom pyiceberg.catalog import load_catalog\\nfrom azure.identity import DefaultAzureCredential\\n# Iceberg base URL at the OneLake table API endpoint\\ntable_api_url = \"https://onelake.table.fabric.microsoft.com/iceberg\"\\n# Entra ID token\\ncredential = DefaultAzureCredential()\\ntoken = credential.get_token(\"https://storage.azure.com/.default\").token'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Use the following sample code to create a new catalog-linked database in Snowflake. This\\ndatabase will automatically include any schemas and tables found within the connected Fabric\\ndata item. This involves the creation of a catalog integration, an external volume, and a\\ndatabase.\\nSQL\\n# Client configuration options\\nfabric_workspace_id = \"12345678-abcd-4fbd-9e50-3937d8eb1915\"\\nfabric_data_item_id = \"98765432-dcba-4209-8ac2-0821c7f8bd91\"\\nwarehouse = f\"{fabric_workspace_id}/{fabric_data_item_id}\"\\naccount_name = \"onelake\"\\naccount_host = f\"{account_name}.blob.fabric.microsoft.com\"\\n# Configure the catalog object for a specific data item\\ncatalog = load_catalog(\"onelake_catalog\", **{\\n    \"uri\": table_api_url,\\n    \"token\": token,\\n    \"warehouse\": warehouse,\\n    \"adls.account-name\": account_name,\\n    \"adls.account-host\": account_host,\\n    \"adls.credential\": credential,\\n})\\n# List schemas and tables within a data item\\nschemas = catalog.list_namespaces()\\nprint(schemas)\\nfor schema in schemas:'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"schemas = catalog.list_namespaces()\\nprint(schemas)\\nfor schema in schemas:\\n    tables = catalog.list_tables(schema)\\n    print(tables)\\nSnowflake\\n-- Create catalog integration object\\nCREATE OR REPLACE CATALOG INTEGRATION IRC_CATINT\\n    CATALOG_SOURCE = ICEBERG_REST\\n    TABLE_FORMAT = ICEBERG\\n    REST_CONFIG = (\\n        CATALOG_URI = 'https://onelake.table.fabric.microsoft.com/iceberg' -- \\nIceberg base URL at the OneLake table endpoint\\n        CATALOG_NAME = '12345678-abcd-4fbd-9e50-3937d8eb1915/98765432-dcba-4209-\\n8ac2-0821c7f8bd91' -- Fabric data item scope, in the form `workspaceID/dataItemID`\\n    )\\n    REST_AUTHENTICATION = (\\n        TYPE = OAUTH -- Entra auth\\n        OAUTH_TOKEN_URI = 'https://login.microsoftonline.com/11122233-1122-4138-\\n8485-a47dc5d60435/oauth2/v2.0/token' -- Entra tenant ID\\n        OAUTH_CLIENT_ID = '44332211-aabb-4d12-aef5-de09732c24b1' -- Entra \\napplication client ID\\n        OAUTH_CLIENT_SECRET = '[secret]' -- Entra application client secret value\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"The response of DESC EXTERNAL VOLUME will return metadata about the external volume,\\nincluding:\\nAZURE_CONSENT_URL, which is the permissions request page that needs to be followed if it\\nhasn’t yet been done for your tenant.\\nAZURE_MULTI_TENANT_APP_NAME, which is the name of the Snowflake client application that\\nneeds access to the data item. Make sure to grant it access to the data item in order for\\nSnowflake to be able to read table contents.\\nSQL\\n        OAUTH_ALLOWED_SCOPES = ('https://storage.azure.com/.default') -- Storage \\ntoken audience\\n    )\\n    ENABLED = TRUE\\n;\\n-- Create external volume object\\nCREATE OR REPLACE EXTERNAL VOLUME IRC_EXVOL\\n    STORAGE_LOCATIONS =\\n    (\\n        (\\n            NAME = 'IRC_EXVOL'\\n            STORAGE_PROVIDER = 'AZURE'\\n            STORAGE_BASE_URL = 'azure://onelake.dfs.fabric.microsoft.com/12345678-\\nabcd-4fbd-9e50-3937d8eb1915/98765432-dcba-4209-8ac2-0821c7f8bd91'\\n            AZURE_TENANT_ID='81ba0d80-2361-40bb-9c1f-bf1c84027025' --tenant id\\n        )\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='AZURE_TENANT_ID=\\'81ba0d80-2361-40bb-9c1f-bf1c84027025\\' --tenant id\\n        )\\n    )\\n    ALLOW_WRITES = FALSE;\\n;\\n-- Describe the external volume\\nDESC EXTERNAL VOLUME IRC_EXVOL;\\n-- Create a Snowflake catalog linked database\\nCREATE OR REPLACE DATABASE IRC_CATALOG_LINKED\\n    LINKED_CATALOG = (\\n        CATALOG = \\'IRC_CATINT\\'\\n    )\\n    EXTERNAL_VOLUME = \\'IRC_EXVOL\\'\\n;\\nSELECT SYSTEM$CATALOG_LINK_STATUS(\\'IRC_CATALOG_LINKED\\');\\nSELECT * FROM IRC_CATALOG_LINKED.\"dbo\".\"sentiment\";\\nDuckDB'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Use the following sample Python code to configure DuckDB to list schemas and tables within\\na data item.\\nThis code assumes there is a default AzureCredential available for a currently signed-in user.\\nAlternatively, you can use the MSAL Python library to obtain a token.\\nPython\\nimport duckdb\\nfrom azure.identity import DefaultAzureCredential\\n# Iceberg API base URL at the OneLake table API endpoint\\ntable_api_url = \"https://onelake.table.fabric.microsoft.com/iceberg\"\\n# Entra ID token\\ncredential = DefaultAzureCredential()\\ntoken = credential.get_token(\"https://storage.azure.com/.default\").token\\n# Client configuration options\\nfabric_workspace_id = \"12345678-abcd-4fbd-9e50-3937d8eb1915\"\\nfabric_data_item_id = \"98765432-dcba-4209-8ac2-0821c7f8bd91\"\\nwarehouse = f\"{fabric_workspace_id}/{fabric_data_item_id}\"\\n# Connect to DuckDB\\ncon = duckdb.connect()\\n# Install & load extensions\\ncon.execute(\"INSTALL iceberg; LOAD iceberg;\")\\ncon.execute(\"INSTALL azure; LOAD azure;\")'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='con.execute(\"INSTALL iceberg; LOAD iceberg;\")\\ncon.execute(\"INSTALL azure; LOAD azure;\")\\ncon.execute(\"INSTALL httpfs; LOAD httpfs;\")\\n# --- Auth & Catalog ---\\n# 1) Secret for the Iceberg REST Catalog (use existing bearer token)\\ncon.execute(\"\"\"\\nCREATE OR REPLACE SECRET onelake_catalog (\\nTYPE ICEBERG,\\nTOKEN ?\\n);\\n\"\"\", [token])\\n# 2) Secret for ADLS Gen2 / OneLake filesystem access via Azure extension\\n#    (access token audience must be https://storage.azure.com; account name is \\n\\'onelake\\')\\ncon.execute(\"\"\"\\nCREATE OR REPLACE SECRET onelake_storage (\\nTYPE AZURE,\\nPROVIDER ACCESS_TOKEN,\\nACCESS_TOKEN ?,\\nACCOUNT_NAME \\'onelake\\'\\n);\\n\"\"\", [token])\\n# 3) Attach the Iceberg REST catalog'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='These example requests and responses illustrate the use of the Iceberg REST Catalog (IRC)\\noperations currently supported at the OneLake table API endpoint. For more information about\\nIRC, see the open standard specification.\\nFor each of these operations:\\n<BaseUrl> is https://onelake.table.fabric.microsoft.com/iceberg\\n<Warehouse> is <Workspace>/<DataItem>, which can be:\\n<WorkspaceID>/<DataItemID>, such as 12345678-abcd-4fbd-9e50-3937d8eb1915/98765432-\\ndcba-4209-8ac2-0821c7f8bd91\\n<WorkspaceName>/<DataItemName>.<DataItemType>, such as\\nMyWorkspace/MyItem.Lakehouse, as long as both names do not contain special\\ncharacters.\\n<Prefix> is returned by the Get configuration call, and its value is usually the same as\\n<Warehouse>.\\n<Token> is the access token value returned by Entra ID upon successful authentication.\\nList Iceberg catalog configuration settings.\\nRequest\\nResponse\\ncon.execute(f\"\"\"\\nATTACH \\'{warehouse}\\' AS onelake (\\nTYPE ICEBERG,\\nSECRET onelake_catalog,\\nENDPOINT \\'{table_api_url}\\'\\n);\\n\"\"\")'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='TYPE ICEBERG,\\nSECRET onelake_catalog,\\nENDPOINT \\'{table_api_url}\\'\\n);\\n\"\"\")\\n# --- Explore & Query ---\\ndisplay(con.execute(\"SHOW ALL TABLES\").fetchdf())\\nExample requests and responses\\nGet configuration\\nGET <BaseUrl>/v1/config?warehouse=<Warehouse>\\nAuthorization: Bearer <Token>'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='JSON\\nList schemas\\nList schemas within a Fabric data item.\\nRequest\\nResponse\\nJSON\\nGet schema\\nGet schema details for a given schema.\\n200 OK\\n{\\n    \"defaults\": {},\\n    \"endpoints\": [\\n        \"GET /v1/{prefix}/namespaces\",\\n        \"GET /v1/{prefix}/namespaces/{namespace}\",\\n        \"HEAD /v1/{prefix}/namespaces/{namespace}\",\\n        \"GET /v1/{prefix}/namespaces/{namespace}/tables\",\\n        \"GET /v1/{prefix}/namespaces/{namespace}/tables/{table}\",\\n        \"HEAD /v1/{prefix}/namespaces/{namespace}/tables/{table}\"\\n    ],\\n    \"overrides\": {\\n        \"prefix\": \"<Prefix>\"\\n    }\\n}\\nGET <BaseUrl>/v1/<Prefix>/namespaces\\nAuthorization: Bearer <Token>\\n200 OK\\n{\\n    \"namespaces\": [\\n        [\\n            \"dbo\"\\n        ]\\n    ],\\n    \"next-page-token\": null\\n}'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Request\\nResponse\\nJSON\\nList tables\\nList tables within a given schema.\\nRequest\\nResponse\\nJSON\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>\\nAuthorization: Bearer <Token>\\n200 OK\\n{\\n    \"namespace\": [\\n        \"dbo\"\\n    ],\\n    \"properties\": {\\n        \"location\": \"d892007b-3216-424a-a339-f3dca61335aa/40ef140a-8542-\\n4f4c-baf2-0f8127fd59c8/Tables/dbo\"\\n    }\\n}\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>/tables\\nAuthorization: Bearer <Token>\\n200 OK\\n{\\n    \"identifiers\": [\\n        {\\n            \"namespace\": [\\n                \"dbo\"\\n            ],\\n            \"name\": \"DIM_TestTime\"\\n        },\\n        {\\n            \"namespace\": ['),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Get table\\nGet table details for a given table.\\nRequest\\nResponse\\nJSON\\n                \"dbo\"\\n            ],\\n            \"name\": \"DIM_TestTable\"\\n        }\\n    ],\\n    \"next-page-token\": null\\n}\\nGET <BaseUrl>/v1/<Prefix>/namespaces/<SchemaName>/tables/<TableName>\\nAuthorization: Bearer <Token>\\n200 OK\\n{\\n    \"metadata-location\": \\n\"abfss://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime/metad\\nata/v3.metadata.json\",\\n    \"metadata\": {\\n        \"format-version\": 2,\\n        \"table-uuid\": \"...\",\\n        \"location\": \\n\"abfss://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime\",\\n        \"last-sequence-number\": 2,\\n        \"last-updated-ms\": ...,\\n        \"last-column-id\": 4,\\n        \"current-schema-id\": 0,\\n        \"schemas\": [\\n            {\\n                \"type\": \"struct\",\\n                \"schema-id\": 0,\\n                \"fields\": [\\n                    {\\n                        \"id\": 1,\\n                        \"name\": \"id\",\\n                        \"required\": false,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='\"name\": \"id\",\\n                        \"required\": false,\\n                        \"type\": \"int\"\\n                    },\\n                    {\\n                        \"id\": 2,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='\"name\": \"name\",\\n                        \"required\": false,\\n                        \"type\": \"string\"\\n                    },\\n                    {\\n                        \"id\": 3,\\n                        \"name\": \"age\",\\n                        \"required\": false,\\n                        \"type\": \"int\"\\n                    },\\n                    {\\n                        \"id\": 4,\\n                        \"name\": \"i\",\\n                        \"required\": false,\\n                        \"type\": \"boolean\"\\n                    }\\n                ]\\n            }\\n        ],\\n        \"default-spec-id\": 0,\\n        \"partition-specs\": [\\n            {\\n                \"spec-id\": 0,\\n                \"fields\": []\\n            }\\n        ],\\n        \"last-partition-id\": 999,\\n        \"default-sort-order-id\": 0,\\n        \"sort-orders\": [\\n            {\\n                \"order-id\": 0,\\n                \"fields\": []\\n            }\\n        ],\\n        \"properties\": {'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='\"fields\": []\\n            }\\n        ],\\n        \"properties\": {\\n            \"schema.name-mapping.default\": \"[ {\\\\n  \\\\\"field-id\\\\\" : 1,\\\\n  \\n\\\\\"names\\\\\" : [ \\\\\"id\\\\\" ]\\\\n}, {\\\\n  \\\\\"field-id\\\\\" : 2,\\\\n  \\\\\"names\\\\\" : [ \\\\\"name\\\\\" \\n]\\\\n}, {\\\\n  \\\\\"field-id\\\\\" : 3,\\\\n  \\\\\"names\\\\\" : [ \\\\\"age\\\\\" ]\\\\n}, {\\\\n  \\\\\"field-\\nid\\\\\" : 4,\\\\n  \\\\\"names\\\\\" : [ \\\\\"i\\\\\" ]\\\\n} ]\",\\n            \"write.metadata.delete-after-commit.enabled\": \"true\",\\n            \"write.data.path\": \\n\"abfs://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime\",\\n            \"XTABLE_METADATA\": \"\\n{\\\\\"lastInstantSynced\\\\\":\\\\\"...\\\\\",\\\\\"instantsToConsiderForNextSync\\\\\":\\n[],\\\\\"version\\\\\":0,\\\\\"sourceTableFormat\\\\\":\\\\\"DELTA\\\\\",\\\\\"sourceIdentifier\\\\\":\\\\\"3\\\\\"\\n}\",\\n            \"write.parquet.compression-codec\": \"zstd\"\\n        },\\n        \"current-snapshot-id\": ...,\\n        \"refs\": {\\n            \"main\": {\\n                \"snapshot-id\": ...,\\n                \"type\": \"branch\"\\n            }\\n        },'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='\"snapshots\": [\\n            {\\n                \"sequence-number\": 2,\\n                \"snapshot-id\": ...,\\n                \"parent-snapshot-id\": ...,\\n                \"timestamp-ms\": ...,\\n                \"summary\": {\\n                    \"operation\": \"overwrite\",\\n                    \"XTABLE_METADATA\": \"\\n{\\\\\"lastInstantSynced\\\\\":\\\\\"...\\\\\",\\\\\"instantsToConsiderForNextSync\\\\\":\\n[],\\\\\"version\\\\\":0,\\\\\"sourceTableFormat\\\\\":\\\\\"DELTA\\\\\",\\\\\"sourceIdentifier\\\\\":\\\\\"3\\\\\"\\n}\",\\n                    \"added-data-files\": \"1\",\\n                    \"deleted-data-files\": \"1\",\\n                    \"added-records\": \"1\",\\n                    \"deleted-records\": \"1\",\\n                    \"added-files-size\": \"2073\",\\n                    \"removed-files-size\": \"2046\",\\n                    \"changed-partition-count\": \"1\",\\n                    \"total-records\": \"6\",\\n                    \"total-files-size\": \"4187\",\\n                    \"total-data-files\": \"2\",\\n                    \"total-delete-files\": \"0\",'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='\"total-data-files\": \"2\",\\n                    \"total-delete-files\": \"0\",\\n                    \"total-position-deletes\": \"0\",\\n                    \"total-equality-deletes\": \"0\"\\n                },\\n                \"manifest-list\": \\n\"abfss://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime/metad\\nata/snap-....avro\",\\n                \"schema-id\": 0\\n            }\\n        ],\\n        \"statistics\": [],\\n        \"snapshot-log\": [\\n            {\\n                \"timestamp-ms\": ...,\\n                \"snapshot-id\": ...\\n            }\\n        ],\\n        \"metadata-log\": [\\n            {\\n                \"timestamp-ms\": ...,\\n                \"metadata-file\": \\n\"abfss://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime/metad\\nata/v1.metadata.json\"\\n            },\\n            {\\n                \"timestamp-ms\": ...,\\n                \"metadata-file\": \\n\"abfss://...@onelake.dfs.fabric.microsoft.com/.../Tables/DIM_TestTime/metad\\nata/v2.metadata.json\"\\n            }\\n        ]\\n    }\\n}'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Learn more about OneLake table APIs.\\nLearn more about OneLake table APIs for Iceberg.\\nSet up automatic Delta Lake to Iceberg format conversion.\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake table APIs for Delta\\n10/27/2025\\nOneLake offers a REST API endpoint for interacting with tables in Microsoft Fabric. This article\\ndescribes how to get started using this endpoint to interact with Delta APIs available at this\\nendpoint for metadata read operations. These operations are compatible with Unity Catalog\\nAPI open standard.\\nFor overall OneLake table API guidance and prerequisite guidance, see the OneLake table API\\noverview.\\nFor detailed API documentation, see the Getting started guide.\\nThe OneLake table API endpoint is:\\nAt the OneLake table API endpoint, the Delta APIs are available under the following <BaseUrl>.\\nThe following Delta API operations are currently supported at this endpoint. Detailed guidance\\nfor these operations is available in the Getting started guide.\\nList schemas\\nGET <BaseUrl>/<WorkspaceName or WorkspaceID>/<ItemName or ItemID>/api/2.1/unity-\\ncatalog/schemas?catalog_name=<ItemName or ItemID>\\n）  Important\\nThis feature is in preview.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='catalog/schemas?catalog_name=<ItemName or ItemID>\\n）  Important\\nThis feature is in preview.\\nDelta table API endpoint\\nhttps://onelake.table.fabric.microsoft.com\\nhttps://onelake.table.fabric.microsoft.com/delta\\nDelta table API operations'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='This operation accepts the workspace ID and data item ID (or their equivalent friendly\\nnames if they don’t contain any special characters).\\nThis operation returns the list of schemas within a data item. If the data item does not\\nsupport schemas, a fixed schema named dbo is returned.\\nList tables\\nGET <BaseUrl>/<WorkspaceName or WorkspaceID>/<ItemName or ItemID>/api/2.1/unity-\\ncatalog/tables?catalog_name=<ItemName or ItemID>&schema_name=<SchemaName>\\nThis operation returns the list of tables found within a given schema.\\nGet table\\nGET <BaseUrl>/<WorkspaceName or WorkspaceID>/<ItemName or ItemID>/api/2.1/unity-\\ncatalog/tables/<TableName>\\nThis operation returns metadata details for a table within a schema, if the table is found.\\nSchema exists\\nHEAD <BaseUrl>/<WorkspaceName or WorkspaceID>/<ItemName or ItemID>/api/2.1/unity-\\ncatalog/schemas/<SchemaName>\\nThis operation checks for the existence of a schema within a data item and returns\\nsuccess if the schema is found.\\nTable exists'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='success if the schema is found.\\nTable exists\\nHEAD <BaseUrl>/<WorkspaceName or WorkspaceID>/<ItemName or ItemID>/api/2.1/unity-\\ncatalog/tables/<TableName>\\nThis operation checks for the existence of a table within a schema and returns success if\\nthe schema is found.\\nThe use of the OneLake table APIs for Delta is subject to the following limitations and\\nconsiderations:\\nCertain data items may not support schemas\\nDepending on the type of data item you use, such as non-schema-enabled Fabric\\nlakehouses, there may not be schemas within the Tables directory. In such cases, for\\nCurrent limitations, considerations'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='compatibility with API clients, the OneLake table APIs provide a default, fixed dbo schema\\n(or namespace) to contain all tables within a data item.\\nAdditional query string parameters required if your schema name or table name\\ncontains dots\\nIf your schema or table name contains dots (.) and is included in the URL, you must also\\nprovide additional query parameters. For example, when the schema name includes dots,\\ninclude the catalog_name as an additional query parameter in the API call to check\\nwhether the schema exists.\\nMetadata write operations, other operations\\nOnly the operations listed in Delta table API operations are supported today. Operations\\nthat handle metadata write operations are not yet supported by the OneLake table Delta\\nAPI endpoint.\\nLearn more about OneLake table APIs.\\nSee detailed guidance and API details.\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Getting started with OneLake table APIs for\\nDelta\\n10/27/2025\\nOneLake offers a REST API endpoint for interacting with tables in Microsoft Fabric. This\\nendpoint supports read-only metadata operations for Delta tables in Fabric. These operations\\nare compatible with Unity Catalog API open standard.\\nThese example requests and responses illustrate the use of the Delta API operations currently\\nsupported at the OneLake table API endpoint.\\nFor each of these operations:\\n<BaseUrl> is https://onelake.table.fabric.microsoft.com/delta\\n<Workspace>/DataItem> can be:\\n<WorkspaceID>/<DataItemID>, such as 12345678-abcd-4fbd-9e50-3937d8eb1915/98765432-\\ndcba-4209-8ac2-0821c7f8bd91\\n<WorkspaceName>/<DataItemName>.<DataItemType>, such as\\nMyWorkspace/MyItem.Lakehouse, as long as both names do not contain special\\ncharacters.\\n<Token> is the access token value returned by Microsoft Entra ID upon successful\\nauthentication.\\nList schemas within a Fabric data item.\\nRequest\\nBash\\n）  Important\\nThis feature is in preview.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='List schemas within a Fabric data item.\\nRequest\\nBash\\n）  Important\\nThis feature is in preview.\\nExample requests and responses\\nList schemas\\ncurl -X GET \\\\\\n  \"<BaseUrl>/<Workspace>/testlh.Lakehouse/api/2.1/unity-catalog/schemas?\\ncatalog_name=testlh.Lakehouse\" \\\\'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Response\\nJSON\\nList tables within a given schema.\\nRequest\\nBash\\nResponse\\nJSON\\n  -H \"Authorization: Bearer <Token>\" \\\\\\n  -H \"Content-Type: application/json\"\\n200 OK\\n{\\n\"schemas\": [\\n{\\n\"name\": \"dbo\",\\n\"catalog_name\": \"testlh.Lakehouse\",\\n        \"full_name\": \"testlh.Lakehouse.dbo\",\\n\"created_at\": 1759768029062,\\n\"updated_at\": 1759768029062,\\n\"comment\": null,\\n\"properties\": null,\\n\"owner\": null,\\n\"created_by\": null,\\n\"updated_by\": null,\\n\"schema_id\": null\\n}\\n],\\n\"next_page_token\": null\\n}\\nList tables\\ncurl -X GET \\\\\\n  \"<BaseUrl>/<Workspace>/testlh.Lakehouse/api/2.1/unity-catalog/tables?\\ncatalog_name=testlh.Lakehouse&schema_name=dbo\" \\\\\\n  -H \"Authorization: Bearer <Token>\" \\\\\\n  -H \"Content-Type: application/json\"\\n200 OK\\n{\\n\"tables\": [\\n    {'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Get table details for a given table.\\nRequest\\nBash\\nResponse\\nJSON\\n        \"name\": \"product_table\",\\n        \"catalog_name\": \"testlh.Lakehouse\",\\n        \"schema_name\": \"dbo\",\\n        \"table_type\": null,\\n        \"data_source_format\": \"DELTA\",\\n        \"columns\": null,\\n        \"storage_location\": \\n\"https://onelake.dfs.fabric.microsoft.com/.../.../Tables/product_table\",\\n        \"comment\": null,\\n        \"properties\": null,\\n        \"owner\": null,\\n        \"created_at\": null,\\n        \"created_by\": null,\\n        \"updated_at\": null,\\n        \"updated_by\": null,\\n        \"table_id\": null\\n    }\\n],\\n\"next_page_token\": null\\n}\\nGet table\\ncurl -X GET \\\\\\n  \"<BaseUrl>/<Workspace>/testlh.Lakehouse/api/2.1/unity-\\ncatalog/tables/testlh.Lakehouse.dbo.product_table\" \\\\\\n  -H \"Authorization: Bearer <Token>\" \\\\\\n  -H \"Content-Type: application/json\"\\n    200 OK\\n    {\\n\"name\": \"product_table\",\\n\"catalog_name\": \"testlh.Lakehouse\",\\n\"schema_name\": \"dbo\",\\n\"table_type\": null,\\n\"data_source_format\": \"DELTA\",\\n\"columns\": [\\n{'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='\"schema_name\": \"dbo\",\\n\"table_type\": null,\\n\"data_source_format\": \"DELTA\",\\n\"columns\": [\\n{\\n\"name\": \"product_id\",\\n\"type_text\": null,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='\"type_json\": null,\\n\"type_name\": \"string\",\\n\"type_precision\": 0,\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 0,\\n\"nullable\": true\\n},\\n{\\n\"name\": \"product_name\",\\n\"type_text\": null,\\n\"type_json\": null,\\n\"type_name\": \"string\",\\n\"type_precision\": 0,\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 1,\\n\"nullable\": true\\n},\\n{\\n\"name\": \"category\",\\n\"type_text\": null,\\n\"type_json\": null,\\n\"type_name\": \"string\",\\n\"type_precision\": 0,\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 2,\\n\"nullable\": true\\n},\\n{\\n\"name\": \"brand\",\\n\"type_text\": null,\\n\"type_json\": null,\\n\"type_name\": \"string\",\\n\"type_precision\": 0,\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 3,\\n\"nullable\": true\\n},\\n{\\n\"name\": \"price\",\\n\"type_text\": null,\\n\"type_json\": null,\\n\"type_name\": \"double\",\\n\"type_precision\": 0,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Learn more about OneLake table APIs.\\nLearn more about OneLake table APIs for Delta.\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 4,\\n\"nullable\": true\\n},\\n{\\n\"name\": \"launch_date\",\\n\"type_text\": null,\\n\"type_json\": null,\\n\"type_name\": \"date\",\\n\"type_precision\": 0,\\n\"type_scale\": 0,\\n\"type_interval_type\": null,\\n\"comment\": null,\\n\"partition_index\": 0,\\n\"position\": 5,\\n\"nullable\": true\\n}\\n],\\n\"storage_location\": \\n\"https://onelake.dfs.fabric.microsoft.com/.../.../Tables/product_table\",\\n\"comment\": null,\\n\"properties\": null,\\n\"owner\": null,\\n\"created_at\": 1759703452000,\\n\"created_by\": null,\\n\"updated_at\": 1759703452000,\\n\"updated_by\": null,\\n\"table_id\": \"df2b3038-c21a-429d-90b8-f3bbf2d3db5d\"\\n    }\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake security overview\\n09/08/2025\\nOneLake is a hierarchical data lake, like Azure Data Lake Storage (ADLS) Gen2 or the Windows\\nfile system. Security in OneLake is enforced at multiple levels, each corresponding to different\\naspects of access and control. Understanding the distinction between control plane and data\\nplane permissions is key to effectively securing your data:\\nControl plane permissions: Govern what actions users can perform within the\\nenvironment (e.g., creating, managing, or sharing items). Control plane permissions often\\nprovide data plane permissions by default.\\nData plane permissions: Govern what data users can access or view, regardless of their\\nability to manage resources.\\nYou can set security at each level within the data lake. However, some levels in the hierarchy\\nare given special treatment because they correlate with Fabric concepts. OneLake security\\ncontrols all access to OneLake data with different permissions inherited from the parent item or'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='controls all access to OneLake data with different permissions inherited from the parent item or\\nworkspace permissions. You can set permissions at the following levels:\\nWorkspace: A collaborative environment for creating and managing items. Security is\\nmanaged through workspace roles at this level.\\nItem: A set of capabilities bundled together into a single component. A data item is a\\nsubtype of item that allows data to be stored within it using OneLake. Items inherit\\npermissions from the workspace roles, but can have additional permissions as well.\\nFolders: Folders within an item that are used for storing and managing data, such as\\nTables/ or Files/.\\nItems always live within workspaces and workspaces always live directly under the OneLake\\nnamespace. You can visualize this structure as follows:'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='This section describes the security model based on generally available OneLake features.\\nWorkspace permissions define what actions users can take within a workspace and its items.\\nThese permissions are managed at the workspace level and are primarily control plane\\npermissions; they determine administrative and item management capabilities, not direct data\\naccess. However, workspace permissions will generally inherit down to the item and folder level\\nto grant data access by default. Workspace permissions allow for defining access to all items\\nwithin that workspace. There are four different workspace roles, each of which grants different\\ntypes of access. Below are the default behaviors of each workspace role.\\nRole Can add\\nadmins?\\nCan add\\nmembers?\\nCan edit\\nOneLake\\nsecurity?\\nCan write data\\nand create items?\\nCan read data in\\nOneLake?\\nAdmin Yes Yes Yes Yes Yes\\nMember No Yes Yes Yes Yes\\nContributorNo No No Yes Yes\\nViewer No No No No No*\\n\\uf80a\\nSecurity in OneLake\\nWorkspace permissions'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='ContributorNo No No Yes Yes\\nViewer No No No No No*\\n\\uf80a\\nSecurity in OneLake\\nWorkspace permissions\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"You can simplify the management of Fabric workspace roles by assigning them to security\\ngroups. This method lets you control access by adding or removing members from the security\\ngroup.\\nWith the sharing feature, you can give a user direct access to an item. The user can only see\\nthat item in the workspace and isn't a member of any workspace roles. Item permissions grant\\naccess to connect to that item and its endpoints the user is able to access.\\nPermission See the item metadata? See data in SQL? See data in OneLake?\\nRead Yes No No\\nReadData No Yes No\\nReadAll No No Yes*\\n*Not applicable to items with OneLake security or data access roles enabled. If the preview is\\nenabled, ReadAll only grants access if the DefaultReader role is in use. If the DefaultReader role\\nis edited or deleted, access is instead granted based on what data access roles the user is part\\nof.\\nAnother way to configure permissions is via an item's Manage permissions page. Using this\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"of.\\nAnother way to configure permissions is via an item's Manage permissions page. Using this\\npage, you can add or remove individual item permission for users or groups. The item type\\ndetermines which permissions are available.\\nOneLake security allows users to define granular role-based security to data stored in OneLake,\\nand enforce that security consistently across all compute engines in Fabric. OneLake security is\\nthe data plane security model for data in OneLake.\\n７ Note\\n*Viewers can be given access to data through OneLake security roles.\\nItem permissions\\nﾉ Expand table\\nOneLake security (preview)\\n７ Note\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Fabric users in the Admin or Member roles can create OneLake security roles to grant users\\naccess to data within an item. Each role has four components:\\nData: The tables or folders that users can access.\\nPermission: The permissions that users have on the data.\\nMembers: The users that are members of the role.\\nConstraints: The components of the data, if any, that are excluded from role access, such\\nas specific rows or columns.\\nOneLake security roles grant access to data for users in the Viewer workspace role or with\\nRead permission on the item. Admins, Members, and Contributors are not affected by OneLake\\nsecurity roles and can read and write all data in an item regardless of their role membership. A\\nDefaultReader role exists in all lakehouses that gives any user with the ReadAll permission\\naccess to data in the lakehouse. The DefaultReader role can be deleted or edited to remove\\nthat access.\\nLearn more about creating OneLake security roles for Tables and folders, Columns, and Rows.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Learn more about creating OneLake security roles for Tables and folders, Columns, and Rows.\\nLearn more about the access control model for OneLake security..\\nCompute permissions are a type of data plane permission that applies to a specific query\\nengine in Microsoft Fabric. The access granted applies only to queries run against that specific\\nengine, such as the SQL endpoint or a Power BI semantic model. However, users might see\\ndifferent results when they access data through a compute engine compared to when they\\naccess data directly in OneLake, depending on the compute permissions applied. OneLake\\nsecurity is the recommended approach to secure data in OneLake to ensure consistent\\nresults across all engines that a user might interact with.\\nCompute engines may have more advanced security features that are not yet available in\\nOneLake security, and in that case using the compute permissions may be required to solve'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake security, and in that case using the compute permissions may be required to solve\\nsome scenarios. When using compute permissions to secure access to data, make sure that\\nend users are given access only to the compute engine where the security is set. This prevents\\ndata from being accessed through a different engine without the necessary security features.\\nOneLake security replaces the existing OneLake data access roles (preview) feature that\\nwas released in April 2024. All data access roles users are seamlessly and automatically\\nupgraded to OneLake security roles when the feature moved to public preview.\\nCompute permissions\\nShortcut security'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Shortcuts in Microsoft Fabric allow for simplified data management. OneLake folder security\\napplies to OneLake shortcuts based on roles defined in the lakehouse where the data is stored.\\nFor more information on shortcut security considerations, see OneLake security access control\\nmodel.\\nFor information on access and authentication details for specific shortcuts, see types of\\nOneLake shortcuts.\\nOneLake uses Microsoft Entra ID for authentication; you can use it to give permissions to user\\nidentities and service principals. OneLake automatically extracts the user identity from tools,\\nwhich use Microsoft Entra authentication and maps it to the permissions you set in the Fabric\\nportal.\\nTo view your OneLake audit logs, follow the instructions in Track user activities in Microsoft\\nFabric. OneLake operation names correspond to ADLS APIs such as CreateFile or DeleteFile.\\nOneLake audit logs don't include read requests or requests made to OneLake via Fabric\\nworkloads.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake audit logs don't include read requests or requests made to OneLake via Fabric\\nworkloads.\\nData stored in OneLake is encrypted at rest by default using Microsoft-managed keys.\\nMicrosoft-managed keys are rotated appropriately. Data in OneLake is encrypted and\\ndecrypted transparently and is FIPS 140-2 compliant.\\nEncryption at rest using customer-managed keys currently isn't supported. You can submit a\\nrequest for this feature on Microsoft Fabric ideas.\\nAuthentication\\n７ Note\\nTo use service principals in a Fabric tenant, a tenant administrator must enable Service\\nPrincipal Names (SPNs) for the entire tenant or specific security groups. Learn more about\\nenabling Service Principals in Developer settings of the tenant admin portal.\\nAudit Logs\\nEncryption and networking\\nData at Rest\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Data in transit across the public internet between Microsoft services is always encrypted with at\\nleast TLS 1.2. Fabric negotiates to TLS 1.3 whenever possible. Traffic between Microsoft services\\nalways routes over the Microsoft global network.\\nInbound OneLake communication also enforces TLS 1.2 and negotiates to TLS 1.3 whenever\\npossible. Outbound Fabric communication to customer-owned infrastructure prefers secure\\nprotocols but might fall back to older, insecure protocols (including TLS 1.0) when newer\\nprotocols aren't supported.\\nTo configure private links in Fabric, see Set up and use private links.\\nYou can allow or restrict access to OneLake data from applications that are outside of the\\nFabric environment. Admins can find this setting in the OneLake section of the admin portal\\ntenant settings.\\nWhen you turn on this setting, users can access data from all sources. For example, turn this\\nsetting on if you have custom applications that use Azure Data Lake Storage (ADLS) APIs or\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"setting on if you have custom applications that use Azure Data Lake Storage (ADLS) APIs or\\nOneLake file explorer. When you turn off this setting, users can still access data from internal\\napps like Spark, Data Engineering, and Data Warehouse, but can't access data from\\napplications running outside of Fabric environments.\\nFabric and OneLake security overview\\nOneLake data access roles (preview)\\nWorkspace roles\\nOneLake file explorer\\nShare items\\nData in transit\\nPrivate links\\nAllow apps running outside of Fabric to access data\\nvia OneLake\\nRelated content\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Get started with OneLake security (preview)\\nOneLake security enables you to apply role-based access control (RBAC) to your data stored in\\nOneLake. You can define security roles that grant access to specific folders within a Fabric item,\\nthen assign these roles to users or groups. Roles can also contain row or column level security\\nto further limit access. The OneLake security permissions determine what data that user can see\\nacross all experiences in Fabric.\\nFabric users with Write and Reshare permissions (generally Admin and Member workspace\\nusers) can get started by creating OneLake security roles to grant access to only specific folders\\nor tables in a Fabric data item. To grant access to data in an item, add users to a data access\\nrole. Users that aren't part of a data access role see no data in that item.\\nTo configure OneLake security, you must be an Admin or Member in the workspace, or have\\nWrite and Reshare permissions. Role creation and membership assignment take effect as soon\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Write and Reshare permissions. Role creation and membership assignment take effect as soon\\nas the role is saved, so make sure you want to grant access before adding someone to a role.\\nThe following table outlines which data items support OneLake security:\\nFabric item Status Supported permissions\\nLakehouse Preview Read, ReadWrite\\nAzure Databricks Mirrored Catalog Preview Read\\nOneLake security is currently in preview and as a result is disabled by default. The preview\\nfeature is configured on a per-item basis. The opt-in control allows for a single item to try the\\npreview without enabling it on any other Fabric items.\\nThe preview feature can't be turned off once enabled.\\n1. Navigate to a lakehouse and select Manage OneLake security (preview).\\n2. Review the confirmation dialog. The data access roles preview isn't compatible with the\\nExternal data sharing preview. If you're ok with the change, select Continue.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"External data sharing preview. If you're ok with the change, select Continue.\\nTo ensure a smooth opt-in experience, all users with read permission to data in the item\\ncontinue to have read access through a default data access role called DefaultReader. With\\nPrerequisites\\nﾉ Expand table\\nHow to opt in\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='virtualized role memberships, all users that had the necessary permissions to view data in the\\nlakehouse (the ReadAll permission) are included as members of this default role. To start\\nrestricting access to those users, delete the DefaultReader role or remove the ReadAll\\npermission from the accessing users.\\nUse OneLake security roles to manage OneLake read access to any tables or folders in an item.\\nAccess to tables can be further restricted using row and/or column level security. Any security\\nset applies to access from all engines in Fabric. For more information, see the data access\\ncontrol model.\\nFor specific item types, ReadWrite access can also be configured. This permission gives users\\nthe ability to edit data in a lakehouse on specified tables or folders without giving them access\\nto create or manage Fabric items. ReadWrite access enables users to perform write operations\\nthrough Spark notebooks, the OneLake file explorer, or OneLake APIs. Write operations'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='through Spark notebooks, the OneLake file explorer, or OneLake APIs. Write operations\\nthrough the Lakehouse UX for viewers is not supported.\\nUse the following steps to create a OneLake security role.\\n1. Open the Fabric item where you want to define security.\\n2. Select Manage OneLake security (preview) from the item menu.\\n3. On the OneLake security (preview) pane, select New.\\n4. Provide a name for the new role that meets the following guidelines:\\nThe role name can only contain alphanumeric characters.\\nThe role name must start with a letter.\\nNames are case insensitive and must be unique.\\nThe maximum name length is 128 characters.\\n5. Select Grant as the type of role.\\n） Important\\nMake sure that any users that are included in a data access role are removed from the\\nDefaultReader role. Otherwise they maintain full access to the data.\\nWhat types of data can be secured?\\nCreate a role'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"6. Choose the permissions you want to grant. Read is selected at a minimum, and you can\\noptionally choose ReadWrite.\\n7. If you want this role to apply to all of the tables and files in this lakehouse, select the All\\ndata toggle.\\nThis selection also provides access to any folders that are added in the future.\\n8. If you want this role to apply only to a selected group of tables and folders, select the\\nSelected data toggle. Then, use the following steps to define the approved data for this\\nrole.\\na. Select Browse Lakehouse or the equivalent for the item that you're working with.\\nb. Expand the Tables and Files directories to view data in your lakehouse.\\nc. Check the boxes next to the tables and files that you want the role to apply to.\\nd. Select Add data to add the selected items to your role.\\n9. Use the Add members to your role textbox to manually enter the names or email\\naddresses of users that you want to include in the role. Or, select Advanced configuration\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='addresses of users that you want to include in the role. Or, select Advanced configuration\\nand follow the guidance in Assign virtual members.\\nTo add members manually:'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='a. Enter the name or email address of a user.\\nb. Select the correct name from the suggested list.\\nc. Select the check icon to confirm your selection, or the X icon to clear the selection.\\n10. Review the Preview role summaries.\\na. To edit the data preview, select Browse Lakehouse and update the selected tables and\\nfolders.\\nb. To remove a user from the members preview, select more options (...) next to their\\nname, then Remove from role.\\n11. Select Create role and wait for the notification that the role was successfully published.\\nUse the following steps to edit an existing OneLake security role.\\n1. Open the item where you want to define security.\\n2. Select Manage OneLake security (preview) from the item menu.\\n3. On the OneLake security (preview) pane, select the role that you want to edit.\\nThis action opens the role details page, which includes two tabs: Data in role and\\nMembers in role.\\n4. Review the information in the Data in role tab:'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Members in role.\\n4. Review the information in the Data in role tab:\\nThis tab shows all of the data that the members of the role can access.\\nThe role name tells you which role you are looking at. To edit the role name, select the\\nEdit dropdown in the upper right corner, select Update role name, enter a new name,\\nand then confirm with the check mark. You can discard your changes by selecting the X.\\nThe Permissions item at the top, tells you what permissions the role currently grants. To\\nchange the role permissions, select the Edit dropdown in the upper right corner, select\\nEdit role permissions, edit the selected permissions with the dropdown, and then confirm\\nwith the check mark. You can discard your changes by selecting the X.\\nThe Data column shows the name of the tables or folders that are part of the role access.\\nYou can expand and collapse schemas to view the items underneath. Hover over an entry'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You can expand and collapse schemas to view the items underneath. Hover over an entry\\nto view the full path of the table or folder. Hover over the ... to see options to configure\\nRow-level security or Column-level security. The row level security and column level\\nsecurity guides provide more information on how that works.\\nEdit a role'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The Type column tells you the type of item that was selected. The values are either:\\nSchema, Table, or Folder.\\nThe Data access column indicates whether any row or column level restrictions are\\napplied to the item. An icon with a lock and horizontal lines indicates row level security is\\napplied, while an icon with a lock and vertical lines indicates column level security is\\napplied.\\n5. To edit the data included in the role, select Add data.\\nThis action opens the table and folder selection dialog.\\n6. Check and uncheck tables or folders to add or remove them from the role.\\n7. Select Add data to confirm your selections.\\n8. Select the Members in role tab to view the members of the role.\\nThe Members column shows the profile picture and name of the member.\\nThe Type column indicates whether the member is a User or Group.\\nThe Added using column denotes whether a user was added via their Email as a member\\nof the role, or included as part of a lakehouse permissions group. For more information'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='of the role, or included as part of a lakehouse permissions group. For more information\\nabout adding users using item permissions, see Assign virtual members.\\n9. To edit the members of the role, select Add members.\\n10. To add members manually, enter a name or email in the Add members to your role\\ntextbox. Select the correct name from the suggested list. Then, select the check icon to\\nconfirm your selection, or select the X icon to clear the selection.\\n11. To remove users from the role, select more options (...) next to their name and select\\nRemove from role.\\nMaking any changes to role membership updates the role immediately. A notification notes\\nthe success or failure of any changes.\\nUse the following steps to delete a OneLake data access role.\\n1. Open the lakehouse where you want to define security.\\n2. Select Manage OneLake security (preview) from the Lakehouse menu.\\nDelete a role'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='3. On the OneLake security (preview) pane, check the box next to the roles you want to\\ndelete.\\n4. Select Delete and wait for the notification that the roles are successfully deleted.\\nOneLake security role supports two methods of adding users to a role. The main method is by\\nadding users or groups directly to a role using the Add people or groups box on the Assign\\nrole page. The second is by creating virtual memberships with permission groups using the\\nAdvanced configuration control.\\nAdding users directly to a role adds the users as explicit members of the role. These users show\\nup with their name and picture shown in the Members list.\\nThe virtual members allow for the membership of the role to be dynamically adjusted based on\\nthe Fabric item permissions of the users. By selecting Advanced configuration and selecting a\\npermission, you add any user in the Fabric workspace who has all of the selected permissions'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"permission, you add any user in the Fabric workspace who has all of the selected permissions\\nas an implicit member of the role. For example, if you chose ReadAll, Write then any user of\\nthe Fabric workspace that has ReadAll and Write permissions to the item would be included as\\na member of the role. You can see which users are being added by a permission group by\\nlooking at the Added using column in the Members in role tab. These members can't be\\nmanually removed directly. To remove a member that was added through a permission group,\\nremove the permission group from the role.\\nRegardless of which membership type you use, OneLake security roles support adding\\nindividual users, Microsoft Entra groups, and security principals.\\nThe permissions that can be used for virtual members are:\\nRead\\nWrite\\nReshare\\nExecute\\nReadAll\\nTo assign users with permission groups, use the following steps:\\n1. Select the name of the role you want to assign members to.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Select the name of the role you want to assign members to.\\n2. On the role details page, select the\\u202fMembers in role tab.\\nAssign a member or group\\nAssign virtual members'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='3. Select Add members.\\n4. Select Advanced configuration.\\n5. In the Permission groups box, select the checkbox next to each permission that you want\\nto include users for.\\nEach permission group shows a count of how many users are included in that group.\\nSelecting multiple permission groups includes users with all of the selected required\\npermissions.\\n6. Select Add to include the groups and save the role.\\nFabric Security overview\\nFabric and OneLake security overview\\nData access control model\\nLast updated on 11/18/2025\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Best practices for OneLake security\\n09/08/2025\\nIn this article, we'll look at best practices around securing data in OneLake. For more\\ninformation on how to implement security for specific use cases, see the how-to guides.\\nLeast privilege access is a fundamental security principle in computer science that advocates\\nfor restricting users' permissions and access rights to only those permissions necessary to\\nperform their tasks. For OneLake, this means assigning permissions at the appropriate level to\\nensure that users aren't over-provisioned and reduce risk.\\nIf users only need access to a single lakehouse or data item, use the share feature to\\ngrant them access to only that item. Assigning a user to a workspace role should only be\\nused if that user needs to see ALL items in that workspace.\\nUse OneLake security to restrict access to folders and tables within a lakehouse. For\\nsensitive data, OneLake security row or column level security ensures that protected row\\nand columns remain hidden.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='and columns remain hidden.\\nDifferent users need the ability to perform different actions in Fabric in order to do their jobs.\\nSome common use cases are identified in this section along with the necessary permissions\\nsetup in Fabric and OneLake.\\nManage workspace access The admin or member workspace roles are required. These roles\\ncan also manage OneLake security roles on an item.\\nCreate new items in Fabric Either Admin, Member, or Contributor roles can create or delete\\nnew items.\\nWrite data to OneLake Either Admin, Member, or Contributor roles can write data to OneLake\\nthrough Spark or through uploads. They can also write data to a warehouse. Users with only\\nread access on a warehouse can be given permissions to write data through SQL permissions.\\nRead data from OneLake A user needs to be a workspace Viewer, or have the Read permission\\nand the ReadAll permission to read data from OneLake. For lakehouses with the OneLake'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"and the ReadAll permission to read data from OneLake. For lakehouses with the OneLake\\nsecurity (preview) feature enabled, access to data is controlled by the user's OneLake security\\nrole permissions.\\nLeast privilege\\nSecure by use case\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Subscribe to OneLake events A user needs SubscribeOneLakeEvents to be able to subscribe to\\nevents from a Fabric item. Admin, Member, and Contributor roles have this permission by\\ndefault. You can add this permission for a user with Viewer role.\\nFabric Security overview\\nFabric and OneLake security overview\\nData Access Control Model\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake security access control model\\n(preview)\\nThis document provides a detailed guide to how the OneLake security access control model\\nworks. It contains details on how the roles are structured, how they apply to data, and what the\\nintegration is with other structures within Microsoft Fabric.\\nOneLake security uses a role based access control (RBAC) model for managing access to data\\nin OneLake. Each role is made up of several key components.\\nType: Whether the role gives access (GRANT) or removes access (DENY). Only GRANT\\ntype roles are supported.\\nPermission: The specific action or actions that are being granted or denied.\\nScope: The OneLake objects that have the permission. Objects are tables, folders, or\\nschemas.\\nMembers: Any Microsoft Entra identity that is assigned to the role, such as users, groups,\\nor nonuser identities. The role is granted to all members of a Microsoft Entra group.\\nBy assigning a member to a role, that user is then subject to the associated permissions on the'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='By assigning a member to a role, that user is then subject to the associated permissions on the\\nscope of that role. Because OneLake security uses a deny-by-default model, all users start with\\nno access to data unless explicitly granted by a OneLake security role.\\nOneLake security roles support the following permission:\\nRead: Grants the user the ability to read data from a table and view the associated table\\nand column metadata. In SQL terms, this permission is equivalent to both\\nVIEW_DEFINITION and SELECT. For more information, see the Metadata security.\\nReadWrite: Grants the user the ability to read and write data from a table or folder and\\nview the associated table and column metadata. In SQL terms, this permission is\\nequivalent to ALTER, DROP, UPDATE, and INSERT. For more information see ReadWrite\\npermission.\\nOneLake security enables users to define data access roles for the following Fabric items only.\\nOneLake security roles\\nPermissions and supported items\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Fabric item Status Supported permissions\\nLakehouse Public Preview Read, ReadWrite\\nAzure Databricks Mirrored Catalog Public Preview Read\\nWorkspace permissions are the first security boundary for data within OneLake. Each\\nworkspace represents a single domain or project area where teams can collaborate on data.\\nYou manage security in the workspace through Fabric workspace roles. Learn more about\\nFabric role-based access control (RBAC): Workspace roles\\nFabric workspace roles give permissions that apply to all items in the workspace. The following\\ntable outlines the basic permissions allowed by workspace roles.\\nPermission Admin Member Contributor Viewer\\nView files in OneLakeAlways*\\nYes\\nAlways*\\nYes\\nAlways* Yes No by default. Use OneLake security\\nto grant the access.\\nWrite files in OneLakeAlways*\\nYes\\nAlways*\\nYes\\nAlways* Yes No\\nCan edit OneLake\\nsecurity roles\\nAlways*\\nYes\\nAlways*\\nYes\\nNo No\\n*Since Workspace Admin, Member and Contributor roles automatically grant Write permissions'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='No No\\n*Since Workspace Admin, Member and Contributor roles automatically grant Write permissions\\nto OneLake, they override any OneLake security Read permissions.\\nWorkspace roles manage the control plane data access, meaning interactions with creating and\\nmanaging Fabric artifacts and permissions. In addition, workspace roles also provide default\\naccess levels to data items by using OneLake security default roles. (Note that default roles\\nonly apply to Viewers, since Admin, Member, and Contributor have elevated access through\\nthe Write permission) A default role is a normal OneLake security role that is created\\nautomatically with every new item. It gives users with certain workspace or item permissions a\\ndefault level of access to data in that item. For example, Lakehouse items have a DefaultReader\\nrole that lets users with the ReadAll permission see data in the Lakehouse. This ensures that\\nusers accessing a newly created item have a basic level of access. All default roles use a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='users accessing a newly created item have a basic level of access. All default roles use a\\nmember virtualization feature, so that the members of the role are any user in that workspace\\nwith the required permission. For example, all users with ReadAll permission on the Lakehouse.\\nOneLake security and workspace permissions\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"The following table shows what the standard default roles are. Items may have specialized\\ndefault roles that apply only to that item type.\\nFabric\\nitem\\nRole name PermissionFolders included Assigned members\\nLakehouseDefaultReader Read All folders under Tables/\\nand Files/\\nAll users with ReadAll\\npermission\\nLakehouseDefaultReadWriter Read All folders All users with Write\\npermission\\nWithin a workspace, Fabric items can have permissions configured separately from the\\nworkspace roles. You can configure permissions either through sharing an item or by managing\\nthe permissions of an item. The following permissions determine a user's ability to perform\\nactions on data in OneLake. For more information on item sharing, see How Lakehouse sharing\\nworks\\nPermission Can view files in OneLake? Can write files in\\nOneLake?\\nCan read data through\\nSQL analytics\\nendpoint?\\nRead No by default. Use OneLake\\nsecurity to grant access.\\nNo No\\nReadAll Yes through the DefaultReader\\nrole. Use OneLake security to\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"security to grant access.\\nNo No\\nReadAll Yes through the DefaultReader\\nrole. Use OneLake security to\\nrestrict access.\\nNo No*\\nWrite Yes Yes Yes\\nExecute, Reshare,\\nViewOutput,\\nN/A - can't be granted on its ownN/A - can't be\\ngranted on its\\nN/A - can't be granted\\non its own\\nﾉ Expand table\\n７ Note\\nTo restrict the access to specific users or specific folders, either modify the default role or\\nremove it and create a new custom role.\\nOneLake security and item permissions\\nﾉ Expand table\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Permission Can view files in OneLake? Can write files in\\nOneLake?\\nCan read data through\\nSQL analytics\\nendpoint?\\nViewLogs own\\n*Depends on the SQL analytics endpoint mode.\\nYou can define and manage OneLake security roles through your lakehouse data access\\nsettings.\\nLearn more in Get started with data access roles.\\nData access to OneLake occurs in one of two ways:\\nThrough a Fabric query engine or\\nThrough user access (Queries from non-Fabric engines are considered user access)\\nOneLake security ensures that data is always kept secure. Because certain OneLake security\\nfeatures like row and column level security aren't supported by storage level operations, not all\\ntypes of access to row or column level secured data can be permitted. This guarantees that\\nusers can't see rows or columns they aren't permitted to. Microsoft Fabric engines are enabled\\nto apply row and column level security filtering to data queries. This means when a user\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='to apply row and column level security filtering to data queries. This means when a user\\nqueries data in a lakehouse or other item with OneLake security RLS or CLS on it, the results\\nthe user sees have the hidden rows and columns removed. For user access to data in OneLake\\nwith RLS or CLS on it, the query is blocked if the user requesting access isn\\'t permitted to see\\nall the rows or columns in that table.\\nThe table below outlines which Microsoft Fabric engines support RLS and CLS filtering.\\nEngine RLS/CLS filtering Status\\nLakehouse Yes Public preview\\nSpark notebooks Yes Public preview\\nSQL Analytics Endpoint in \"user\\'s identity mode\" Yes Public preview\\nCreate roles\\nEngine and user access to data\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Engine RLS/CLS filtering Status\\nSemantic models using DirectLake on OneLake modeYes Public preview\\nEventhouse No Planned\\nData warehouse external tables No Planned\\nThis section provides details on how OneLake security roles grant access to specific scopes,\\nhow that access operates, and how access is resolved across multiple roles and access types.\\nAll OneLake tables are represented by folders in the lake, but not all folders in the lake are\\ntables from the perspective of OneLake security and query engines in Fabric. To be considered\\na valid table, the following conditions must be met:\\nThe folder exists in the Tables/ directory of an item.\\nThe folder contains a _delta_log folder with corresponding JSON files for the table\\nmetadata.\\nThe folder does not contain any child shortcuts.\\nAny tables that do not meet those criteria will have access denied if table level security is\\nconfigured on them.\\nOneLake security's Read access to data grants full access to the data and metadata in a table.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake security's Read access to data grants full access to the data and metadata in a table.\\nFor users with no access to a table, the data is never exposed and generally the metadata isn't\\nvisible. This also applies to column level security and a user's ability to see or not see a column\\nin that table. However, OneLake security doesn't guarantee that the metadata for a table won't\\nbe accessible, specifically in the following cases:\\nSQL Endpoint queries: SQL Analytics Endpoint uses the same metadata security behavior\\nas SQL Server. This means that if a user doesn't have access to a table or column, the\\nerror message for that query will explicitly state the table or column names the user\\ndoesn't have access to.\\nSemantic models: Giving a user Build permission on a semantic model allows them access\\nto see the table names included in the model, regardless of whether the user has access\\nOneLake security access control model details\\nTable level security\\nMetadata security\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"to them or not. In addition, report visuals that contain hidden columns show the column\\nname in the error message.\\nFor any given folder, OneLake security permissions always inherit to the entire hierarchy of the\\nfolder's files and subfolders.\\nFor example, consider the following hierarchy of a lakehouse in OneLake:\\nBash\\nYou create two roles for this lakehouse. Role1 grants read permission to folder1, and Role2\\ngrants read permission to folder2.\\nFor the given hierarchy, OneLake security permissions for Role1 and Role2 inherit in the\\nfollowing way:\\nRole1: Read folder1\\nBash\\nRole2: Read folder2\\nBash\\nPermission inheritance\\nTables/\\n──── (empty folder)\\nFiles/\\n────folder1\\n│   │   file11.txt\\n│   │\\n│   └───subfolder11\\n│       │   file1111.txt\\n|       │\\n│       └───subfolder111\\n|            │   file1111.txt\\n│   \\n└───folder2\\n    │   file21.txt\\n│   │   file11.txt\\n│   │\\n│   └───subfolder11\\n│       │   file1111.txt\\n|       │\\n│       └───subfolder111\\n|            │   file1111.txt\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake security provides automatic traversal of parent items to ensure that data is easy to\\ndiscover. Granting a user Read permissions to subfolder11 grants the user the ability to list and\\ntraverse the parent directory folder1. This functionality is similar to Windows folder permissions\\nwhere giving access to a subfolder provides discovery and traversal for the parent directories.\\nThe list and traversal granted to the parent doesn't extend to other items outside of the direct\\nparents, ensuring other folders are kept secure.\\nFor example, consider the following hierarchy of a lakehouse in OneLake.\\nBash\\nFor the given hierarchy, OneLake security permissions for 'Role1' provides the following access.\\nAccess to file11.txt isn't visible as it isn't a parent of subfolder11. Likewise for Role2, file111.txt\\nisn't visible either.\\nRole1: Read subfolder11\\nBash\\n    │   file21.txt\\nTraversal and listing in OneLake security\\nTables/\\n──── (empty folder)\\nFiles/\\n────folder1\\n│   │   file11.txt\\n│   │\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Tables/\\n──── (empty folder)\\nFiles/\\n────folder1\\n│   │   file11.txt\\n│   │\\n│   └───subfolder11\\n│       │   file111.txt\\n|       │\\n│       └───subfolder111\\n|            │   file1111.txt\\n│   \\n└───folder2\\n    │   file21.txt\\nFiles/\\n────folder1\\n│   │\\n│   └───subfolder11\\n│       │   file111.txt\\n|       │\\n│       └───subfolder111\\n|            │   file1111.txt'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Role2: Read subfolder111\\nBash\\nFor shortcuts, the listing behavior is slightly different. Shortcuts to external data sources behave\\nthe same as folders do, however shortcuts to other OneLake locations have specialized\\nbehavior. The target permissions of the shortcut determine access to a OneLake shortcut.\\nWhen listing shortcuts, no call is made to check the target access. As a result, when listing a\\ndirectory all internal shortcuts are returned regardless of a user's access to the target. When a\\nuser tries to open the shortcut, the access check evaluates and a user only sees data that they\\nhave the required permissions to see. For more information on shortcuts, see the shortcuts\\nsecurity section.\\nConsider the following folder hierarchy that contains shortcuts.\\nBash\\nRole1: Read folder1\\nBash\\nRole2: No permissions defined\\nBash\\nFiles/\\n────folder1\\n│   │\\n│   └───subfolder11\\n|       │\\n│       └───subfolder111\\n|            │   file1111.txt\\nFiles/\\n────folder1\\n│   \\n└───shortcut2\\n|\\n└───shortcut3\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='|            │   file1111.txt\\nFiles/\\n────folder1\\n│   \\n└───shortcut2\\n|\\n└───shortcut3\\nFiles/\\n────folder1\\n│   \\n└───shortcut2\\n|\\n└───shortcut3'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake security allows users to specify row level security by writing SQL predicates to limit\\nwhat data is shown to a user. RLS operates by showing rows where the predicate evaluates to\\ntrue. For more information, see the row level security.\\nRow level security evaluates string data as case insensitive, using the following collation for\\nsorting and comparisons: Latin1_General_100_CI_AS_KS_WS_SC_UTF8\\nWhen using row level security, ensure that the RLS statements are clean and easy to\\nunderstand. Use integer columns for sorting and greater than or less than operations. Avoid\\nstring equivalencies if you don't know the format of the input data, especially in relation to\\nunicode characters or accent sensitivity.\\nOneLake security supports limiting access to columns by removing (hiding) a user's access to a\\ncolumn. A hidden column is treated as having no permissions assigned to it, resulting in the\\ndefault policy of no access. Hidden columns won't be visible to users, and queries on data\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"default policy of no access. Hidden columns won't be visible to users, and queries on data\\ncontaining hidden columns return no data for that column. As noted in metadata security there\\nare certain case where the metadata of a column might still be visible in some error messages.\\nColumn level security also follows a more strict behavior in SQL Endpoint by operating through\\na deny semantic. Deny on a column in SQL Endpoint ensures that all access to the column is\\nblocked, even if multiple roles would combine to give access to it. As a result, CLS in SQL\\nEndpoint operates using an intersection between all roles a user is part of instead of the union\\nbehavior in place for all other permission types. See the Evaluating multiple OneLake security\\nroles section for more information on how roles combine.\\nThe ReadWrite permission gives read-only users the ability to perform write operations to\\nspecific items. ReadWrite permission is only applicable for Viewers or users with the Read\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='specific items. ReadWrite permission is only applicable for Viewers or users with the Read\\npermission on an item. Assigning ReadWrite access to an Admin, Member, or Contributor has\\nno effect as those roles already have that permission implicitly.\\nFiles/\\n│   \\n└───shortcut2\\n|\\n└───shortcut3\\nRow level security\\nColumn level security\\nReadWrite permission'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='ReadWrite access enables users to perform write operations through Spark notebooks, the\\nOneLake file explorer, or OneLake APIs. Write operations through the Lakehouse UX for viewers\\nis not supported.\\nThe ReadWrite permission operates in the following ways:\\nThe ReadWrite permission includes all privileges granted by the Read permission.\\nUsers with ReadWrite permissions on an object can perform write operations on that\\nobject, inclusive. That is, any operations can also be performed on the object itself.\\nReadWrite allows the following actions:\\nCreate a new folder or table\\nDelete a folder or table\\nRename a folder or table\\nUpload or edit a file\\nCreate a shortcut\\nDelete a shortcut\\nRename a shortcut\\nOneLake security roles with ReadWrite access cannot contain RLS or CLS constraints.\\nBecause Fabric only supports single engine writes to data, users with ReadWrite\\npermission on an object can only Write to that data through OneLake. However, the Read'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"permission on an object can only Write to that data through OneLake. However, the Read\\noperations will be enforced consistently through all querying engines.\\nOneLake security integrates with shortcuts in OneLake to ensure data inside and outside of\\nOneLake can be easily secured. There are two main authentication modes for shortcuts:\\nPassthrough shortcuts (SSO): The credential of the querying user is evaluated against the\\nshortcut target to determine what data is allowed to be seen.\\nDelegated shortcuts: The shortcut uses a fixed credential to access the target and the\\nquerying user is evaluated against OneLake security prior to checking the delegated\\ncredential's access to the source.\\nIn addition, OneLake security permissions are evaluated when creating any shortcuts in\\nOneLake. Read about shortcut permissions in the shortcut security document.\\nShortcuts\\nShortcuts overview\\nOneLake security in passthrough shortcuts\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Security set on a OneLake folder always flows across any internal shortcuts to restrict access to\\nthe shortcut source path. When a user accesses data through a shortcut to another OneLake\\nlocation, the identity of the calling user is used to authorize access to the data in the target\\npath of the shortcut. As a result, this user must have OneLake security permissions in the target\\nlocation to read the data.\\nDefining OneLake security permissions for the internal shortcut isn't allowed and must be\\ndefined on the target folder located in the target item. The target item must be an item type\\nthat supports OneLake security roles. If the target item doesn't support OneLake security, the\\nuser's access is evaluated based on whether they have the Fabric ReadAll permission on the\\ntarget item. Users don't need Fabric Read permission on an item in order to access it through a\\nshortcut.\\nOneLake supports defining permissions for shortcuts such as ADLS, S3, and Dataverse\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"shortcut.\\nOneLake supports defining permissions for shortcuts such as ADLS, S3, and Dataverse\\nshortcuts. In this case, the permissions are applied on top of the delegated authorization\\nmodel enabled for this type of shortcut.\\nSuppose user1 creates an S3 shortcut in a lakehouse pointing to a folder in an AWS S3 bucket.\\nThen user2 is attempting to access data in this shortcut.\\nDoes S3 connection authorize\\naccess for the delegated user1?\\nDoes OneLake security authorize\\naccess for the requesting user2?\\nResult: Can user2 access\\ndata in S3 Shortcut?\\nYes Yes Yes\\nNo No No\\nNo Yes No\\nYes No No\\n） Important\\nWhen accessing shortcuts through Power BI semantic models using DirectLake over SQL\\nor T-SQL engines in Delegated identity mode, the calling user's identity isn't passed\\nthrough to the shortcut target. The calling item owner's identity is passed instead,\\ndelegating access to the calling user. To resolve this, use Power BI semantic models in\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"delegating access to the calling user. To resolve this, use Power BI semantic models in\\nDirectLake over OneLake mode or T-SQL in User's identity mode.\\nOneLake security in delegated shortcuts\\nﾉ Expand table\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The OneLake security permissions can be defined either for the entire scope of the shortcut or\\nfor selected subfolders. Permissions set on a folder inherit recursively to all subfolders, even if\\nthe subfolder is within the shortcut. Security set on an external shortcut can be scoped to grant\\naccess either to the entire shortcut, or any subpath inside the shortcut. Another internal\\nshortcut pointing to an external shortcut still requires the user to have access to the original\\nexternal shortcut.\\nUnlike other types of access in OneLake security, a user accessing an external shortcut requires\\nFabric Read permission on the data item where the external shortcut resides. This is necessary\\nfor securely resolving the connection to the external system.\\nLearn more about S3, ADLS, and Dataverse shortcuts in OneLake shortcuts.\\nUsers can be members of multiple different OneLake security roles, each one providing its own'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Users can be members of multiple different OneLake security roles, each one providing its own\\naccess to data. The combination of these roles together is called the \"effective role\" and is what\\na user will see when accessing data in OneLake. Roles combine in OneLake security using a\\nUNION or least-restrictive model. This means if Role1 gives access to TableA, and Role2 gives\\naccess to TableB, then the user will be able to see both TableA and TableB.\\nOneLake security roles also contain row and column level security, which limits access to the\\nrows and columns of a table. Each RLS and CLS policy exists within a role and limits access to\\ndata for all users within that single role. For example, if Role1 gives access to Table1, but has\\nRLS on Table1 and only shows some columns of Table1 then the effective role for Role1 is\\ngoing to be the RLS and CLS subsets of Table1. This can be expressed as (R1ols n R1cls n R1rls)\\nwhere n is the INTERSECTION of each component in the role.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"where n is the INTERSECTION of each component in the role.\\nWhen dealing with multiple roles, RLS and CLS combine with a UNION semantic on the\\nrespective tables. CLS is a direct set UNION of the tables visible in each role. RLS is combined\\nacross predicates using an OR operator. For example, WHERE city = 'Redmond' OR city = 'New\\nYork'.\\nTo evaluate multiple roles each with RLS or CLS, each role is first resolved based on the access\\ngiven by the role itself. This means evaluating the INTERSECTION of all object, row, and column\\nlevel security. Each evaluated role is then combined with all other roles a user is a member of\\nvia the UNION operation. The output is the effective role for that user. This can be expressed\\nas:\\n( (R1ols n R1cls n R1rls) u (R2ols n R2cls n R2rls) )\\nLastly, each shortcut in a lakehouse generates a set of inferred roles that are used to propagate\\nthe shortcut target's permissions to the item being queried. Inferred roles operate in a similar\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"the shortcut target's permissions to the item being queried. Inferred roles operate in a similar\\nEvaluating multiple OneLake security roles\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"way to noninferred roles except they're resolved first in place on the shortcut target before\\nbeing combined with roles in the shortcut lakehouse. This ensures that any inheritance of\\npermissions on the shortcut lakehouse is broken and the inferred roles are evaluated correctly.\\nThe full combination logic can then be expressed as:\\n( (R1ols n R1cls n R1rls) u (R2ols n R2cls n R2rls) ) n ( (R1'ols n R1'cls n R1'rls) u\\n(R2'ols n R2'cls n R2'rls)) )\\nWhere R1' and R2' are the inferred roles and R1 and R2 are the shortcut lakehouse roles.\\nIf you assign a OneLake security role to a B2B guest user, you must configure your\\nexternal collaboration settings for B2B in Microsoft Entra External ID. The Guest user\\naccess setting must be set to Guest users have the same access as members (most\\ninclusive).\\nOneLake security doesn't support cross-region shortcuts. Any attempts to access shortcut\\nto data across different capacity regions result in 404 errors.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"to data across different capacity regions result in 404 errors.\\nIf you add a distribution list to a role in OneLake security, the SQL endpoint can't resolve\\nthe members of the list to enforce access. The result is that users appear not to be\\nmembers of the role when they access the SQL endpoint. DirectLake on SQL semantic\\nmodels are subject to this limitation too.\\nTo query data from a Spark notebook using Spark SQL, the user must have at least Viewer\\naccess in the workspace they're querying.\\nMixed-mode queries are not supported. Single queries that access both OneLake security\\nenabled and non-OneLake security enabled data will fail with query errors.\\nSpark notebooks require that the environment be 3.5 or higher and using Fabric runtime\\n1.3.\\nOneLake security doesn't work with private link protection.\\nThe external data sharing preview feature isn't compatible with the data access roles\\npreview. When you enable the data access roles preview on a lakehouse, any existing\\n） Important\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"preview. When you enable the data access roles preview on a lakehouse, any existing\\n） Important\\nIf two roles combine such that the columns and rows aren't aligned across the queries,\\naccess is blocked to ensure that no data is leaked to the end user.\\nOneLake security limitations\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"external data shares might stop working.\\nAzure Mirrored Databricks Catalog does not support Manage Catalog functionality if\\nOneLake security is enabled on that item. This functionality is coming in November, 2025.\\nThe following table provides the limitations of OneLake data access roles.\\nScenario Limit\\nMaximum number of OneLake security roles per Fabric Item 250 roles per lakehouse\\nMaximum number of members per OneLake security role500 users or user groups per role\\nMaximum number of permissions per OneLake security role500 permissions per role\\nChanges to role definitions take about 5 minutes to apply.\\nChanges to a user group in a OneLake security role take about an hour for OneLake to\\napply the role's permissions on the updated user group.\\nSome Fabric engines have their own caching layer, so might require an extra hour to\\nupdate access in all systems.\\nLast updated on 11/18/2025\\nﾉ Expand table\\nLatencies in OneLake security\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Table and folder security in OneLake\\n(preview)\\n10/16/2025\\nTable-level and folder-level security, or object level security (OLS), is a feature of OneLake\\nsecurity (preview) that lets you grant access to specific tables or folders in a data item. Using\\nOLS you create permissions for both structured and unstructured data at the folder level.\\nAn item in Fabric with OneLake security turned on. For more information, see Get started\\nwith OneLake security.\\nSwitch the SQL analytics endpoint on the lakehouse to User's identity mode through the\\nSecurity tab.\\nFor creating semantic models, use the steps to create a DirectLake model.\\nFor a full list of limitations, see the known limitations section.\\nYou can define object-level security on any folder within a data item. Because delta-parquet\\ntables in OneLake are represented as folders, security can also be configured on tables.\\nLikewise, schemas are also folders and can be secured similarly.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Likewise, schemas are also folders and can be secured similarly.\\nUse the following steps to define security roles for tables or folders.\\n1. Navigate to your Lakehouse and select Manage OneLake security (preview).\\n2. Select an existing role that you want to define table or folder security for, or select New\\nto create a new role.\\n3. On the role details page, select Add data. This action opens the data browsing\\nexperience.\\nPrerequisites\\nDefine security rules'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='4. Expand the Tables or Files directories to browse to the items you want to include in the\\nrole.\\nFor tables, you can expand schemas to choose individual tables.\\nFor files, you can expand any number of folders to identify the right items.\\n5. Select the checkbox next to the items you want to grant access to. You can select up to\\n500 items per role.\\n6. Once you have made your selection, select Add data to save your changes and return to\\nthe data in role page'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Your changes to the role are saved automatically.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Column-level security in OneLake (preview)\\n09/08/2025\\nColumn-level security (CLS) is a feature of OneLake security (preview) that allows you to have\\naccess to selected columns in a table instead of full access to the table. CLS lets you specify a\\nsubset of tables that users can access. Any columns that are removed from the list aren't visible\\nto users.\\nAn item in OneLake with OneLake security turned on. For more information, see Get\\nstarted with OneLake data access roles.\\nSwitch the SQL analytics endpoint on the lakehouse to User's identity mode through the\\nSecurity tab.\\nFor creating semantic models, use the steps to create a DirectLake model.\\nFor a full list of limitations, see the known limitations section.\\nOneLake security CLS gets enforced in one of the following two ways:\\nFiltered tables in Fabric engines: Queries to the Fabric engines, like Spark notebooks,\\nresult in the user seeing only the columns they're allowed to see per the CLS rules.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"result in the user seeing only the columns they're allowed to see per the CLS rules.\\nBlocked access to tables: Tables with CLS rules applied to them can't be read outside of\\nsupported Fabric engines.\\nFor filtered tables, the following behaviors apply:\\nCLS rules don't restrict access to users with the Admin, Member, and Contributor roles.\\nIf the CLS rule has a mismatch with the table it's defined on, the query fails and no\\ncolumns are returned. For example, if CLS is defined for a column that isn't part of the\\ntable.\\nQueries of CLS tables fail with an error if a user is part of two different roles and one of\\nthe roles has row-level security (RLS).\\nCLS rules can only be enforced for Delta parquet table objects.\\nCLS rules that are applied to non-Delta table objects block access to the entire table\\nfor members of the role.\\nIf a user runs a select * query on a table where they only have access to some of the\\ncolumns, CLS rules behave differently depending on the Fabric engine.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='columns, CLS rules behave differently depending on the Fabric engine.\\nSpark notebooks: The query succeeds and only shows the allowed columns.\\nPrerequisites\\nEnforce column-level security'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"SQL analytics Endpoint: Column access is blocked for the columns the user can't\\naccess.\\nSemantic models: Column access is blocked for the columns the user can't access.\\nYou can define column-level security as part of a OneLake security role for any Delta-parquet\\ntable in the Tables section of an item. CLS is always specified for a table and is either enabled\\nor disabled. By default, CLS is disabled and users have access to all columns. Users can enable\\nCLS and remove columns from the list to revoke access.\\nUse the following steps to define column-level security:\\n1. Navigate to your data item and select Manage OneLake security (preview).\\n2. Select an existing role that you want to define table or folder security for, or select New\\nto create a new role.\\n3. On the role details page, select more options (...) next to the table you want to define CLS\\nfor, then select Column security (preview).\\nDefine column-level security rules\\n） Important\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"for, then select Column security (preview).\\nDefine column-level security rules\\n） Important\\nRemoving access to a column doesn't deny access to that column if another role grants\\naccess to it.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"4. By default, CLS for a table is disabled. Select Enable CLS or create a New rule to enable it.\\nThe UI populates with a list of columns for that table that the users are allowed to see. By\\ndefault, it shows all of the columns.\\n5. To restrict access to a column, select the checkbox next to the column name, then select\\nRemove. At least one column must remain in the list of allowed columns.\\n6. Select Save to update the role.\\n7. If you want to add a removed column, select New rule. This action adds a new CLS rule\\nentry to the end of the list. Then, use the dropdown to choose the column you want to\\ninclude in the access.\\n8. Once you complete your changes, select Save.\\nBefore you can use OneLake security with SQL analytics endpoint, you must enable its User's\\nidentity mode. Newly created SQL analytics endpoints will default to user's identity mode, so\\nthese steps must be followed for existing SQL analytics endpoints.\\nEnable OneLake security for SQL analytics endpoint\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"1. Navigate to SQL analytics endpoint.\\n2. In the SQL analytics endpoint experience, select the Security tab in the top ribbon.\\n3. Select User's identity under OneLake access mode.\\n4. In the prompt, select Yes, use the user's identity.\\nNow the SQL analytics endpoint is ready to use with OneLake security.\\nRow-level and column-level security can be used together to restrict user access to a table.\\nHowever, the two policies have to be applied using a single OneLake security role. In this\\nscenario, access to data is restricted according to the rules that are set in the one role.\\nOneLake security doesn't support the combination of two or more roles where one contains\\nRLS rules and another contains CLS rules. Users that try to access tables that are part of an\\n７ Note\\nSwitching to User's identity mode only needs to be done once per SQL analytics endpoint.\\nEndpoints that are not switched to user's identity mode will continue to use a delegated\\nidentity to evaluate permissions.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='identity to evaluate permissions.\\nCombine row-level and column-level security'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='unsupported role combination receive query errors.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Row-level security in OneLake (preview)\\n09/08/2025\\nRow-level security (RLS) is a feature of OneLake security (preview) that allows for defining row-\\nlevel data restrictions for tabular data stored in OneLake. Users can define roles in OneLake\\nthat contain rules for filtering rows of data for members of that role. When a member of an RLS\\nrole goes to query that data, the RLS rules are evaluated and only allowed rows are returned.\\nAn item in OneLake with OneLake security turned on. For more information, see Get\\nstarted with OneLake data access roles.\\nSwitch the SQL Analytics Endpoint on the lakehouse to \"User\\'s identity\" mode through\\nthe Security tab.\\nFor creating semantic models, use the steps to create a DirectLake model.\\nFor a full list of limitations, see the known limitations section.\\nOneLake security RLS gets enforced in one of two ways:\\nFiltered tables in Fabric engines: Queries to the list of supported Fabric engines, like'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Filtered tables in Fabric engines: Queries to the list of supported Fabric engines, like\\nSpark notebooks, result in the user seeing only the rows they're allowed to see per the\\nRLS rules.\\nBlocked access to tables: Tables with RLS rules applied to them can't be read outside of\\nsupported Fabric engines.\\nFor filtered tables, the following behaviors apply:\\nRLS rules don't restrict access for users in the Admin, Member, and Contributor roles.\\nIf the RLS rule has a mismatch with the table it's defined on, the query fails and no rows\\nare returned. For example, if the RLS rule references a column that isn't part of the table.\\nQueries of RLS tables fail with an error if a user is part of two different roles and one of\\nthe roles has column-level security (CLS).\\nRLS rules can only be enforced for objects that are Delta parquet tables.\\nRLS rules that are applied to non-Delta table objects instead block access to the entire\\ntable for members of the role.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='table for members of the role.\\nAccess to a table might be blocked if the RLS statement contains syntax errors that\\nprevent it from being evaluated.\\nPrerequisites\\nEnforce row-level security'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You can define row-level security rules as part of any OneLake security role that grants access\\nto table data in Delta parquet format. Rows are a concept only relevant to tabular data, so RLS\\ndefinitions aren\\'t allowed for non-table folders or unstructured data.\\nRLS rules use SQL syntax to specify the rows that a user can see. This syntax takes the form of a\\nSQL SELECT statement with the RLS rules defined in the WHERE clause. RLS rules only support a\\nsubset of the SQL language as defined in Syntax rules. Queries with invalid RLS syntax or RLS\\nsyntax that doesn\\'t match the underlying table result in no rows being shown to users, or query\\nerrors in the SQL analytics endpoint.\\nAs a best practice, avoid using vague or overly complex RLS expressions. Strongly-typed\\nexpressions with integer or string lookups with \"=\" will be the most secure and easy to\\nunderstand.\\nUse the following steps to define RLS rules:\\n1. Navigate to your Lakehouse and select Manage OneLake security (preview).'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Navigate to your Lakehouse and select Manage OneLake security (preview).\\n2. Select an existing role that you want to define table or folder security for, or select New\\nto create a new role.\\n3. On the role details page, select more options (...) next to the table you want to define RLS\\nfor, then select Row security (preview).\\nDefine row-level security rules'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"4. Type the SQL statement for defining which rows you want users to see in the code editor.\\nUse the Syntax rules section for guidance.\\n5. Select Save to confirm the row security rules.\\nBefore you can use OneLake security with SQL analytics endpoint, you must enable its User's\\nidentity mode. Newly created SQL analytics endpoints will default to user's identity mode, so\\nthese steps must be followed for existing SQL analytics endpoints.\\n1. Navigate to SQL analytics endpoint.\\n2. In the SQL analytics endpoint experience, select the Security tab in the top ribbon.\\nEnable OneLake security for SQL analytics endpoint\\n７ Note\\nSwitching to User's identity mode only needs to be done once per SQL analytics endpoint.\\nEndpoints that are not switched to user's identity mode will continue to use a delegated\\nidentity to evaluate permissions.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"3. Select User's identity under OneLake access mode.\\n4. In the prompt, select Yes, use the user's identity.\\nNow the SQL analytics endpoint is ready to use with OneLake security.\\nAll row-level security rules take the following form:\\nSELECT * FROM {schema_name}.{table_name} WHERE {column_level_boolean_1}\\n{column_level_boolean_2}...{column_level_boolean_N}\\nFor example:\\nSELECT * FROM Sales WHERE Amount>'50000' AND State='CA'\\nThe maximum number of characters in a row-level security rule is 1000.\\nPlaceholder Description\\n{schema_name} The name of the schema where {table_name} is located. If the artifact supports\\nschemas, then {schema_name} is required.\\nSyntax rules\\nﾉ Expand table\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Placeholder Description\\n{table_name} The name of the table that the RLS predicate gets applied to. This value must\\nbe an exact match with the name of the table, or the RLS results in no rows\\nbeing shown.\\n{column_level_boolean}A Boolean statement containing the following components:\\n* Column name: The name of a column in {table_name} as specified in the Delta\\nlog schema. Column names can be formatted either as {column_name} or\\n{table_name}.{column_name}.\\n* Operator: One of the Supported operators that evaluates the column name\\nand value to a Boolean value.\\n* Value: A static value or set of values to be evaluated against.\\nYou can have one or more Boolean statements separated by AND or OR.\\nRow-level security rules support the following list of operators and keywords:\\nOperator Description\\n= (equals) Evaluates to true if the two values are the same data type and exact matches.\\n<> (not equals)Evaluates to true if the two values aren't the same data type and not exact matches.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"<> (not equals)Evaluates to true if the two values aren't the same data type and not exact matches.\\n> (greater than)Evaluates to true if the column value is greater than the evaluation value. For string\\nvalues, this operator uses bitwise comparison to determine if one string is greater\\nthan the other.\\n>= (greater than\\nor equal to)\\nEvaluates to true if the column value is greater than or equal to the evaluation value.\\nFor string values, this operator uses bitwise comparison to determine if one string is\\ngreater than or equal to the other.\\n< (less than) Evaluates to true if the column value is less than the evaluation value. For string\\nvalues, this operator uses bitwise comparison to determine if one string is less than\\nthe other.\\n<= (less than or\\nequal to)\\nEvaluates to true if the column value is less than or equal to the evaluation value. For\\nstring values, this operator uses bitwise comparison to determine if one string is less\\nthan or equal to the other.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='than or equal to the other.\\nIN Evaluates to true if any of the evaluation values are the same data type and exactly\\nmatch the column value.\\nSupported operators\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Operator Description\\nNOT Evaluates to true if any of the evaluation values aren't the same data type or not an\\nexact match of the column value.\\nAND Combines the previous statement and the subsequent statement using a Boolean\\nAND operation. Both statements must be true for the entire predicate to be true.\\nOR Combines the previous statement and the subsequent statement using a Boolean OR\\noperation. One of the statements must be true for the entire predicate to be true.\\nTRUE The Boolean expression for true.\\nFALSE The Boolean expression for false.\\nBLANK The blank data type, which can be used with the IS operator. For example, row IS\\nBLANK.\\nNULL The null data type, which can be used with the IS operator. For example, row IS NULL.\\nRow-level and column-level security can be used together to restrict user access to a table.\\nHowever, the two policies have to be applied using a single OneLake security role. In this\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"However, the two policies have to be applied using a single OneLake security role. In this\\nscenario, access to data is restricted according to the rules that are set in the one role.\\nOneLake security doesn't support the combination of two or more roles where one contains\\nRLS rules and another contains CLS rules. Users that try to access tables that are part of an\\nunsupported role combination receive query errors.\\nCombine row-level and column-level security\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Customer-managed keys for Fabric\\nworkspaces\\nMicrosoft Fabric encrypts all data-at-rest using Microsoft managed keys. With customer-\\nmanaged keys for Fabric workspaces, you can use your Azure Key Vault keys to add another\\nlayer of protection to the data in your Microsoft Fabric workspaces - including all data in\\nOneLake. A customer-managed key provides greater flexibility, allowing you to manage its\\nrotation, control access, and usage auditing. It also helps organizations meet data governance\\nneeds and comply with data protection and encryption standards.\\nAll Fabric data stores are encrypted at rest with Microsoft-managed keys. Customer-managed\\nkeys use envelope encryption, where a Key Encryption Key (KEK) encrypts a Data Encryption\\nKey (DEK). When using customer-managed keys, the Microsoft managed DEK encrypts your\\ndata, and then the DEK is encrypted using your customer-managed KEK. Use of a KEK that\\nnever leaves Key Vault allows the data encryption keys themselves to be encrypted and'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"never leaves Key Vault allows the data encryption keys themselves to be encrypted and\\ncontrolled. This ensures that all customer content in a CMK enabled workspace is encrypted\\nusing your customer-managed keys.\\nWorkspace admins can set up encryption using CMK at the workspace level. Once the\\nworkspace admin enables the setting in the portal, all customer content stored in that\\nworkspace is encrypted using the specified CMK. CMK integrates with AKV's access policies\\nand role-based access control (RBAC), allowing you flexibility to define granular permissions\\nbased on your organization's security model. If you choose to disable CMK encryption later,\\nthe workspace will revert to using Microsoft-managed keys. You can also revoke the key at any\\ntime and access to the encrypted data will be blocked within an hour of revocation. With\\nworkspace level granularity and control, you elevate the security of your data in Fabric.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='workspace level granularity and control, you elevate the security of your data in Fabric.\\nCustomer-managed keys are currently supported for the following Fabric items:\\nLakehouse\\nWarehouse\\nNotebook\\nHow customer-managed keys work\\nEnable encryption with customer-managed keys\\nfor your workspace\\nSupported items'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Environment\\nSpark Job Definition\\nAPI for GraphQL\\nML model\\nExperiment\\nPipeline\\nDataflow\\nIndustry solutions\\nSQL Database (preview)\\nThis feature can't be enabled for a workspace that contains unsupported items. When\\ncustomer-managed key encryption for a Fabric workspace is enabled, only supported items can\\nbe created in that workspace. To use unsupported items, create them in a different workspace\\nthat does not have this feature enabled.\\nCustomer-managed key for Fabric workspaces requires an initial setup. This setup includes\\nenabling the Fabric encryption tenant setting, configuring Azure Key Vault, and granting the\\nFabric Platform CMK app access to Azure Key Vault. Once the setup is complete, a user with an\\nadmin workspace role can enable the feature on the workspace.\\nA Fabric administrator needs to make sure that the Apply customer-managed keys setting is\\nenabled. For more information, see Encryption tenant setting article.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='enabled. For more information, see Encryption tenant setting article.\\nFabric uses the Fabric Platform CMK app to access your Azure Key Vault. For the app to work, a\\nservice principal must be created for the tenant. This process is performed by a user that has\\nMicrosoft Entra ID privileges, such as a Cloud Application Administrator.\\nFollow the instructions in Create an enterprise application from a multitenant application in\\nMicrosoft Entra ID to create a service principal for an application called Fabric Platform CMK\\nwith app ID 61d6811f-7544-4e75-a1e6-1c59c0383311 in your Microsoft Entra ID tenant.\\nConfigure encryption with customer-managed keys\\nfor your workspace\\nStep 1: Enable the Fabric tenant setting\\nStep 2: Create a Service Principal for the Fabric Platform CMK\\napp\\nStep 3: Configure Azure Key Vault'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"You need to configure your Key Vault so that Fabric can access it. This step is performed by a\\nuser that has Key Vault privileges, such as a Key Vault Administrator. For more information, see\\nAzure Security roles.\\n1. Open the Azure portal and navigate to your Key Vault. If you don't have Key Vault, follow\\nthe instructions in Create a key vault using the Azure portal.\\n2. In your Key Vault, configure the following settings:\\nSoft delete - Enabled\\nPurge protection - Enabled\\n3. In your Key Vault, open Access control (IAM).\\n4. From the Add dropdown, select Add Role assignment.\\n5. Select the Members tab and then click on Select members.\\n6. In the Select members pane, search for Fabric Platform CMK\\n7. Select the Fabric Platform CMK app and then Select.\\n8. Select the Role tab and search for Key Vault Crypto Service Encryption User or a role that\\nenables get, wrapkey, and unwrap key permissions.\\n9. Select Key Vault Crypto Service Encryption User.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"9. Select Key Vault Crypto Service Encryption User.\\n10. Select Review + assign and then select Review + assign to confirm your choice.\\nTo create an Azure Key Vault key, follow the instructions in Create a key vault using the Azure\\nportal.\\nFabric only supports versionless customer-managed keys, which are keys in the format\\nhttps://{vault-name}.vault.azure.net/{key-type}/{key-name} for Vaults and https://{hsm-\\nname}.managedhsm.azure.net/{key-type}/{key-name} for Managed HSM. Fabric checks the key\\nvault daily for a new version, and uses the latest version available. To avoid having a period\\nwhere you can't access data in the workspace after a new key is created, wait 24 hours before\\ndisabling the older version.\\nKey Vault and Managed HSM must have both soft-delete and purge protection enabled and\\nthe key must be of RSA or RSA-HSM type. The supported key sizes are:\\nStep 4: Create an Azure Key Vault key\\nKey Vault requirements\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"2,048 bit\\n3,072 bit\\n4,096 bit\\nFor more information, see About keys.\\nYou can also use Azure Key Vaults for which the firewall setting is enabled. When you disable\\npublic access to the Key Vault, you can choose the option to 'Allow Trusted Microsoft Services\\nto bypass this firewall.'\\nAfter completing the prerequisites, follow the steps in this section to enable customer-\\nmanaged keys in your Fabric workspace.\\n1. From your Fabric workspace, select Workspace settings.\\n2. From the Workspace settings pane, select Encryption.\\n3. Enable Apply customer-managed keys.\\n4. In the Key identifier field, enter your customer-managed key identifier.\\n5. Select Apply.\\nOnce you complete these steps, your workspace is encrypted with a customer-managed key.\\nThis means that all data in Onelake is encrypted and that existing and future items in the\\nworkspace will be encrypted by the customer-managed key you used for the setup. You can\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"workspace will be encrypted by the customer-managed key you used for the setup. You can\\nreview the encryption status Active, In progress or Failed in the Encryption tab in workspace\\nsettings. Items for which encryption is in progress or failed are listed categorically too. The key\\nneeds to remain active in the Key Vault while encryption is in progress (Status: In progress).\\nRefresh the page to view the latest encryption status. If encryption has failed for some items in\\nthe workspace, you can retry using a different key.\\nTo revoke access to data in a workspace that's encrypted using a customer-managed key,\\nrevoke the key in the Azure Key Vault. Within 60 minutes from the time the key is revoked, read\\nand write calls to the workspace fail.\\n７  Note\\n4,096 bit keys are not supported for SQL database in Microsoft Fabric.\\nStep 5: Enable encryption using customer-managed keys\\nRevoke access\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"You can revoke a customer-managed encryption key by changing the access policy, by\\nchanging the permissions on the key vault, or by deleting the key.\\nTo reinstate access, restore access to the customer-managed key in the Key Vault.\\nTo disable encrypting the workspace using a customer-managed key, go to Workspace settings\\ndisable Apply customer-managed keys. The workspace remains encrypted using Microsoft\\nManaged keys.\\nYou can track encryption configuration requests for your Fabric workspaces by audit log\\nentries. The following operation names are used in audit logs:\\nApplyWorkspaceEncryption\\nDisableWorkspaceEncryption\\nGetWorkspaceEncryption\\nBefore you configure your Fabric workspace with a customer-managed key, consider the\\nfollowing limitations:\\nThe data listed below isn't protected with customer-managed keys:\\nLakehouse column names, table format, table compression.\\nAll data stored in the Spark Clusters (data stored in temp discs as part of shuffle or\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"All data stored in the Spark Clusters (data stored in temp discs as part of shuffle or\\ndata spills or RDD caches in a spark application) are not protected. This includes all the\\n７  Note\\nThe workspace does not automatically revalidate the key for SQL Database in Microsoft\\nFabric. Instead, the user must manually revalidate the CMK to restore access.\\nDisable the encryption\\n７  Note\\nYou can't disable customer-managed keys while encryption for any of the Fabric items in\\nyour workspace is in progress.\\nMonitoring\\nConsiderations and limitations\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Spark Jobs from Notebooks, Lakehouses, Spark Job Definitions, Lakehouse Table Load\\nand Maintenance jobs, Shortcut Transforms, Fabric Materialized View Refresh.\\nThe job logs stored in the history server\\nLibraries attached as part of environments or added as part of the Spark session\\ncustomization using magic commands are not protected\\nMetadata generated when creating a Pipeline and Copy job, such as DB name, table,\\nschema\\nMetadata of ML model and experiment, like the model name, version, metrics\\nWarehouse queries on Object Explored and backend cache, which is evicted after each\\nuse\\nCMK is supported on all F SKUs. Trial capacities cannot be used for encryption using CMK.\\nCMK cannot be enabled for workspaces that have BYOK enabled and CMK workspaces\\ncannot be moved to capacities for which BYOK is enabled either.\\nCMK can be enabled using the Fabric portal and does not have API support.\\nCMK can be enabled and disabled for the workspace while the tenant level encryption'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='CMK can be enabled and disabled for the workspace while the tenant level encryption\\nsetting is on. Once the tenant setting is turned off, you can no longer enable CMK for\\nworkspaces in that tenant or disable CMK for workspaces that already have CMK turned\\non in that tenant. Data in workspaces that enabled CMK before the tenant setting was\\nturned off will remain encrypted with the customer managed key. Keep the associated\\nkey active to be able to unwrap data in that workspace.\\nSecurity fundamentals\\nMicrosoft Fabric licenses\\nLast updated on 11/18/2025\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"What is a OneLake shared access signature\\n(SAS)?\\nArticle• 04/11/2025\\nA OneLake shared access signature (SAS) provides secure, short-term, and delegated access to\\nresources in your OneLake. With a OneLake SAS, you have granular control over how a client\\ncan access your data. For example:\\nWhat resources the client can access.\\nWhat permissions they have to the resources.\\nHow long the SAS is valid.\\nEvery OneLake SAS (and user delegation key) is always backed by a Microsoft Entra identity,\\nhas a maximum lifetime of 1 hour, and can only grant access to folders and files within a data\\nitem, like a lakehouse.\\nA shared access signature is a token appended to the URI for a OneLake resource. The token\\ncontains a special set of query parameters that indicate how the client can access the resource.\\nOne of the query parameters is the signature. It's constructed from the SAS parameters and\\nsigned with the key that was used to create the SAS. OneLake uses this signature to authorize\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"signed with the key that was used to create the SAS. OneLake uses this signature to authorize\\naccess to the folder or file in OneLake. OneLake SAS uses the same format and properties as\\nAzure Storage user-delegated SAS, but with more security restrictions on the lifetime and\\nscope.\\nA OneLake SAS is signed with a user delegation key (UDK), which is backed by a Microsoft\\nEntra credential. You can request a user delegation key with the Get User Delegation Key\\noperation. Then, you use this key (while it's still valid) to build the OneLake SAS. The\\npermissions of that Microsoft Entra credential, along with the permissions explicitly granted to\\nthe SAS, determine the client's access to the resource.\\nWhen a client or application accesses OneLake with a OneLake SAS, the request is authorized\\nusing the Microsoft Entra credentials that requested the UDK used to create the SAS.\\nTherefore, all OneLake permissions granted to that Microsoft Entra identity apply to the SAS,\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Therefore, all OneLake permissions granted to that Microsoft Entra identity apply to the SAS,\\nmeaning a SAS can never exceed the permissions of the user creating it. Furthermore, when\\ncreating a SAS you explicitly grant permissions, letting you provide even more scoped-down\\npermissions to the SAS. Between the Microsoft Entra identity, the explicitly granted\\nHow a shared access signature works\\nAuthorizing a OneLake SAS'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"permissions, and the short-lifetime, OneLake follows security best practices for providing\\ndelegated access to your data.\\nOneLake SAS delegates secure and temporary access to OneLake, backed by a Microsoft Entra\\nidentity. Applications without native Microsoft Entra support can use a OneLake SAS to gain\\ntemporary access to load data without complicated set-up and integration work.\\nOneLake SAS also supports applications serving as proxies between users and their data. For\\nexample, some independent software vendors (ISVs) run between users and their Fabric\\nworkspace, providing extra functionality and possibly a different authentication model. By\\ndelegating access with a OneLake SAS, these ISVs can manage access to the underlying data\\nand provide direct access to data, even if their users don't have Microsoft Entra identities.\\nTwo settings in your Fabric tenant manage the use of OneLake SAS.\\nThe first setting is a tenant-level setting, Use short-lived user-delegated SAS tokens, which\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"The first setting is a tenant-level setting, Use short-lived user-delegated SAS tokens, which\\nmanages the generation of user delegation keys. Because user delegation keys are generated\\nat the tenant-level, they're controlled by a tenant setting. This setting is turned on by default,\\nsince these user delegation keys have equivalent permissions to the Microsoft Entra identity\\nrequesting them and are always short-lived.\\nThe second setting is a delegated workspace setting, Authenticate with OneLake user-\\ndelegated SAS tokens, which controls whether a workspace accepts OneLake SAS. This setting\\nis turned off by default. A workspace admin can turn on this setting to allow authentication\\nwith OneLake SAS in their workspace. A tenant admin can turn this setting on for all\\nworkspaces via the tenant setting, or leave it to workspace admins to turn on.\\nYou can also monitor the creation of user delegation keys in the Microsoft Purview portal. To\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"You can also monitor the creation of user delegation keys in the Microsoft Purview portal. To\\nview all keys generated in your tenant, search for the operation name generateonelakeudk.\\nBecause creating a SAS is a client-side operation, you can't monitor or limit the creation of a\\nOneLake SAS, only the generation of a UDK.\\nWhen to use a OneLake SAS\\nManaging OneLake SAS\\n７  Note\\nTurning off this feature prevents all workspaces from using OneLake SAS, as all users will\\nbe unable to generate user delegation keys.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Always use HTTPS to create or distribute a SAS to protect against man-in-the-middle\\nattacks seeking to intercept the SAS.\\nTrack your, key, and SAS token expiry times. OneLake user delegation keys and SAS\\ntokens have a maximum lifetime of 1 hour. Attempting to request a UDK or build a SAS\\nwith a lifetime longer than 1 hour causes the request to fail. To prevent SAS being used to\\nextend the lifetime of expiring OAuth tokens, the lifetime of the token must also be\\nlonger than the expiry time of the user delegation key and the SAS.\\nBe careful with a SAS token's start time. Setting the start time for a SAS as the current\\ntime might cause failures for the first few minutes, due to differing start times between\\nmachines (clock skew). Setting the start time to be a few minutes in the past helps protect\\nagainst these errors.\\nGrant the least possible privileges to the SAS. Providing the minimum required privileges\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Grant the least possible privileges to the SAS. Providing the minimum required privileges\\nto the fewest possible resources is a security best-practice and lessens the impact if a SAS\\nis compromised.\\nMonitor the generation of user delegation keys. You can audit the creation of user\\ndelegation keys in the Microsoft Purview portal. Search for the operation name\\ngenerateonelakeudk to view keys generated in your tenant.\\nUnderstand the limitations of OneLake SAS. Because OneLake SAS tokens can't have\\nworkspace-level permissions, they aren't compatible with some Azure Storage tools which\\nexpect container-level permissions to traverse data, like Azure Storage Explorer.\\nHow to create a OneLake SAS\\nGenerate a user delegation key\\nFabric and OneLake data security\\nCreate a user delegation SAS for a blob with Python\\nBest practices with OneLake SAS\\nRelated content\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Create a OneLake shared access signature\\nArticle• 05/20/2025\\nYou can create a OneLake shared access signature (SAS) to provide short-term, delegated\\naccess to a folder or file in OneLake backed by your Microsoft Entra credentials. A OneLake SAS\\ncan provide temporary access to applications that don't support Microsoft Entra. These\\napplications can then load data or serve as proxies between other customer applications or\\nsoftware development companies.\\nTo create a OneLake SAS, request a user delegation key by calling the Get User Delegation Key\\noperation. Then, use the key to sign the SAS.\\nA OneLake SAS can grant access to files and folders within data items only. You can't use it for\\nmanagement operations such as creating or deleting workspaces or items.\\nCreating a OneLake SAS is similar to creating an Azure Storage user-delegated SAS. You use\\nthe same parameters for compatibility with tools and applications that work with Azure\\nStorage.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"the same parameters for compatibility with tools and applications that work with Azure\\nStorage.\\nRequesting a user delegation key is a tenant-level operation in Microsoft Fabric. The user or\\nsecurity principal who requests a user delegation key must have at least read permissions in\\none workspace in the Fabric tenant. The requesting user's identity is used to authenticate the\\nSAS, so the user must have permission to the data that they grant the SAS access to.\\nTo get the user delegation key, first request an OAuth 2.0 token from Microsoft Entra ID.\\nAuthorize the call to the Get User Delegation Key operation by providing the bearer token. For\\nmore information about requesting an OAuth token from Microsoft Entra ID, see the article\\nabout authentication flows and application scenarios.\\nCalling the Get User Delegation Key operation returns the key as a set of values that are used\\nas parameters on the user delegation SAS token. These parameters are described in the Get\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='as parameters on the user delegation SAS token. These parameters are described in the Get\\nUser Delegation Key reference and in the next section.\\nRequired permissions\\nAcquire an OAuth 2.0 token\\nRequest the user delegation key\\n７ Note'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"When a client requests a user delegation key by using an OAuth 2.0 token, OneLake returns the\\nkey on behalf of the client. A SAS created with this user delegation key is granted, at most, the\\npermissions granted to the client. But they're scoped down to the permissions explicitly\\ngranted in the SAS.\\nYou can create any number of OneLake SAS tokens for the lifetime of the user delegation key.\\nHowever, a OneLake SAS and user delegation keys can be valid for no more than one hour.\\nThey can't exceed the lifetime of the token that requested them. These lifetime restrictions are\\nshorter than the maximum lifetime of an Azure Storage user delegation SAS.\\nThe following table summarizes the fields that are supported for a OneLake SAS token.\\nSubsequent sections provide more details about these parameters and how they differ from\\nAzure Storage SAS tokens. OneLake doesn't support every optional parameter that Azure\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Azure Storage SAS tokens. OneLake doesn't support every optional parameter that Azure\\nStorage supports. A OneLake SAS constructed with an unsupported parameter will be rejected.\\nSAS field name SAS token\\nparameter\\nStatus Description\\nsignedVersion sv Required This field indicates the version of the\\nstorage service that's used to construct the\\nsignature field. OneLake supports version\\n2020-02-10 and earlier, or version 2020-12-\\n06 and later.\\nsignedResource sr Required This field specifies which resources are\\naccessible via the shared access signature.\\nOnly blob (b) and directory (d) are\\napplicable to OneLake.\\nsignedStart st Optional This field specifies the time when the\\nshared access signature becomes valid. It's\\nin ISO 8601 UTC format.\\nsignedExpiry se Required This field specifies the time when the\\nshared access signature expires.\\nCalling the Get User Delegation Key operation by using a Fabric workload, such as a\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Calling the Get User Delegation Key operation by using a Fabric workload, such as a\\nPython notebook, requires the regional endpoint for OneLake. The capacity region\\ndetermines this endpoint. Otherwise, the received response is 200 Healthy instead of the\\ndelegation key.\\nConstruct a user delegation SAS\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"SAS field name SAS token\\nparameter\\nStatus Description\\nsignedPermissions sp Required This field indicates which operations the\\nSAS can perform on the resource. For more\\ninformation, see the Specify permissions\\nsection.\\nsignedObjectId skoid Required This field identifies a Microsoft Entra\\nsecurity principal.\\nsignedtenantId sktid Required This field specifies the Microsoft Entra\\ntenant in which a security principal is\\ndefined.\\nsignedKeyStartTime skt Optional This field specifies the time in UTC when\\nthe signing key starts. The Get User\\nDelegation Key operation returns it.\\nsignedKeyExpiryTime ske Required This field specifies the time in UTC when\\nthe signing key ends. The Get User\\nDelegation Key operation returns it.\\nsignedKeyVersion skv Required This field specifies the storage service\\nversion that's used to get the user\\ndelegation key. The Get User Delegation\\nKey operation returns it. OneLake supports\\nversion 2020-02-10 and earlier, or version\\n2020-12-06 and later.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"version 2020-02-10 and earlier, or version\\n2020-12-06 and later.\\nsignedKeyService sks Required This field indicates the valid service for the\\nuser delegation key. OneLake supports\\nonly Azure Blob Storage (sks=b).\\nsignature sig Required The signature is a hash-based message\\nauthentication code (HMAC) computed\\nover the string-to-sign and key by using\\nthe SHA256 algorithm, and then encoded\\nby using Base64 encoding.\\nsignedDirectoryDepth sdd Optional This field indicates the number of\\ndirectories within the root folder of the\\ndirectory specified in the\\ncanonicalizedResource field of the string-\\nto-sign. It's supported only when sr=d.\\nsignedProtocol spr Optional OneLake supports only HTTPS requests.\\nsignedAuthorizedObjectId saoid UnsupportedA OneLake SAS doesn't support this\\nfeature.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"SAS field name SAS token\\nparameter\\nStatus Description\\nsignedUnauthorizedObjectId suoid UnsupportedA OneLake SAS doesn't support this\\nfeature.\\nsignedCorrelationId suoid UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nsignedEncryptionScope ses UnsupportedA OneLake SAS doesn't currently support\\ncustom encryption scopes.\\nsignedIP sip UnsupportedA OneLake SAS doesn't currently support\\nIP filtering.\\nCache-Control response\\nheader\\nrscc UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nContent-Disposition\\nresponse header\\nrscd UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nContent-Encoding response\\nheader\\nrsce UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nContent-Language response\\nheader\\nrscl UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nContent Type response\\nheader\\nrsct UnsupportedA OneLake SAS doesn't support this\\nparameter.\\nThe permissions specified in the signedPermissions (sp) field on the SAS token indicate which\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"The permissions specified in the signedPermissions (sp) field on the SAS token indicate which\\noperations a client that possesses the SAS can perform on the resource.\\nPermissions can be combined to permit a client to perform multiple operations with the same\\nSAS. When you construct the SAS, you must include permissions in the following order:\\nracwdxyltmeopi.\\nExamples of valid permission settings include rw, rd, rl, wd, wl, and rl. You can't specify a\\npermission more than once.\\nTo ensure parity with existing Azure Storage tools, OneLake uses the same permission format\\nas Azure Storage. OneLake evaluates the permissions granted to a SAS in signedPermissions,\\nthe permissions of the signing identity in Fabric, and any OneLake data access roles, if\\napplicable.\\nSpecify permissions\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Remember that some operations, such as setting permissions or deleting workspaces, generally\\naren't permitted on OneLake via Azure Storage APIs. Granting that permission (sp=op) doesn't\\nallow a OneLake SAS to perform those operations.\\nPermission URI\\nsymbol\\nResource Allowed operations\\nRead r Directory,\\nblob\\nRead the content, blocklist, properties, and metadata of any\\nblob in the container or directory. Use a blob as the source\\nof a copy operation.\\nAdd a Directory,\\nblob\\nAdd a block to an append blob.\\nCreate c Directory,\\nblob\\nWrite a new blob, snapshot a blob, or copy a blob to a new\\nblob.\\nWrite w Directory,\\nblob\\nCreate or write content, properties, metadata, or a blocklist.\\nSnapshot or lease the blob. Use the blob as the destination\\nof a copy operation.\\nDelete d Directory,\\nblob\\nDelete a blob.\\nDelete versionx Blob Delete a blob version.\\nPermanent\\ndelete\\ny Blob Permanently delete a blob snapshot or version.\\nList l Directory List blobs nonrecursively.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='y Blob Permanently delete a blob snapshot or version.\\nList l Directory List blobs nonrecursively.\\nTags t Blob Read or write the tags on a blob.\\nMove m Directory,\\nblob\\nMove a blob or a directory and its contents to a new\\nlocation.\\nExecute e Directory,\\nblob\\nGet the system properties. If the hierarchical namespace is\\nenabled for the storage account, get the POSIX access\\ncontrol list of a blob.\\nOwnership o Directory,\\nblob\\nSet the owner or owning group. This operation is\\nunsupported in OneLake.\\nPermissions p Directory,\\nblob\\nSet the permissions. This operation is unsupported in\\nOneLake.\\nSet immutability\\npolicy\\ni Blob Set or delete the immutability policy or legal hold on a blob.\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The signature (sig) field is used to authorize a request that a client made with the shared\\naccess signature. The string-to-sign is a unique string that\\'s constructed from the fields that\\nmust be verified to authorize the request. The signature is an HMAC that\\'s computed over the\\nstring-to-sign and key by using the SHA256 algorithm, and then encoded by using Base64\\nencoding.\\nTo construct the signature string of a user delegation SAS:\\n1. Create the string-to-sign from the fields that the request made.\\n2. Encode the string as UTF-8.\\n3. Compute the signature by using the HMAC SHA256 algorithm.\\nThe fields that are included in the string-to-sign must be URL decoded. The required fields\\ndepend on the service version that\\'s used for the authorization (sv) field. The following\\nsections describe the string-to-sign configurations for versions that support OneLake SASs.\\nHTTP\\nSpecify the signature\\nVersion 2020-12-06 and later\\nStringToSign =  signedPermissions + \"\\\\n\" +'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='HTTP\\nSpecify the signature\\nVersion 2020-12-06 and later\\nStringToSign =  signedPermissions + \"\\\\n\" +\\n                signedStart + \"\\\\n\" +\\n                signedExpiry + \"\\\\n\" +\\n                canonicalizedResource + \"\\\\n\" +\\n                signedKeyObjectId + \"\\\\n\" +\\n                signedKeyTenantId + \"\\\\n\" +\\n                signedKeyStart + \"\\\\n\" +\\n                signedKeyExpiry  + \"\\\\n\" +\\n                signedKeyService + \"\\\\n\" +\\n                signedKeyVersion + \"\\\\n\" +\\n                signedAuthorizedUserObjectId + \"\\\\n\" +\\n                signedUnauthorizedUserObjectId + \"\\\\n\" +\\n                signedCorrelationId + \"\\\\n\" +\\n                signedIP + \"\\\\n\" +\\n                signedProtocol + \"\\\\n\" +\\n                signedVersion + \"\\\\n\" +\\n                signedResource + \"\\\\n\" +\\n                signedSnapshotTime + \"\\\\n\" +\\n                signedEncryptionScope + \"\\\\n\" +\\n                rscc + \"\\\\n\" +\\n                rscd + \"\\\\n\" +\\n                rsce + \"\\\\n\" +\\n                rscl + \"\\\\n\" +'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='rscd + \"\\\\n\" +\\n                rsce + \"\\\\n\" +\\n                rscl + \"\\\\n\" +\\n                rsct'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='This configuration applies to version 2020-02-10 and earlier, except for version 2020-01-10\\n(which the next section describes).\\nHTTP\\nHTTP\\nVersion 2020-02-10 and earlier\\nStringToSign =  signedPermissions + \"\\\\n\" +  \\n                signedStart + \"\\\\n\" +  \\n                signedExpiry + \"\\\\n\" +  \\n                canonicalizedResource + \"\\\\n\" +  \\n                signedKeyObjectId + \"\\\\n\" +\\n                signedKeyTenantId + \"\\\\n\" +\\n                signedKeyStart + \"\\\\n\" +\\n                signedKeyExpiry  + \"\\\\n\" +\\n                signedKeyService + \"\\\\n\" +\\n                signedKeyVersion + \"\\\\n\" +\\n                signedAuthorizedUserObjectId + \"\\\\n\" +\\n                signedUnauthorizedUserObjectId + \"\\\\n\" +\\n                signedCorrelationId + \"\\\\n\" +\\n                signedIP + \"\\\\n\" +  \\n                signedProtocol + \"\\\\n\" +  \\n                signedVersion + \"\\\\n\" +  \\n                signedResource + \"\\\\n\" +\\n                rscc + \"\\\\n\" +\\n                rscd + \"\\\\n\" +'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='rscc + \"\\\\n\" +\\n                rscd + \"\\\\n\" +  \\n                rsce + \"\\\\n\" +  \\n                rscl + \"\\\\n\" +  \\n                rsct\\nVersion 2020-01-10\\nStringToSign =  signedPermissions + \"\\\\n\" +\\n                signedStart + \"\\\\n\" +\\n                signedExpiry + \"\\\\n\" +\\n                canonicalizedResource + \"\\\\n\" +\\n                signedKeyObjectId + \"\\\\n\" +\\n                signedKeyTenantId + \"\\\\n\" +\\n                signedKeyStart + \"\\\\n\" +\\n                signedKeyExpiry  + \"\\\\n\" +\\n                signedKeyService + \"\\\\n\" +\\n                signedKeyVersion + \"\\\\n\" +\\n                signedAuthorizedUserObjectId + \"\\\\n\" +\\n                signedUnauthorizedUserObjectId + \"\\\\n\" +\\n                signedCorrelationId + \"\\\\n\" +\\n                signedIP + \"\\\\n\" +\\n                signedProtocol + \"\\\\n\" +\\n                signedVersion + \"\\\\n\" +'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The canonicalizedResource portion of the string is a canonical path to the resource. It must\\ninclude the OneLake endpoint and the resource name, and it must be URL decoded. A OneLake\\npath must include its workspace. A directory path must include the number of subdirectories\\nthat correspond to the sdd parameter.\\nThe following examples show how to convert your OneLake URL to the corresponding\\ncanonicalized resource. Remember that OneLake supports both Distributed File System (DFS)\\nand blob operations and endpoints. The account name for OneLake is always onelake.\\nHTTP\\nHTTP\\nThe following example shows a OneLake SAS URI with a OneLake SAS token appended to it.\\nThe SAS token provides read and write permissions to the Files folder in the lakehouse.\\n                signedResource + \"\\\\n\" +\\n                signedSnapshotTime + \"\\\\n\" +\\n                rscc + \"\\\\n\" +\\n                rscd + \"\\\\n\" +\\n                rsce + \"\\\\n\" +\\n                rscl + \"\\\\n\" +\\n                rsct\\nCanonicalized resource'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='rscl + \"\\\\n\" +\\n                rsct\\nCanonicalized resource\\nBlob file\\nURL = \\nhttps://onelake.blob.fabric.microsoft.com/myWorkspace/myLakehouse.Lakehouse/Files/\\nsales.csv\\ncanonicalizedResource = \\n\"/blob/onelake/myWorkspace/myLakehouse.Lakehouse/Files/sales.csv\"\\nDFS directory\\nURL = \\nhttps://onelake.dfs.fabric.microsoft.com/myWorkspace/myLakehouse.Lakehouse/Files/\\ncanonicalizedResource = \"/blob/onelake/myWorkspace/myLakehouse.Lakehouse/Files/\"\\nOneLake SAS example'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='HTTP\\nCreate a user delegation SAS\\nRequest a user delegation key\\nGet started with OneLake data access roles\\nhttps://onelake.blob.fabric.microsoft.com/myWorkspace/myLakehouse.Lakehouse/Files/\\n?sp=rw&st=2023-05-24T01:13:55Z&se=2023-05-24T09:13:55Z&skoid=<object-id>&sktid=\\n<tenant-id>&skt=2023-05-24T01:13:55Z&ske=2023-05-24T09:13:55Z&sks=b&skv=2022-11-\\n02&sv=2022-11-02&sr=d&sig=<signature>\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake diagnostics\\nOneLake diagnostics provides end-to-end visibility into how data is accessed and used across\\nyour Microsoft Fabric environment. It enables organizations to answer critical questions like\\n\"who accessed what, when, and how,\" supporting data governance, operational insight, and\\ncompliance reporting.\\nWhen enabled at the workspace level, OneLake diagnostics streams data access events as\\nJSON logs into a Lakehouse of your choice within the same capacity. These logs can be easily\\ntransformed into analytics-ready Delta tables, allowing teams to build dashboards and reports\\nthat track usage patterns, top-accessed items, and trends over time.\\nAs all data in Fabric is unified in OneLake, diagnostics at the workspace level provide a\\nconsistent, trustworthy record of data activity—regardless of how or where the data is\\nconsumed. This includes:\\nUser actions in the Fabric web experience\\nProgrammatic access via APIs, pipelines, and analytics engines'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Programmatic access via APIs, pipelines, and analytics engines\\nCross-workspace shortcuts, with events captured from the source workspace\\nThis unified logging approach ensures that even when data is accessed through shortcuts or\\nacross workspaces, visibility is preserved.\\nDiagnostic events are captured for both Fabric and non-Fabric sources. For access through the\\nFabric UI and the Blob or Azure Data Lake Storage (ADLS) APIs, every operation is logged. For\\nFabric workloads access, it records that temporary access was granted, so you can look further\\nin the engine specific logs. This ensures efficient logging while maintaining visibility into how\\ndata is consumed across your organization.\\nSecurity investigation: Track which users accessed sensitive datasets, when, and from\\nwhere. Helps identify unauthorized access attempts or unusual patterns.\\nPerformance troubleshooting: Diagnose latency or failure issues by correlating diagnostic\\nevents with user actions or system interactions.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='events with user actions or system interactions.\\nUsage analytics and optimization: Understand which datasets are most frequently\\naccessed, by whom, and how often. Supports data governance and resource optimization.\\nIntegration monitoring: Monitor external systems interacting with OneLake (via APIs or\\nconnectors), ensuring integrations are functioning as expected and diagnosing issues\\nwhen they arise.\\nExample scenarios supported by OneLake\\ndiagnostics'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='To simplify management and improve access control, consider using a dedicated workspace to\\nstore diagnostic events. If you\\'re enabling diagnostics across multiple workspaces in the same\\ncapacity, consider centralizing logs in a single Lakehouse to make analysis easier.\\nCreate a Lakehouse to store OneLake diagnostic events.\\nThe Lakehouse must reside in the same capacity as the workspaces you want to enable\\ndiagnostics for.\\nIf the workspace uses private links for inbound network protection, it must be within the\\nsame virtual network as the Lakehouse.\\nYou must be a workspace admin for the workspace where you\\'re enabling OneLake\\ndiagnostics, and a contributor to the destination Lakehouse.\\nUse the following steps to enable OneLake diagnostics:\\n1. Open the workspace settings.\\n2. Navigate to the OneLake settings tab.\\n3. Toggle \"Add diagnostic events to a Lakehouse\" to On.\\nConfiguring OneLake diagnostics\\nBest practice recommendations\\nPrerequisites\\nEnabling OneLake diagnostics\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"4. Select the Lakehouse where you want to store the diagnostic events.\\nOneLake diagnostic events can be made immutable, this means that the JSON files that contain\\ndiagnostic events can't be tampered with, or deleted, during the immutability retention period.\\nOneLake diagnostics immutability is built on the Immutable storage for Azure Blob Storage\\ncapability. For more information, please read Store business-critical blob data with immutable\\nstorage in a write once, read many (WORM) state\\nThe immutability period is configured on the workspace that contains diagnostic events. To\\nconfigure the immutability period, you must have previously configured a workspace to store\\ndiagnostic events in this workspace. The immutability period applies to all events stored in this\\nworkspace.\\n1. Enter the required immutability period\\n2. Press apply\\n７ Note\\nIt takes up to 1 hour for diagnostic events to begin flowing into the Lakehouse.\\nEnabling immutable diagnostic logs\\n\\uf80a\\n７ Note\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Enabling immutable diagnostic logs\\n\\uf80a\\n７ Note\\nOnce the immutability policy is applied, the files can't be modified or deleted until the\\nimmutability retention period passes. Use caution while applying the policy as it can't be\\nchanged once set.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Open the workspace settings.\\n2. Go to the OneLake settings tab.\\n3. Select Replace Lakehouse.\\n4. Choose a new Lakehouse.\\n1. Open the workspace settings.\\n2. Navigate to the OneLake settings tab.\\n3. Toggle \"Add diagnostic events to a Lakehouse\" to Off.\\nChanging the OneLake diagnostic Lakehouse\\n７ Note\\nPreviously captured diagnostic events remain in the original Lakehouse. New events are\\nstored in the newly selected Lakehouse.\\nDisabling OneLake diagnostics\\n７ Note\\nThe previously selected Lakehouse is retained. If you re-enable diagnostics, it uses the\\nsame Lakehouse as before.\\nOneLake diagnostic events\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake diagnostic events are stored in the DiagnosticLogs folder within the Files section of a\\nLakehouse. JSON files are written to a folder with the following path:\\nFiles/DiagnosticLogs/OneLake/Workspaces/WorkspaceId/y=YYYY/m=MM/d=DD/h=HH/m=00/PT1H.json\\nThe JSON event contains the following attributes:\\nProperty Description\\nworkspaceId The GUID of the workspace with diagnostics enabled.\\nitemId The GUID of the fabric item, for example the Lakehouse, which is performing the\\nOneLake operation\\nitemType The kind of item that performed the OneLake operation\\ntenantId The tenant identifier that performed the OneLake operation\\nexecutingPrincipalIdThe GUID of the Microsoft Entra principle performing the OneLake operation\\ncorrelationId A GUID correlation identifier for the OneLake operation\\noperationName The OneLake operation being performed (not provided for internal Fabric\\noperations). See Operations below for more details.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='operations). See Operations below for more details.\\noperationCategory The broad category of the OneLake operation (for example, Read)\\nexecutingUPN The Microsoft Entra unique principal name performing the operation (not\\nprovided for internal Fabric operations)\\nexecutingPrincipalType The type of principal being used, for example User or Service Principal\\naccessStartTime The time the operation was performed. When temporary access is provided, the\\ntime temporary access started\\naccessEndTime The time the operation was completed. When temporary access is provided, the\\ntime temporary access completed\\noriginatingApp The workload performing the operation. For external access, then\\noriginatingApp is the user agent string\\nserviceEndpoint The OneLake service endpoint being used (DFS, Blob or Other)\\nResource The resources being accessed (relative to the workspace)\\ncapacityId The identifier of the capacity performing the OneLake operation\\nhttpStatusCode The status code returned to the user'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='httpStatusCode The status code returned to the user\\nisShortcut Indicates if access was performed via a shortcut\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Property Description\\naccessedViaResource The resource the data was accessed via. When a shortcut is used, this is the\\nlocation of the shortcut\\ncallerIPAddress The IP address of the caller\\nOneLake diagnostic events include executingUPN and callerIpAddress. To redact this data,\\ntenant admins can disable the setting \"Include end-user identifiers in OneLake diagnostic logs\"\\nin the Fabric Admin Portal. When disabled, these fields are excluded from new diagnostic\\nevents.\\nIf the Lakehouse selected for diagnostics is deleted:\\nDiagnostics will be automatically disabled for all workspaces that were pointing to it.\\nPreviously captured diagnostic data is not deleted—it remains in the deleted\\nLakehouse\\'s storage until the workspace itself is deleted. To resume diagnostics, select a\\nnew Lakehouse in the same workspace. OneLake will enable diagnostics, and all\\npreviously captured logs remain accessible.\\nIf a workspace is deleted, OneLake diagnostics for that workspace are also deleted.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='If a workspace is deleted, OneLake diagnostics for that workspace are also deleted.\\nIf the workspace is restored, the diagnostic data is restored.\\nOnce the workspace is permanently deleted, the associated diagnostic events are also\\npermanently removed.\\nWhen a workspace is moved to a different capacity, diagnostic logging is disabled.\\nYou must select a new Lakehouse within the new capacity to re-enable diagnostics.\\nPersonal Data\\nFrequently Asked Questions (FAQ)\\nWhat happens if the destination Lakehouse is deleted?\\nWhat happens if the workspace is deleted?\\nWhat happens when you change capacities?\\nWhat happens when BCDR is enabled for the workspace?'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"When Business Continuity and Disaster Recovery (BCDR) is enabled, OneLake\\ndiagnostics data is replicated to the secondary region, and is accessible via the OneLake\\nAPIs if a failover occurs.\\nYes. When workspace monitoring is enabled, disabled, or the Lakehouse is updated, a\\nModifyOneLakeDiagnosticSettings event is captured in the Microsoft 365 security logs,\\nallowing you to audit changes to diagnostic settings.\\nOneLake diagnostics is comparable in cost to Azure Storage diagnostics when emitting to\\na storage account. For the latest details, see the official pricing page: OneLake\\nconsumption – Microsoft Fabric | Microsoft Learn.\\nOneLake diagnostics isn't currently compatible with Workspace outbound access protection\\n(OAP) across workspaces. If you require OneLake diagnostics and OAP to work together, you\\nmust select a Lakehouse in the same Workspace.\\nWhen OneLake diagnostics is configured, the selection of the workspace honors workspace\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"When OneLake diagnostics is configured, the selection of the workspace honors workspace\\nprivate link configuration by limiting your selection to workspaces within the same private\\nnetwork. However, OneLake diagnostics doesn't automatically respond to networking changes.\\nOperation Category\\nReadFileOrGetBlob Read\\nGetFileOrBlobProperties Read\\nGetActionFileOrBlobProperties Read\\nCheckAccessFileOrBlob Read\\nCan you audit OneLake diagnostics?\\nHow much consumption does OneLake diagnostics generate?\\nLimitations\\nOperations\\nGlobal operations\\nﾉ Expand table\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Operation Category\\nDeleteFileOrBlob Delete\\nOperation Category\\nGetBlockList Read\\nListBlob Read\\nGetBlob Read\\nDeleteBlob Delete\\nUndeleteBlob Write\\nGetBlobMetadata Read\\nSetBlobExpiry Write\\nSetBlobMetadata Write\\nSetBlobProperties Write\\nSetBlobTier Write\\nLeaseBlob Write\\nAbortCopyBlob Write\\nPutBlockFromURL Write\\nPutBlock Write\\nPutBlockList Write\\nAppendBlockFromURL Write\\nAppendBlock Write\\nAppendBlobSeal Write\\nPutBlobFromURL Write\\nCopyBlob Write\\nPutBlob Write\\nBlob operations\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Operation Category\\nQueryBlobContents Read\\nGetBlobProperties Read\\nCreateContainer Write\\nDeleteContainer Delete\\nGetContainerMetadata Read\\nGetContainerProperties Read\\nSetContainerMetadata Write\\nSetContainerAcl Write\\nLeaseContainer Write\\nRestoreContainer Write\\nSnapshotBlob Write\\nCreateFastPathReadSession Read\\nCreateFastPathWriteSession Write\\nOperation Category\\nCreateFileSystem Write\\nPatchFileSystem Write\\nDeleteFileSystem Delete\\nGetFileSystemProperties Read\\nCreateDirectory Write\\nCreateFile Write\\nDeleteDirectory Delete\\nDeleteFile Delete\\nRenameFileOrDirectory Write\\nDFS operations\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Operation Category\\nListFilePath Read\\nAppendDataToFile Write\\nFlushDataToFile Write\\nSetFileProperties Write\\nSetAccessControlForFile Write\\nSetAccessControlForDirectory Write\\nLeasePath Write\\nGetPathStatus Read\\nGetAccessControlListForFile Read\\nOperation Category\\nFabricWorkloadAccess Read\\nLast updated on 12/09/2025\\nFabric operations\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Limit inbound requests with inbound\\naccess protection\\n08/21/2025\\nInbound access protection secures connections between your virtual network and Microsoft\\nFabric. By setting up private links, you can prevent access to your data in OneLake from the\\npublic internet.\\nTurning on inbound access protection restricts public access to your Fabric tenant or\\nworkspace. All inbound calls must use an approved private endpoint, either from your own\\nvirtual network or from an approved service such as another Fabric workspace. These private\\nendpoints ensure that connections come only from trusted sources and not the public internet.\\nTo learn more about how to set up private endpoints and block public access to your\\nworkspace, see Fabric workspace private links.\\nTo connect to your workspace over a private endpoint, you need to use the workspace fully\\nqualified domain name (FQDN). This workspace FQDN ensures the call goes directly to the'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='qualified domain name (FQDN). This workspace FQDN ensures the call goes directly to the\\nprivate endpoint, and is specific to each workspace. OneLake supports both Blob and DFS\\n(Data File System) versions of the workspace FQDN:\\nhttps://{workspaceid}.z{xy}.dfs.fabric.microsoft.com\\nhttps://{workspaceid}.z{xy}.blob.fabric.microsoft.com\\\\\\nWhere:\\n{workspaceid} is the workspace GUID without dashes.\\n{xy} is the first two characters of the workspace GUID.\\nTo connect to your tenant private endpoint, you can continue to use the OneLake global\\nFQDN:\\nhttps://onelake.dfs.fabric.microsoft.com\\nWhat is inbound access protection?\\n\\uf80a \\nOneLake and private links'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"https://onelake.blob.fabric.microsoft.com\\nIf your environment doesn't have a workspace private link set up, the workspace FQDN\\nconnects over the public internet. If only a tenant private link is set up, the workspace FQDN\\nconnects to the tenant private link. If both a tenant and workspace private link are set up, the\\nworkspace FQDN connects to the workspace private link.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Limit outbound requests with outbound\\naccess protection\\nOutbound access protection protects data by limiting OneLake's outbound requests made\\nthrough shortcuts and copy operations.\\nOutbound access protection helps ensure that data is shared securely within your network\\nsecurity perimeter. For example, data exfiltration protection solutions use outbound access\\nprotection controls to limit a malicious actor's ability to move large amounts of data to an\\nuntrusted external location. Outbound protections only limit requests that originate in the\\nworkspace and communicate with different workspace or location. A comprehensive network\\nsecurity solution also involves inbound network protection through private links, combined\\nwith data access controls to limit access to your data.\\nTo learn more about managing outbound access protection, see Workspace outbound access\\nprotection.\\nThere are two scenarios where OneLake makes an outbound request: shortcuts and copy\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"protection.\\nThere are two scenarios where OneLake makes an outbound request: shortcuts and copy\\noperations. An outbound request is defined as any request made from within the workspace\\ntowards a location outside the workspace. Only the directionality of the call matters - both\\nreads and writes to external locations can exfiltrate sensitive information to untrusted locations.\\nShortcuts are objects in OneLake that point to other storage locations, which can be internal or\\nexternal to OneLake. The location that a shortcut points to is known as the target path, and the\\nlocation where the shortcut appears is the shortcut path. If the shortcut target is a different\\nworkspace or external storage location than the shortcut path, it's an outbound shortcut and\\nsubject to outbound access protection.\\nOutbound access protection doesn't restrict shortcuts with a source and target within the same\\nworkspace, because all OneLake calls remain within the boundary of the workspace.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='workspace, because all OneLake calls remain within the boundary of the workspace.\\nWhat is outbound access protection?\\nWhen does OneLake make outbound requests?\\nShortcuts'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"When you copy data between two OneLake workspaces using Azure Storage copy APIs,\\nOneLake makes an outbound call from the source workspace to the target workspace. If\\noutbound access protection is enabled on the source workspace, that outbound call is blocked,\\nand the copy operation fails. To allow data movement, you must create a managed private\\nendpoint from the source workspace to the target workspace.\\nThe following copy operation from Workspace A to Workspace B is blocked when outbound\\naccess protection is enabled, unless there's an approved managed private endpoint from\\nWorkspace A to Workspace B. As a reminder, AzCopy operations always following the format\\nazcopy copy <source> <destination>.\\nSyntax\\nAzCopy\\nOutbound access protection doesn't block copy operations that move data within a workspace.\\nWhen you copy data between Azure Storage and OneLake, the direction of the outbound\\nrequests is reversed. The destination account makes an outbound call to the source account.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='requests is reversed. The destination account makes an outbound call to the source account.\\nThis behavior applies to copy operations made directly with Azure Storage Copy Blob from URL\\nand Put Block from URL APIs. It also applies to managed copy experiences with AzCopy and\\nAzure Storage Explorer. Outbound access protection restricts this outbound call from\\ndestination to source. However, this means outbound access protection does not restrict\\nyour workspace from being the source of a copy operation, as no outbound call is made\\nfrom the source workspace.\\nFor example, the following AzCopy sample moves data from the source Azure Storage account\\n\"source\" to the destination lakehouse in OneLake. If Workspace A has outbound protection\\n\\uf80a \\nCopying data within OneLake\\nazcopy copy \\n\"https://onelake.dfs.fabric.microsoft.com/WorkspaceA/LakehouseA.Lakehouse/Files/sale\\ns.csv\" \\n\"https://onelake.dfs.fabric.microsoft.com/WorkspaceB/LakehouseB.Lakehouse/Files/sale'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='s.csv\" \\n\"https://onelake.dfs.fabric.microsoft.com/WorkspaceB/LakehouseB.Lakehouse/Files/sale\\ns.csv\" --trusted-microsoft-suffixes \"fabric.microsoft.com\"\\nCopying data between Azure Storage and OneLake'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"turned on, then the outbound call from Workspace A to the external Azure Storage account is\\nblocked, and the data isn't loaded.\\nSyntax\\nAzCopy\\nHowever, in the following scenario, Workspace A is now the source of the copy operation, with\\nthe external Azure Data Lake Storage (ADLS) account as the destination. In this scenario,\\noutbound access protection does not block this call, as only inbound calls are made to\\nWorkspace A. To restrict these types of operations, see Protect inbound traffic.\\nSyntax\\nAzCopy\\nTo ensure you can continue to read and write data across workspaces, you can create a\\nmanaged private endpoint between workspaces. When a valid managed private endpoint\\nexists from one workspace to another, outbound requests are permitted from the source\\nworkspace to the target workspace even when outbound access is restricted.\\nFor example, creating a managed private endpoint from Workspace A to Workspace B lets\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='For example, creating a managed private endpoint from Workspace A to Workspace B lets\\nusers read data in Workspace B through a shortcut, or copy data from Workspace B to\\nWorkspace A using AzCopy.\\nFabric outbound access protection.\\nFabric inbound access protection.\\nManage inbound access to OneLake with workspace private links.\\nazcopy copy \"https://source.blob.core.windows.net/myContainer/sales.csv\" \\n\"https://onelake.dfs.fabric.microsoft.com/WorkspaceA/LakehouseA.Lakehouse/Files/sale\\ns.csv\" --trusted-microsoft-suffixes \"fabric.microsoft.com\"\\nazcopy copy \\n\"https://onelake.dfs.fabric.microsoft.com/WorkspaceA/LakehouseA.Lakehouse/Files/sale\\ns.csv\" \"https://source.blob.core.windows.net/myContainer/sales.csv\"  --trusted-\\nmicrosoft-suffixes \"fabric.microsoft.com\"\\nCross-workspace operations\\nRelated Content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Last updated on 11/26/2025'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Disaster recovery and data protection for\\nOneLake\\nArticle• 05/20/2025\\nAll data in OneLake is accessed through data items. These data items can reside in different\\nregions depending on their workspace, because a workspace is created under a capacity that's\\ntied to a specific region.\\nOneLake uses zone-redundant storage (ZRS) where that storage type is available. (See Azure\\nregions with availability zones.) Elsewhere, OneLake uses locally redundant storage (LRS). With\\nboth LRS and ZRS, your data is resilient to transient hardware failures within a datacenter.\\nJust like for Azure Storage, LRS replicates data within a single datacenter in the primary region.\\nLRS provides at least 99.999999999% (11 nines) durability of objects over a year. This durability\\nhelps protect against server rack and drive failures, but not against datacenter disasters.\\nMeanwhile, ZRS provides fault tolerance to datacenter failures by copying data synchronously\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Meanwhile, ZRS provides fault tolerance to datacenter failures by copying data synchronously\\nacross three Azure availability zones in the primary region. ZRS offers a durability of at least\\n99.9999999999% (12 nines) over a year.\\nThis article provides guidance on how to further protect your data from rare region-wide\\noutages.\\nYou can enable or disable business continuity and disaster recovery (BCDR) for a specific\\ncapacity through the capacity admin portal. If your capacity has BCDR activated, your data is\\nduplicated and stored in two geographic regions so that it's geo-redundant. The standard\\nregion pairings in Azure determine the choice of the secondary region. You can't modify the\\nsecondary region.\\nIf a disaster makes the primary region unrecoverable, OneLake might initiate a regional\\nfailover. After the failover finishes, you can use the OneLake APIs through the global endpoint\\nto access your data in the secondary region. Data replication to the secondary region is\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='to access your data in the secondary region. Data replication to the secondary region is\\nasynchronous, so any data not copied during the disaster is lost. After a failover, the new\\nprimary datacenter has local redundancy only.\\nFor a comprehensive understanding of the end-to-end experience, see Reliability in Microsoft\\nFabric.\\nDisaster recovery\\nSoft deletion for OneLake files'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"In OneLake, soft deletion prevents accidental file loss by retaining deleted files for seven days\\nbefore permanent removal. Soft-deleted data is billed at the same rate as active data.\\nYou can restore files and folders by using Azure Blob Storage REST APIs, Azure Storage SDKs,\\nand the Azure PowerShell Az.Storage module. Learn how to list and restore files by using these\\nPowerShell instructions and how to connect to OneLake with PowerShell.\\nYou can restore deleted Lakehouse files by using Azure Storage Explorer. First, connect to your\\nworkspace from Storage Explorer by using the workspace ID in the URL. For example, use\\nhttps://onelake.dfs.fabric.microsoft.com/aaaaaaaa-0000-1111-2222-bbbbbbbbbbbb. You can\\nfind the workspace ID from the Microsoft Fabric portal's browser URL (/groups/{workspaceID}).\\nEnsure that you use the GUID-based OneLake path to restore data.\\nAfter you connect to your workspace, follow these steps to restore soft-deleted data:\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='After you connect to your workspace, follow these steps to restore soft-deleted data:\\n1. Select the dropdown button next to the path bar, and then select Active and soft deleted\\nblobs instead of the default Active blobs.\\n2. Go to the folder that contains the soft-deleted file.\\n3. Right-click the file, and then select Undelete.\\nOneLake compute and storage consumption\\nRestore soft-deleted files via Azure Storage Explorer\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Use OneLake file explorer (preview) to\\naccess Fabric data\\n07/26/2025\\nThe OneLake file explorer application seamlessly integrates OneLake with Windows File\\nExplorer. This application automatically syncs all OneLake items that you have access to in\\nWindows File Explorer. \"Sync\" refers to pulling up-to-date metadata on files and folders, and\\nsending changes made locally to the OneLake service. Syncing doesn’t download the data, it\\ncreates placeholders. You must double-click on a file to download the data locally.\\nWhen you create, update, or delete a file via Windows File Explorer, it automatically syncs the\\nchanges to OneLake service. Updates to your item made outside of your File Explorer aren\\'t\\nautomatically synced. To pull these updates, you need to right-click on the item or subfolder in\\nWindows File Explorer and select OneLake > Sync from OneLake.\\nOneLake file explorer currently supports Windows and is validated on Windows 10 and 11.\\nTo install:\\n1. Download the OneLake file explorer.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='To install:\\n1. Download the OneLake file explorer.\\n2. Double-click the file to start installing.\\n\\uf80a\\n） Important\\nThis feature is in preview.\\nInstallation instructions'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The storage location on your PC for the placeholders and any downloaded content is\\n\\\\%USERPROFILE%\\\\OneLake - Microsoft\\\\.\\nOnce the application is installed and running, you can see your OneLake data in Windows File\\nExplorer.\\nStarting in version 1.0.13, the OneLake file explorer app notifies you when a new update is\\navailable. When a new version becomes available, you receive a Windows notification and the\\nOneLake icon changes. Right-click on the OneLake icon in the Windows notification area.\\nSelect Update Available and follow steps to update.\\nWorkspace names with the \"/\" character, encoded escape characters such as %23, and\\nnames that look like GUIDs fail to sync.\\nFiles or folders containing Windows reserved characters fail to sync. For more\\ninformation, see Naming files, paths, and namespaces.\\nIf Windows search is disabled, OneLake file explorer fails to start.\\nWindows File Explorer is case insensitive, while OneLake is case sensitive. You can create'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Windows File Explorer is case insensitive, while OneLake is case sensitive. You can create\\nfiles with the same name but different cases in the OneLake service using other tools, but\\nWindows File Explorer only shows the oldest of these files.\\nIf a file fails to sync due to a network issue, you have to trigger the sync to OneLake. To\\nprompt the sync process, open the file and save it. Alternatively, you can trigger a modify\\nevent using PowerShell by executing this command: (Get-Item -Path \"\\n<file_path>\").LastWriteTimeUtc = Get-Date\\nOneLake File Explorer does not support environments that require network proxy\\nconfigurations. Attempts to launch or authenticate the application behind a proxy may\\nresult in connection failures or sign-in issues.\\nOneLake File Explorer does not support syncing files marked as read-only. This limitation\\napplies specifically to files marked as read-only by the user on their local machine. This'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='applies specifically to files marked as read-only by the user on their local machine. This\\nbehavior is by design to prevent conflicts with local file system permissions and to avoid\\nunintended edits to protected content\\nThe following scenarios provide details for working with the OneLake file explorer.\\nLimitations and considerations\\nScenarios'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake file explorer starts automatically at startup of Windows. You can disable the\\napplication from starting automatically by selecting Startup apps in Windows Task Manager\\nand then right-clicking OneLake, and selecting Disable.\\nTo manually start the application, search for OneLake using Windows search (Windows+S)\\nand select the OneLake application. The views for any folders that were previously synced\\nrefresh automatically.\\nTo exit, right-click on the OneLake icon in the Windows notification area, located at the\\nfar right of the taskbar, and select Exit. The sync pauses, and you can't access placeholder\\nfiles and folders. You continue to see the blue cloud icon for placeholders that were\\npreviously synced but not downloaded.\\nTo optimize performance during the initial sync, OneLake file explorer syncs the placeholder\\nfiles for the top-level workspaces and item names. When you open an item, OneLake file\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"files for the top-level workspaces and item names. When you open an item, OneLake file\\nexplorer syncs the files directly in that folder. Then, opening a folder within the item syncs the\\nfiles directly in that folder. This functionality allows you to navigate your OneLake content\\nseamlessly, without having to wait for all files to sync before starting to work.\\nWhen you create, update, or delete a file via OneLake file explorer, it automatically syncs the\\nchanges to OneLake service. Updates to your item made outside of your OneLake file explorer\\naren't automatically synced. To pull these updates, right-click on the workspace name, item\\nname, folder name, or file in OneLake file explorer and select OneLake > Sync from OneLake.\\nThis action refreshes the view for any folders that were previously synced. To pull updates for\\nall workspaces, right-click on the OneLake root folder and select OneLake > Sync from\\nOneLake.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='all workspaces, right-click on the OneLake root folder and select OneLake > Sync from\\nOneLake.\\nStarting in version 1.0.9.0, when you install OneLake file explorer, you can choose which\\naccount to sign in with. To switch accounts, right-click the OneLake icon in the Windows\\nnotification area, select Account, and then Sign Out. Signing out exits OneLake file explorer\\nand pauses the sync. To sign in with another account, start OneLake file explorer again and\\nchoose the desired account.\\nWhen you sign in with another account, you see the list of workspaces and items refresh in\\nOneLake file explorer. If you navigate to workspaces associated with the previous account, you\\nStart and exit OneLake file explorer\\nSync updates from OneLake\\nSign in to different accounts'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"can manually refresh the view by right-clicking the workspace, then selecting OneLake > Sync\\nfrom OneLake. Those workspaces are inaccessible while you're signed in to a different account.\\nStarting in version 1.0.10.0, you can transition between using OneLake file explorer and the\\nFabric web portal. From OneLake file explorer, right-click on a workspace and select OneLake >\\nView workspace online. This action opens the workspace browser on the Fabric web portal.\\nYou can right-click on an item, subfolder, or file and select OneLake > View item online. This\\naction opens the item browser on the Fabric web portal. If you select a subfolder or file, the\\nFabric web portal always opens the root folder of the item.\\nThe OneLake file explorer only syncs updates when you're online and the application is\\nrunning. When the application starts, the views for any folders that were previously synced\\nrefresh automatically. Any files that you added or updated while offline show as sync pending\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"refresh automatically. Any files that you added or updated while offline show as sync pending\\nuntil you save them again. Any files that you deleted while offline are recreated during the\\nrefresh if they still exist on the service.\\n1. Navigate to the OneLake section in Windows File Explorer.\\n2. Navigate to the appropriate folder in your item.\\n3. Right-click and select New folder or New file type.\\n1. Navigate to the OneLake section in Windows File Explorer.\\n2. Navigate to the Files or Tables folder in your item.\\nOption to open workspaces and items on the web portal\\nOffline support\\nCreate files or folders in OneLake file explorer\\n７ Note\\nOneLake sync fails if you write data to locations where you don't have write permission,\\nsuch as the root of the item or workspace. Clean up files or folders that fail to sync by\\nmoving them to the correct location or deleting them.\\nDelete files or folders in OneLake file explorer\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"3. Select a file or folder and delete.\\nYou can open files using your favorite apps and make edits. Select Save to sync the file to\\nOneLake. Starting in version 1.0.11, you can also make updates with Excel to your files. Close\\nthe file after the update in Excel and it initiates the sync to OneLake.\\nIf you edit a file locally and select Save, the OneLake file explorer app detects if that file was\\nupdated elsewhere (by someone else) since you last selected Sync from OneLake. A Confirm\\nthe action dialog box appears:\\nIf you select Yes, your local changes overwrite any other changes made to the file since the last\\ntime you selected Sync from OneLake.\\nIf you select No, the local changes aren't sent to the OneLake service. You can then select Sync\\nfrom OneLake to revert your local changes and pull the file from the service. Or you can copy\\nthe file with a new name to avoid conflicts.\\nYou can copy files to, from, and within your items using standard keyboard shortcuts like\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You can copy files to, from, and within your items using standard keyboard shortcuts like\\nCtrl+C and Ctrl+V. You can also move files by dragging and dropping them.\\nWhen you upload or download files using the OneLake file explorer, the performance should\\nbe similar to using OneLake APIs. In general, the time it takes to sync changes from OneLake is\\nproportional to the number of files.\\nEdit files\\nCopy or move files\\nSupport for large files and a large number of files\\nOneLake shortcut support'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='All folders in your items including OneLake shortcuts are visible. You can view, update, and\\ndelete the files and folders in those shortcuts.\\nStarting in version 1.0.10, you can find your client-side logs by right-clicking on the OneLake\\nicon in the Windows notification area, located at the far right of the taskbar. Select Diagnostic\\noperations > Open logs folder. This action opens your logs directory in a new Windows file\\nexplorer window.\\nClient-side logs are stored on your local machine under %temp%\\\\OneLake\\\\Diagnostics\\\\.\\nYou can enable more client-side logging by selecting Diagnostic operations > Enable tracing.\\nStarting in version 1.0.11, you can find information about each release of the OneLake file\\nexplorer by right-clicking on the OneLake icon in the Windows notification area, located at the\\nfar right of the taskbar. Select About > Release Notes. This action opens the OneLake file\\nexplorer release notes page in your browser window.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"explorer release notes page in your browser window.\\nTo uninstall the app, search for OneLake in Windows. Select Uninstall in the list of options\\nunder OneLake.\\nTenant admins can restrict access to OneLake file explorer for their organization in the\\nMicrosoft Fabric admin portal. When the setting is disabled, no one in your organization can\\nstart the OneLake file explorer app. If the application is already running and the tenant admin\\ndisables the setting, the application exits. Placeholders and any downloaded content remain on\\nlocal machines, but users can't sync data to or from OneLake.\\nThese OneLake file explorer icons appear in Windows File Explorer to indicate the sync state of\\nthe file or folder.\\nClient-side logs\\nRelease Notes\\nUninstall instructions\\nTenant setting enables access to OneLake file explorer\\nOneLake file explorer icons\\nﾉ Expand table\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Icon Icon\\ndescription\\nMeaning\\nBlue cloud\\nicon\\nThe file is only available online. Online-only files don’t take up space on your\\ncomputer.\\nGreen tick The file is downloaded to your local computer.\\nSync pending\\narrows\\nSync is in progress. This icon might appear when you're uploading files. If the sync\\npending arrows are persistent, then your file or folder might have an error\\nsyncing. You can find more information in the client-side logs on your local\\nmachine under %temp%\\\\OneLake\\\\Diagnostics\\\\.\\nLearn more about Fabric and OneLake security.\\nWhat's new in the latest OneLake file explorer?\\nRelated content\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"What's new in the latest OneLake file\\nexplorer?\\n09/02/2025\\nContinue reading for information on major updates to OneLake file explorer.\\nOneLake File Explorer has been migrated to .NET 8, ensuring the application remains aligned\\nwith Microsoft’s latest supported frameworks. This upgrade enhances security and ensures\\ncontinued eligibility for long-term support.\\nTemporary files (.tmp) created during file edits— often created during edits in applications like\\nMicrosoft Excel —will no longer get stuck in the sync pending state. This issue typically\\noccurred when there were network connectivity problems, preventing .tmp files from\\nuploading and causing conflicts with the server. These files are now cleaned up automatically\\nonce the network connection is restored, eliminating persistent sync indicators on temporary\\nfiles.\\nWe’ve resolved a memory access violation that previously caused crashes when syncing files\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='files.\\nWe’ve resolved a memory access violation that previously caused crashes when syncing files\\nthat had not been opened before. This fix improves reliability and prevents unexpected\\nshutdowns during background sync operations.\\n）  Important\\nThis feature is in preview.\\nSeptember 2025 Update (v 1.0.14.0)\\n.NET 8 Migration\\nSmarter sync on temporary files\\nStability improvements\\nApril 2024 Update\\nv 1.0.13.0 - Update Notifications'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='We believe that staying informed about app updates is crucial. Whether it’s a bug fix,\\nperformance improvement, or exciting new features. Starting with this version, the OneLake file\\nexplorer app will now notify you when a new update is available. You’ll receive a Windows\\nnotification when a new version is available and the OneLake icon in the Windows notification\\narea will change. Simply right-click the icon to see if an update is available.\\nThis release includes minor internal fixes to enhance functionality.\\nWith this release users can make edits and updates with Excel to OneLake files, similar to the\\nexperience with OneDrive. Start by opening a csv or xlsx file using Excel, make updates and\\nclose the file. Closing the file will initiate the sync to OneLake. You can then view the updated\\nfile online in the Fabric web portal. This enhancement aims to streamline your workflow and\\nprovide a more intuitive approach to managing and editing your files with Excel.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"provide a more intuitive approach to managing and editing your files with Excel.\\nWith this menu option, you can easily find details about what's new in the latest OneLake File\\nExplorer version. Right-click on the OneLake icon in the Windows notification area, located at\\nthe far right of the taskbar, and select About > Release Notes. This action opens the OneLake\\nfile explorer release notes page in your browser window.\\nOneLake file explorer will default to the latest TLS version supported by Windows, currently TLS\\n1.3. Support for TLS 1.3 is recommended for maintaining the security and privacy of data\\nexchanged over the internet.\\nNow you can seamlessly transition between using OneLake file explorer and the Fabric web\\nportal. Browse your OneLake data using OneLake file explorer, right-click on a workspace, and\\nv 1.0.12.0 - Internal update\\nDecember 2023 Update (v 1.0.11.0)\\nAbility to update files using Excel\\nMenu option to view Release Notes\\nTLS 1.3 support\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Ability to update files using Excel\\nMenu option to view Release Notes\\nTLS 1.3 support\\nSeptember 2023 Update (v 1.0.10.0)\\nOption to open workspaces and items on the web portal'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='select OneLake > View Workspace Online. This action opens the workspace browser on the\\nFabric web portal.\\nIn addition, you can right-click on an item, subfolder, or file and select OneLake > View Item\\nOnline. This action opens the item browser on the Fabric web portal. If you select a subfolder\\nor file, the Fabric web portal always opens the root folder of the item.\\nWith this menu option, you can now easily find your client-side logs, which can help you\\ntroubleshoot issues. Right-click on the OneLake icon in the Windows notification area, located\\nat the far right of the taskbar, and select Select Diagnostic Operations > Open logs directory.\\nThis action opens your logs directory in a new Windows file explorer window.\\nWhen you install OneLake file explorer, you can now choose which account to sign-in with. To\\nswitch accounts, right-click the OneLake icon in the Windows notification area, select Account,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='switch accounts, right-click the OneLake icon in the Windows notification area, select Account,\\nand then select Sign Out. Signing out exits OneLake file explorer and pauses the sync. To sign\\nin with another account, start OneLake file explorer again by searching for \"OneLake\" using\\nWindows search (Windows + S) and select the OneLake application. Previously, when you\\nstarted OneLake file explorer, it automatically used the Microsoft Entra ID currently logged into\\nWindows to sync Fabric workspaces and items.\\nWhen you sign in with another account, you see the list of workspaces and items refresh in\\nOneLake file explorer. If you continue to workspaces associated with the previous account, you\\ncan manually refresh the view by selecting Sync from OneLake. Those workspaces are\\ninaccessible while you\\'re signed into a different account.\\nNow when you move a folder (cut and paste or drag and drop) from a location outside of'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Now when you move a folder (cut and paste or drag and drop) from a location outside of\\nOneLake to OneLake, the contents sync to OneLake successfully. Previously, you had to trigger\\na sync by either opening the files and saving them or moving them back out of OneLake and\\nthen copying and pasting (versus moving).\\nMenu option to open logs directory\\nJuly 2023 Update (v 1.0.9.0)\\nOption to sign in to different accounts\\nFix known issue during folder moves from outside of OneLake\\nto OneLake'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Only the most recent version of OneLake file explorer is supported. If you contact support for\\nOneLake file explorer, they ask you to upgrade to the most recent version. Download the latest\\nOneLake file explorer.\\nUse OneLake file explorer to access Fabric data\\nSupport lifecycle\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Get the size of OneLake items\\nArticle• 05/09/2025\\nLearn how to get the size of your OneLake data to manage and plan storage costs. Capacity\\nadmins can use the Microsoft Fabric Capacity Metrics app to find the total size of OneLake data\\nstored in a given capacity or workspace. But you might want a way to measure size at a more\\ngranular level.\\nThis article describes Azure Storage PowerShell commands that you can use to understand the\\nsize of data in a specific item or folder. Because OneLake is compatible with Azure Data Lake\\nStorage (ADLS) tools, many of the commands work by just replacing the ADLS Gen2 URL with a\\nOneLake URL.\\nTo automate the steps in this article, use REST API commands to get the workspace and item\\ninformation instead of providing them manually. For more information, see List workspaces\\nand List items.\\nAzure PowerShell. For more information, see How to install Azure PowerShell\\nThe Azure Storage PowerShell module.\\nPowerShell'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The Azure Storage PowerShell module.\\nPowerShell\\nSign in to PowerShell with your Azure account.\\nPowerShell\\nEach time you run an Azure Storage command against OneLake, you need to include the -\\nContext parameter with an Azure Storage context object. To create a context object that points\\nto OneLake, run the New-AzStorageContext command with the following values:\\nPrerequisites\\nInstall-Module Az.Storage -Repository PSGallery -Force\\nConnect-AzAccount\\nCreate a context object for OneLake\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Parameter Value\\n-StorageAccountName 'onelake'\\n-UseConnectedAccount None; instructs the cmdlet to use your Azure account.\\n-Endpoint 'fabric.microsoft.com'\\nFor ease of reuse, create this context as a local variable:\\nPowerShell\\nTo get an item size, use the Get-AzDataLakeGen2ChildItem command with the following\\nvalues:\\nParameter Value\\n-Context An Azure Storage context object. For more information, see Create a context object for\\nOneLake.\\n-FileSystem Fabric workspace name or GUID. Azure Storage naming criteria for containers only\\nsupports lowercase letters, numbers, and hyphens. If you have any other characters in\\nyour workspace name, use its GUID instead.\\n-Path Local path to the item or folder inside the workspace. Azure Storage naming criteria for\\ncontainers only supports lowercase letters, numbers, and hyphens. If you have any other\\ncharacters in any resources in your item path, use the equivalent GUID instead.\\n-Recurse None; instructs the cmdlet to recursively get the child item.\\n-\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"-Recurse None; instructs the cmdlet to recursively get the child item.\\n-\\nFetchProperty\\nNone; instructs the cmdlet to fetch the item properties.\\nUse a pipeline to pass the output of the Get-AzDataLakeGen2ChildItem command to the\\nMeasure-object command with the following values:\\n$ctx = New-AzStorageContext -StorageAccountName 'onelake' -UseConnectedAccount -\\nendpoint 'fabric.microsoft.com'\\nGet the size of an item or folder\\nﾉ Expand table\\nﾉ Expand table\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Parameter Value\\n-Property Length\\n-Sum None; indicates that the cmdlet displays the sum of the values of the specified property.\\nCombined, the full command looks like the following example:\\nPowerShell\\nPowerShell\\nPowerShell\\nPowerShell\\nGet-AzDataLakeGen2ChildItem -Context <CONTEXT_OBJECT> -FileSystem <WORKSPACE_NAME> \\n-Path <ITEM_PATH> -Recurse -FetchProperty | Measure-Object -property Length -sum\\nExample: Get the size of an item\\n$ctx = New-AzStorageContext -StorageAccountName \\'onelake\\' -UseConnectedAccount -\\nendpoint \\'fabric.microsoft.com\\'\\n$workspaceName = \\'myworkspace\\'\\n$itemPath = \\'mylakehouse.lakehouse\\'\\n$colitems = Get-AzDataLakeGen2ChildItem -Context $ctx -FileSystem $workspaceName -\\nPath $itemPath -Recurse -FetchProperty | Measure-Object -property Length -sum\\n\"Total file size: \" + ($colitems.sum / 1GB) + \" GB\"\\nExample: Get the size of a folder\\n$ctx = New-AzStorageContext -StorageAccountName \\'onelake\\' -UseConnectedAccount -\\nendpoint \\'fabric.microsoft.com\\''),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='endpoint \\'fabric.microsoft.com\\'\\n$workspaceName = \\'myworkspace\\'\\n$itemPath = \\'mylakehouse.lakehouse/Files/folder1\\'\\n$colitems = Get-AzDataLakeGen2ChildItem -Context $ctx -FileSystem $workspaceName -\\nPath $itemPath -Recurse -FetchProperty | Measure-Object -property Length -sum\\n\"Total file size: \" + ($colitems.sum / 1GB) + \" GB\"\\nExample: Get the size of a table with GUIDs\\n$ctx = New-AzStorageContext -StorageAccountName \\'onelake\\' -UseConnectedAccount -\\nendpoint \\'fabric.microsoft.com\\'\\n$workspaceName = \\'aaaaaaaa-0000-1111-2222-bbbbbbbbbbbb\\'\\n$itemPath = \\'bbbbbbbb-1111-2222-3333-cccccccccccc/Tables/table1\\''),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='These PowerShell commands don’t work on shortcuts that point to ADLS containers directly.\\nInstead, we recommended that you create ADLS shortcuts to directories that are at least one\\nlevel below a container.\\n$colitems = Get-AzDataLakeGen2ChildItem -Context $ctx -FileSystem $workspaceName -\\nPath $itemPath -Recurse -FetchProperty | Measure-Object -property Length -sum\\n\"Total file size: \" + ($colitems.sum / 1GB) + \" GB\"\\nLimitations'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake catalog overview\\n10/23/2025\\nOneLake catalog is a centralized place that helps you find, explore, and use the Fabric items\\nyou need, and govern the data you own. It features two tabs:\\nExplore tab: The explore tab has an items list with an in-context item details view that\\nmakes it possible to browse through and explore items without losing your list context. It\\nalso provides selectors and filters to narrow down and focus the list, making it easier to\\nfind what you need. By default, the OneLake catalog opens on the Explore tab. Along with\\nthe OneLake catalog, you can open and work across multiple workspaces side by side\\nusing the object explorer.\\nGovern tab: The govern tab provides insights that help you understand the governance\\nposture of all the data you own in Fabric, and presents recommended actions you can\\ntake to improve the governance status of your data.\\nSecure tab: The OneLake catalog Secure tab centralizes security management in'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Secure tab: The OneLake catalog Secure tab centralizes security management in\\nMicrosoft Fabric by providing a unified view of workspace roles and OneLake security\\nroles across items. It enables admins to audit permissions, view user access, and create,\\nedit, or delete security roles from a single location, ensuring streamlined governance and\\nconsistent enforcement of data access policies.\\nTo open the OneLake catalog, select the OneLake icon in the Fabric navigation pane. Select the\\ntab you're interested if it's not displayed by default.\\nOpen the OneLake catalog\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Discover and explore Fabric items in the OneLake catalog\\nView item details\\nGovern your data in Fabric\\nEndorsement\\nFabric domains\\nLineage in Fabric\\nMonitor hub\\n\\uf80a \\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake compute and storage\\nconsumption\\n10/07/2025\\nOneLake usage is defined by data stored and the number of transactions. For OneLake security,\\ncapacity usage is based on the number of rows in the table being secured. This page contains\\ninformation on how all of OneLake usage is billed and reported.\\nOneLake storage is billed at a pay-as-you-go rate per GB of data used and doesn't consume\\nFabric Capacity Units (CUs). Fabric items like lakehouses and warehouses consume OneLake\\nstorage. For Mirroring storage, data up to a certain limit is free based on the purchased\\ncompute capacity SKU you provision. For more information about pricing, see Fabric pricing.\\nFor native mirrored storage, OneLake storage isn't billed as it's included in the cost of items\\nlike Power BI import semantic models and Fabric SQL database.\\nYou can visualize your OneLake storage usage in the Fabric Capacity Metrics app in the Storage\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='You can visualize your OneLake storage usage in the Fabric Capacity Metrics app in the Storage\\ntab. Also note that soft-deleted data is billed at the same rate as active data. For more\\ninformation about monitoring usage, see the Metrics app Storage page. To understand\\nOneLake consumption more, see the OneLake Capacity Consumption page\\nRequests to OneLake, such as reading or writing data, consume Fabric Capacity Units. The rates\\nin this page define how much capacity units are consumed for a given type of operation.\\nOneLake uses the same mappings as Azure Data Lake Storage (ADLS) to classify the operation\\nto the category.\\nOneLake supports two access paths: redirect and proxy. Applications will use one of these\\npaths based on specific circumstances, some determined by the workload and others outside\\nits control. Requests via proxy and redirect share the same consumption rate.\\nThis table defines CU consumption when OneLake data is accessed using applications that'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='This table defines CU consumption when OneLake data is accessed using applications that\\nredirect certain requests.\\nStorage\\nTransactions\\nOperation types'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Operation in Metrics AppDescription Operation Unit of\\nMeasure\\nConsumption\\nrate\\nOneLake Read via Redirect OneLake Read via Redirect Every 4 MB, per\\n10,000*\\n104 CU seconds\\nOneLake Write via Redirect OneLake Write via Redirect Every 4 MB, per\\n10,000*\\n1626 CU\\nseconds\\nOneLake Other Operations\\nvia Redirect\\nOneLake Other Operations\\nvia Redirect\\nPer 10,000 104 CU seconds\\nOneLake Iterative Read via\\nRedirect\\nOneLake Iterative Read via\\nRedirect\\nPer 10,000 1626 CU\\nseconds\\nOneLake Iterative Write via\\nRedirect\\nOneLake Iterative Write via\\nRedirect\\nPer 100 1300 CU\\nseconds\\nThis table defines CU consumption when OneLake data is accessed using applications that\\nproxy requests, which match the rate for transactions via redirect.\\nOperation in Metrics AppDescription Operation Unit of\\nMeasure\\nConsumption\\nrate\\nOneLake Read via Proxy OneLake Read via Proxy Every 4 MB, per\\n10,000*\\n104 CU seconds\\nOneLake Write via Proxy OneLake Write via ProxyEvery 4 MB, per\\n10,000*\\n1626 CU seconds'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake Write via Proxy OneLake Write via ProxyEvery 4 MB, per\\n10,000*\\n1626 CU seconds\\nOneLake Other OperationsOneLake Other OperationsPer 10,000 104 CU seconds\\nOneLake Iterative Read via\\nProxy\\nOneLake Iterative Read via\\nProxy\\nPer 10,000 1626 CU seconds\\nOneLake Iterative Write via\\nProxy\\nOneLake Iterative Write via\\nProxy\\nPer 100 1300 CU seconds\\n*For files > 4 MB in size, OneLake counts a transaction for every 4 MB block of data read or\\nwritten. For files < 4 MB, a full transaction is counted. For example, if you do 10,000 read\\noperations via Redirect and each file read is 16 MB in size, your capacity consumption is 40,000\\ntransactions or 416 CU seconds.\\nﾉ Expand table\\nﾉ Expand table\\nShortcuts'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='When you access data via OneLake shortcuts, the transaction usage counts against the capacity\\ntied to the workspace where the shortcut is created. The capacity where the data is ultimately\\nstored (that the shortcut points to) is billed for the data stored.\\nWhen you access data via a shortcut to a source external to OneLake, such as to ADLS Gen2,\\nOneLake does not count the CU usage for that external request. The transactions would be\\ncharged directly to you by the external service such as ADLS Gen2.\\nWhen a capacity is paused, the data stored continues to be billed using the pay-as-you-go rate\\nper GB. All transactions to that capacity are rejected when it is paused, so no Fabric CUs are\\nconsumed due to OneLake transactions. To access your data or delete a Fabric item, the\\ncapacity needs to be resumed. You can delete the workspace while a capacity is paused.\\nThe consumption of the data via shortcuts is always counted against the consumer’s capacity,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='The consumption of the data via shortcuts is always counted against the consumer’s capacity,\\nso the capacity where the data is stored can be paused without disrupting downstream\\nconsumers in other capacities. See an example on the OneLake Capacity Consumption page\\nOneLake usage when disaster recovery is enabled is also defined by the amount of data stored\\nand the number of transactions.\\nWhen disaster recovery is enabled, the data in OneLake gets geo-replicated. Thus, the storage\\nis billed as Business Continuity and Disaster Recovery (BCDR) Storage. For more information\\nabout pricing, see Fabric pricing.\\nWhen disaster recovery is enabled for a given capacity, write operations consume higher\\ncapacity units.\\nThis table defines CU consumption when disaster recovery is enabled and OneLake data is\\naccessed using applications that redirect certain requests. Redirection is an implementation\\nthat reduces consumption of OneLake compute.\\nPaused Capacity\\nDisaster recovery'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='that reduces consumption of OneLake compute.\\nPaused Capacity\\nDisaster recovery\\nDisaster recovery storage\\nDisaster recovery transactions\\nDisaster recovery operation types'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Operation Description Operation Unit of\\nMeasure\\nCapacity\\nUnits\\nOneLake BCDR Read via\\nRedirect\\nOneLake BCDR Read via\\nRedirect\\nEvery 4 MB, per\\n10,000\\n104 CU\\nseconds\\nOneLake BCDR Write via\\nRedirect\\nOneLake BCDR Write via\\nRedirect\\nEvery 4 MB, per\\n10,000\\n3056 CU\\nseconds\\nOneLake BCDR Other\\nOperations Via Redirect\\nOneLake BCDR Other\\nOperations Via Redirect\\nPer 10,000 104 CU\\nseconds\\nOneLake BCDR Iterative Read\\nvia Redirect\\nOneLake BCDR Iterative Read\\nvia Redirect\\nPer 10,000 1626 CU\\nseconds\\nOneLake BCDR Iterative Write\\nvia Redirect\\nOneLake BCDR Iterative Write\\nvia Redirect\\nPer 100 2730 CU\\nseconds\\nThis table defines CU consumption when disaster recovery is enabled and OneLake data is\\naccessed using applications that proxy requests, which match the rate for transactions via\\nredirect.\\nOperation Description Operation Unit of\\nMeasure\\nCapacity\\nUnits\\nOneLake BCDR Read via Proxy OneLake BCDR Read via ProxyEvery 4 MB, per\\n10,000\\n104 CU\\nseconds\\nOneLake BCDR Write via\\nProxy\\nOneLake BCDR Write via\\nProxy'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='10,000\\n104 CU\\nseconds\\nOneLake BCDR Write via\\nProxy\\nOneLake BCDR Write via\\nProxy\\nEvery 4 MB, per\\n10,000\\n3056 CU\\nseconds\\nOneLake BCDR Other\\nOperations\\nOneLake BCDR Other\\nOperations\\nPer 10,000 104 CU\\nseconds\\nOneLake BCDR Iterative Read\\nvia Proxy\\nOneLake BCDR Iterative Read\\nvia Proxy\\nPer 10,000 1626 CU\\nseconds\\nOneLake BCDR Iterative Write\\nvia Proxy\\nOneLake BCDR Iterative Write\\nvia Proxy\\nPer 100 2730 CU\\nseconds\\nOneLake security consumes capacity for row level security (RLS) transactions based on the\\nnumber of rows in the table secured by RLS. When you access a table secured with RLS, the\\nﾉ Expand table\\nﾉ Expand table\\nOneLake security'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='capacity consumption applies to the Fabric item used to execute the query according to the\\ntable below.\\nOperation Description Operation Unit of Measure Capacity Units\\nOneLake security RLS OneLake security RLSMillion rows in the table 0.1 CU seconds\\nOneLake diagnostics consumes capacity when diagnostic events are captured and written to a\\ndestination Lakehouse according to the table below.\\nOperation Description Operation Unit of\\nMeasure\\nCapacity\\nUnits\\nOneLake Diagnostics Event\\nOperation\\nOneLake diagnostic write operationsEvery 4 MB, per\\n10,000\\n1626 CU\\nseconds\\nOneLake BCDR Diagnostics\\nEvent Operation\\nOneLake diagnostic write operations\\nwhen BCDR is enabled\\nEvery 4 MB, per\\n10,000\\n3056 CU\\nseconds\\nOneLake Diagnostics Data\\nTransfer\\nOneLake diagnostic data transferPer GB 1.389 CU\\nHours\\nConsumption rates are subject to change at any time. Microsoft will use reasonable efforts to\\nprovide notice via email or through in-product notification. Changes shall be effective on the'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"provide notice via email or through in-product notification. Changes shall be effective on the\\ndate stated in Microsoft's Release Notes or Microsoft Fabric Blog. If any change to a Microsoft\\nFabric Workload Consumption Rate materially increases the Capacity Units (CU) required to use\\na particular workload, customers may use the cancellation options available for the chosen\\npayment method.\\nDisaster recovery guidance for OneLake\\nﾉ Expand table\\nOneLake diagnostics\\nﾉ Expand table\\nChanges to Microsoft Fabric workload\\nconsumption rate\\nRelated content\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Fabric capacity and OneLake\\nconsumption\\nArticle• 12/26/2024\\nYou only need one capacity to drive all your Microsoft Fabric experiences, including\\nMicrosoft OneLake. Keep reading if you want a detailed example of how OneLake\\nconsumes storage and compute.\\nOneLake comes automatically with every Fabric tenant and is designed to be the single\\nplace for all your analytics data. All the Fabric data items are prewired to store data in\\nOneLake. For example, when you store data in a lakehouse or warehouse, your data is\\nnatively stored in OneLake.\\nWith OneLake, you pay for the data stored, similar to services like Azure Data Lake\\nStorage (ADLS) Gen2 or Amazon S3. However, unlike other services, OneLake doesn't\\ninclude a separate charge for transactions (for example, reads, writes) to your data.\\nInstead, transactions consume from existing Fabric capacity that is also used to run your\\nother Fabric experiences. For information about pricing, which is comparable to ADLS\\nGen2, see Fabric pricing.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Gen2, see Fabric pricing.\\nTo illustrate, let’s walk through an example.\\nLet’s say you purchase an F2 SKU with 2 Capacity Units (CU) every second. Let’s\\nname this Capacity1.\\nYou then create Workspace1 and upload a 450 MB file to a lakehouse using the\\nFabric portal. This action consumes both OneLake storage and OneLake\\ntransactions.\\nNow, let’s dive into each of these dimensions.\\nSince OneLake storage operates on a pay-as-you-go model, a separate charge for\\n\"OneLake Storage\" appears in your bill corresponding to the 450 MB of data stored.\\nIf you\\'re a capacity admin, you can view your storage consumption in the Fabric\\nCapacity Metrics app. Open the Storage tab and choose Experience as lake to see the\\nOverview\\nOneLake Storage'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"cost of OneLake storage. If you have multiple workspaces in the capacity, you can see\\nthe storage per workspace.\\nThe following image, shows two columns: Billable storage and Current Storage. Billable\\nstorage reflects cumulative data usage over the month. Because the total charge for\\ndata stored isn't taken on one day of the month, but on a pro-rated basis throughout\\nthe month. You can estimate the monthly price as the billable storage (GB) multiplied by\\nthe price per GB per month.\\nFor example, storing 1 TB of data on day 1, adds to 33 GB daily billable storage. On day\\none it's 1 TB / 30 days = 33 GB and every day adds 33 GB until the month ends.\\nOneLake soft delete protects individual files from accidental deletion by retaining files\\nfor a default retention period before it's permanently deleted. Soft-deleted data is billed\\nat the same rate as active data.\\nRequests to OneLake (e.g., read, write, or list) consume Fabric capacity. OneLake maps\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Requests to OneLake (e.g., read, write, or list) consume Fabric capacity. OneLake maps\\nAPIs to operations like ADLS. Capacity usage for each operation is visible in the Capacity\\nMetrics app. In the above example, the file upload resulted in a write transaction\\nconsuming 127.46 CU seconds. This consumption is reported as OneLake Write via\\nProxy under the operation name column in the capacity metrics App.\\n\\uf80a\\n\\uf80a\\nOneLake Compute'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Now if you read this data using a notebook. You consume 1.39 CU seconds of read\\ntransactions. This consumption is reported as OneLake Read via Redirect in the metrics\\napp. See OneLake consumption page to learn how each type of operation consumes\\ncapacity units.\\nTo understand more about the various terminologies on the metrics app, see\\nUnderstand the metrics app compute page - Microsoft Fabric.\\nYou may be wondering, how do shortcuts affect my OneLake usage? In the above\\nexample, both storage and compute are billed to Capacity1. Now, let’s say you have a\\nsecond capacity Capacity2, that contains Workspace2. You create a lakehouse and create\\na shortcut to the parquet file you uploaded in Workspace1. You create a notebook to\\nquery the parquet file. As Capacity2 accesses the data, the compute or transaction cost\\nfor this read operation consumes CU from Capacity2. The storage continues to be billed\\nto Capacity1.\\nIf Capacity2 is paused but Capacity1 is active, you can’t read the data via the'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='to Capacity1.\\nIf Capacity2 is paused but Capacity1 is active, you can’t read the data via the\\nshortcut in Workspace2 (Capacity2) but can access the data directly in Workspace1\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Feedback\\nWas this page helpful?\\nProvide product feedback | Ask the community\\n(Capacity1).\\nIf Capacity1 is paused and Capacity2 is active, you can’t read the data in\\nWorkspace1 (Capacity1) but you can still use the data using the shortcut in\\nWorkspace2. In both cases, as the data is still stored in Capacity1, storage costs\\nremain billed to Capacity1\\nIf your CU consumption exceeds the capacity limit, throttling may occur, causing\\ntransactions to be delayed or rejected temporarily.\\nStart Fabric’s 60-day free trial to explore OneLake and other features, and visit the Fabric\\nforum for questions.\\n\\ue8e1Yes \\ue8e0No'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Explore OneLake events in Fabric Real-Time\\nhub\\n07/22/2025\\nOneLake events inform you about changes in your data lake, such as the creation, modification,\\nor deletion of files and folders.\\nReal-Time Hub enables you to discover and subscribe to these changes within OneLake,\\nallowing you to react instantly. For instance, you can monitor changes in Lakehouse files and\\nfolders and utilize Activator's alerting capabilities to set up alerts based on specific conditions\\nand define actions to take when those conditions are met. This article guides you on how to\\nexplore OneLake events using the Real-Time Hub\\n1. In Real-Time hub, select Fabric events.\\n2. Select OneLake events from the list.\\n3. You should see the detail view for OneLake events.\\n７ Note\\nConsuming Fabric and Azure events via Eventstream or Fabric Activator isn't supported if\\nthe capacity region of the Eventstream or Activator is in the following regions: West India,\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='the capacity region of the Eventstream or Activator is in the following regions: West India,\\nIsrael Central, Korea Central, Qatar Central, Singapore, UAE Central, Spain Central, Brazil\\nSoutheast, Central US, South Central US, West US 2, West US 3.\\nView OneLake events detail page\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"At the top of the detail page, you see the following two actions.\\nCreate eventstream, which lets you create an eventstream based on events from the\\nselected OneLake item.\\nSet alert, which lets you set an alert when an operation is done for a OneLake item, such\\nas a new file is created.\\nThese actions are also available in the Fabric events list view.\\nThis section shows the artifacts using OneLake events. Here are the columns and their\\ndescriptions:\\n\\uf80a\\nActions\\nSee what's using this category\\nﾉ Expand table\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Column Description\\nName Name of the artifact that's using OneLake events.\\nType Artifact type – Activator or Eventstream\\nWorkspace Workspace where the artifact lives.\\nSource Name of the workspace that is source of the events.\\nHere are the supported OneLake events:\\nEvent type name Description\\nMicrosoft.Fabric.OneLake.FileCreated Raised when a file is created or replaced in OneLake.\\nMicrosoft. Fabric.OneLake.FileDeleted Raised when a file is deleted in OneLake.\\nMicrosoft. Fabric.OneLake.FileRenamed Raised when a file is renamed in OneLake.\\nMicrosoft.Fabric.OneLake.FolderCreatedRaised created when a folder is created in OneLake.\\nMicrosoft. Fabric.OneLake.FolderDeletedRaised when a folder is deleted in OneLake.\\nOneLake events profile\\nEvent types\\nﾉ Expand table\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Event type name Description\\nMicrosoft. Fabric.OneLake.FolderRenamed Raised when a folder is renamed in OneLake.\\nAn event has the following top-level data:\\nProperty Type Description Example\\nsource string Identifies the\\ncontext in which\\nan event\\nhappened.\\n/aaaaaaaa-0000-1111-2222-\\nbbbbbbbbbbbb/workspaces/bbbbbbbb-1111-2222-\\n3333-cccccccccccc/items/cccccccc-2222-3333-\\n4444-dddddddddddd\\nsubject string Identifies the\\nsubject of the\\nevent in the\\ncontext of the\\nevent producer.\\n/Files/FolderA/FileName.txt\\ntype string One of the\\nregistered event\\ntypes for this\\nevent source.\\nMicrosoft.Fabric.OneLake.FileCreated\\ntime timestampThe time the event\\nis generated\\nbased on the\\nprovider's UTC\\ntime.\\n2017-06-26T18:41:00.9584103Z\\nid string Unique identifier\\nfor the event.\\nbbbbbbbb-1111-2222-3333-cccccccccccc\\ndata object Event data. See the next table for details.\\ndataschemaversion string The version of the\\ndata schema.\\n1.0\\nspecversion string The version of the\\nCloud Event spec.\\n1.0\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='data schema.\\n1.0\\nspecversion string The version of the\\nCloud Event spec.\\n1.0\\nThe data object has the following properties:\\nSchemas\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Property Type Description Example\\neTag string The value that\\nyou can use to\\nrun operations\\nconditionally.\\n\"\\\\\"0x8D4BCC2E4835CD0\\\\\"\\ncontentLength string Size of the file\\nin bytes.\\n0\\ncontentType string Content type\\nspecified for the\\nfile.\\ntext/plain\\nblobUrl string Blob URL to the\\npath of the file.\\nhttps://onelake.blob.fabric.microsoft.com/55556666-\\nffff-7777-aaaa-8888bbbb9999 < 66667777-aaaa-8888-\\nbbbb-9999cccc0000/Files/FolderA/File1.txt\\nurl string OneLake URL to\\nthe path of the\\nfile.\\nhttps://onelake.dfs.fabric.microsoft.com/eeeeeeee-\\n4444-5555-6666-ffffffffffff < aaaaaaaa-6666-7777-\\n8888-bbbbbbbbbbbb/Files/FolderA/File1.txt\\napi string The operation\\nthat triggered\\nthe event.\\nCreateFile\\nclientRequestId string A client-\\nprovided\\nrequest ID for\\nthe storage API\\noperation.\\naaaabbbb-0000-cccc-1111-dddd2222eeee\\nrequestId string Service-\\ngenerated\\nrequest ID for\\nthe storage API\\noperation.\\naaaabbbb-0000-cccc-1111-dddd2222eeee\\ncontentOffset numberThe offset in\\nbytes of a write\\noperation taken'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='contentOffset numberThe offset in\\nbytes of a write\\noperation taken\\nat the point\\nwhere the\\nevent-triggering\\napplication\\ncompleted\\nwriting to the\\nfile.\\n0\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Property Type Description Example\\nsequencer string An opaque\\nstring value\\nrepresenting the\\nlogical\\nsequence of\\nevents.\\n00000000000004420000000000028963\\nFor more information, see subscribe permission for Fabric events.\\nExplore Azure blob storage events\\nSubscribe permission\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Get OneLake events in Fabric Real-Time\\nhub\\n07/22/2025\\nThis article describes how to get OneLake events as an eventstream in Fabric Real-Time hub.\\nReal-Time hub allows you to discover and subscribe to changes in files and folders in OneLake,\\nand then react to those changes in real-time. For example, you can react to changes in files and\\nfolders in Lakehouse and use Activator alerting capabilities to set up alerts based on conditions\\nand specify actions to take when the conditions are met. This article explains how to explore\\nOneLake events in Real-Time hub.\\nWith Fabric event streams, you can capture these OneLake events, transform them, and route\\nthem to various destinations in Fabric for further analysis. This seamless integration of OneLake\\nevents within Fabric event streams gives you greater flexibility for monitoring and analyzing\\nactivities in your OneLake.\\nHere are the supported OneLake events:\\nEvent type name Description'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"activities in your OneLake.\\nHere are the supported OneLake events:\\nEvent type name Description\\nMicrosoft.Fabric.OneLake.FileCreated Raised when a file is created or replaced in OneLake.\\nMicrosoft. Fabric.OneLake.FileDeleted Raised when a file is deleted in OneLake.\\nMicrosoft. Fabric.OneLake.FileRenamed Raised when a file is renamed in OneLake.\\nMicrosoft.Fabric.OneLake.FolderCreatedRaised created when a folder is created in OneLake.\\nMicrosoft. Fabric.OneLake.FolderDeletedRaised when a folder is deleted in OneLake.\\nMicrosoft. Fabric.OneLake.FolderRenamed Raised when a folder is renamed in OneLake.\\nFor more information, see Explore OneLake events.\\nEvent types\\nﾉ Expand table\\n７ Note\\nConsuming Fabric and Azure events via Eventstream or Fabric Activator isn't supported if\\nthe capacity region of the Eventstream or Activator is in the following regions: West India,\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Access to a workspace in the Fabric capacity license mode (or) the Trial license mode with\\nContributor or higher permissions.\\nSusbcribeOneLakeEvent permission on the data sources.\\nYou can create streams for OneLake events in Real-Time hub using one of the ways:\\nUsing the Data sources page\\nUsing the Fabric events page\\n1. Sign in to Microsoft Fabric .\\n2. If you see Power BI at the bottom-left of the page, switch to the Fabric workload by\\nselecting Power BI and then by selecting Fabric.\\n3. Select Real-Time on the left navigation bar.\\nIsrael Central, Korea Central, Qatar Central, Singapore, UAE Central, Spain Central, Brazil\\nSoutheast, Central US, South Central US, West US 2, West US 3.\\nPrerequisites\\nCreate streams for OneLake events\\nData sources page'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='4. On the Real-Time hub page, select + Data sources under Connect to on the left\\nnavigation menu.\\nYou can also get to the Data sources page from the Real-Time hub page by selecting the\\n+ Add data button in the top-right corner.\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='4. On the Data sources page, select OneLake events category at the top, and then select\\nConnect on the OneLake events tile. You can also use the search bar to search for\\nOneLake events.\\nNow, use instructions from the Configure and create an eventstream section.\\n\\uf80a\\n\\uf80a\\nFabric events page'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='In Real-Time hub, select Fabric events on the left menu. You can use either the list view of\\nFabric events or the detail view of OneLake events to create an eventstream for OneLake\\nevents.\\nMove the mouse over OneLake events, and select the Create Eventstream link or select ...\\n(Ellipsis) and then select Create Eventstream.\\n1. On the Fabric events page, select OneLake events from the list of Fabric events\\nsupported.\\n2. On the Detail page, select + Create eventstream from the menu.\\nNow, use instructions from the Configure and create an eventstream section, but skip the\\nfirst step of using the Add source page.\\n1. On the Connect page, for Event types, select the event types that you want to monitor.\\nUsing the list view\\n\\uf80a\\nUsing the detail view\\n\\uf80a\\nConfigure and create an eventstream'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='2. This step is optional. To see the schemas for event types, select View selected event type\\nschemas. If you select it, browse through schemas for the events, and then navigate back\\nto previous page by selecting the backward arrow button at the top.\\n3. Select Add a OneLake source under Select data source for events.\\n4. On the Choose the data you want to connect page:\\na. View all available data sources or only your data sources (My data) or your favorite\\ndata sources by using the category buttons at the top. You can use the Filter by\\nkeyword text box to search for a specific source. You can also use the Filter button to\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"filter based on the type of the resource (KQL Database, Lakehouse, SQL Database,\\nWarehouse). The following example uses the My data option.\\nb. Select the data source from the list.\\nc. Select Next at the bottom of the page.\\n5. Select all tables or a specific table that you're interested in, and then select Add.\\n\\uf80a\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='6. Now, on the Configure connection settings page, you can add filters to set the filter\\nconditions by selecting fields to watch and the alert value. To add a filter:\\na. Select + Filter.\\nb. Select a field.\\nc. Select an operator.\\nd. Select one or more values to match.\\n\\uf80a\\n７ Note\\nOneLake events are supported for data in OneLake. However, events for data in\\nOneLake via shortcuts are not yet available.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='7. In the Stream details section to the right, follow these steps.\\na. Select the workspace where you want to save the eventstream.\\nb. Enter a name for the eventstream. The Stream name is automatically generated for\\nyou.\\n8. Then, select Next at the bottom of the page.\\n9. On the Review + connect page, review settings, and select Connect.\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"10. When the wizard succeeds in creating a stream, use Open eventstream link to open the\\neventstream that was created for you. Select Finish to close the wizard.\\nSelect Real-Time hub on the left navigation menu, and confirm that you see the stream you\\ncreated. Refresh the page if you don't see it.\\n\\uf80a\\n\\uf80a\\nView stream from the Real-Time hub page\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='For detailed steps, see View details of data streams in Fabric Real-Time hub.\\nTo learn about consuming data streams, see the following articles:\\nProcess data streams\\nAnalyze data streams\\nSet alerts on data streams\\n\\uf80a\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Set alerts on OneLake events in Real-Time\\nhub\\n10/15/2025\\nThis article describes how to set alerts on OneLake events in Real-Time hub.\\n1. Sign in to Microsoft Fabric .\\n2. If you see Power BI at the bottom-left of the page, switch to the Fabric workload by\\nselecting Power BI and then by selecting Fabric.\\n3. Select Real-Time on the left navigation bar.\\n７ Note\\nConsuming Fabric and Azure events via Eventstream or Fabric Activator isn't supported if\\nthe capacity region of the Eventstream or Activator is in the following regions: West India,\\nIsrael Central, Korea Central, Qatar Central, Singapore, UAE Central, Spain Central, Brazil\\nSoutheast, Central US, South Central US, West US 2, West US 3.\\nNavigate to Real-Time hub\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Do steps from one of the following sections, which opens a side panel where you can configure\\nthe following options:\\nEvents you want to monitor.\\nConditions you want to look for in the events.\\nAction you want Activator to take.\\n1. In Real-Time hub, select Fabric events.\\n2. Move the mouse over OneLake events, and do one of the following steps:\\nSelect the Alert button.\\nSelect ellipsis (...), and select Set alert.\\n\\uf80a\\nLaunch the Set alert page\\nUsing the events list\\n\\uf80a\\nUsing the event detail page'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Select OneLake events from the list see the detail page.\\n2. On the detail page, select Set alert button at the top of page.\\nOn the Add rule page, in the Details section, for Rule name, enter a name for the rule.\\n1. In the Monitor section, for Source, choose Select source events.\\n\\uf80a\\nDetails section\\nMonitor section'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='2. In the Connect data source wizard, do these steps:\\na. For Event types, select event types that you want to monitor.\\nb. Select Add a OneLake source button in the Select data source for events section.\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='c. On the Choose the data you want to connect page:\\ni. View all available data sources or only your data sources (My data) or your favorite\\ndata sources by using the category buttons at the top. You can use the Filter by\\nkeyword text box to search for a specific source. You can also use the Filter button\\nto filter based on the type of the resource (KQL Database, Lakehouse, SQL Database,\\nWarehouse). The following example uses the My data option.\\nii. Select the data source from the list.\\niii. Select Next at the bottom of the page.\\n\\uf80a\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"iv. Select all tables or a specific table that you're interested in, and then select Add.\\nd. Now, on the Configure connection settings page, you can add filters to set the filter\\nconditions by selecting fields to watch and the alert value. To add a filter:\\ni. Select + Filter.\\nii. Select a field.\\niii. Select an operator.\\niv. Select one or more values to match.\\n\\uf80a\\n７ Note\\nOneLake events are supported for data in OneLake. However, events for data in\\nOneLake via shortcuts aren't yet available.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='e. Select Next at the bottom of the page.\\nf. On the Review + connect page, review the settings, and select Save.\\nIn the Condition section, for Check, select On each event.\\n\\uf80a\\nCondition section'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='In the Action section, select one of the following actions:\\nTo configure the alert to send an email when the condition is met, follow these steps:\\n1. For Select action, select Send email.\\n2. For To, enter the email address of the receiver or use the drop-down list to select a\\nproperty whose value is an email address.\\n3. For Subject, enter a subject for the email.\\n4. For Headline, enter a headline for the email.\\n5. For Notes, enter notes for the emails.\\n6. For Context, select values from the drop-down list that you want to include in the\\ncontext.\\nAction section\\nEmail\\n７ Note\\nWhen entering subject, headline, or notes, you can refer to properties in the data by\\ntyping @ or by selecting the button next to the text boxes. For example,\\n@BikepointID.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='To configure the alert to send a Teams message to an individual or a group chat or a channel\\nwhen the condition is met, follow these steps:\\n1. For Select action, select Teams -> Message to individuals or Group chat message, or\\nChannel post.\\n2. Follow one of these steps depending on the option you selected in the previous step:\\nIf you selected the Message to individuals option, enter email addresses of\\nreceivers or use the drop-down list to select a property whose value is an email\\naddress. When the condition is met, an email is sent to specified individuals.\\nIf you selected the Group chat message option, select a group chat from the drop-\\ndown list. When the condition is met, a message is posted to the group chat.\\nIf you selected the Channel post option, select a team and a channel. When the\\ncondition is met, a message is posted in the channel.\\n3. For Headline, enter a headline for the email.\\n4. For Notes, enter notes for the emails.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='3. For Headline, enter a headline for the email.\\n4. For Notes, enter notes for the emails.\\n5. For Context, select values from the drop-down list that you want to include in the\\ncontext.\\nTeams message\\n７ Note\\nWhen entering headline, or notes, you can refer to properties in the data by typing @\\nor by selecting the button next to the text boxes. For example, @BikepointID.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='To configure the alert to launch a Fabric item (pipeline, notebook, Spark job, etc.) when the\\ncondition is met, follow these steps:\\n1. For Selection action, select Run a Fabric item.\\n2. Choose Select Fabric item to run, and then select the Fabric item from the list.\\n3. Select Add parameter and specify the name of the parameter for the Fabric item and a\\nvalue for it. You can add more than one parameter.\\nRun a Fabric item'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"In the Save location section, for Workspace, select the workspace where you want to Fabric\\nactivator item to be created or that already exists. If you're creating a new activator item, enter\\na name for the activator item.\\nSave location section\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='1. Select Create at the bottom of the page to create the alert.\\nCreate alert'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='2. You see the Alert created page with a link to open the rule in the Fabric activator user\\ninterface in a separate tab. Select Done to close the Alert created page.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='3. You see a page with the activator item created by the Add rule wizard. If you are on the\\nFabric events page, select Job events to see this page.\\n\\uf80a'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"4. Move the mouse over the Activator item, and select Open.\\n5. You see the Activator item in the Fabric Activator editor user interface. Select the rule if\\nit's not already selected. You can update the rule in this user interface. For example,\\nupdate the subject, headline, or change the action from email to Teams message.\\n\\uf80a\\n\\uf80a\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Set alerts on Azure blob storage events\\nSet alerts on Fabric workspace item events\\n\\uf80a\\nRelated content'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake Shortcuts\\nService:Core\\nAPI Version:v1\\nCreate Shortcut Creates a new shortcut or updates an existing shortcut.\\nCreates Shortcuts In\\nBulk\\nCreates bulk shortcuts.\\nDelete Shortcut Deletes the shortcut but does not delete the destination storage folder.\\nGet Shortcut Returns shortcut properties.\\nList Shortcuts Returns a list of shortcuts for the item, including all the subfolders\\nexhaustively.\\nReset Shortcut CacheDeletes any cached files that were stored while reading from shortcuts.\\nOperations\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake Data Access Security\\nService:Core\\nAPI Version:v1\\nCreate Or Update Data Access Roles Creates or updates data access roles in OneLake.\\nList Data Access Roles Returns a list of OneLake roles.\\nOperations\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake security for SQL analytics\\nendpoints (Preview)\\nWith OneLake security, Microsoft Fabric is expanding how organizations can manage and\\nenforce data access across workloads. This new security framework gives administrators greater\\nflexibility to configure permissions. Administrators can choose between centralized\\ngovernance through OneLake or granular SQL-based control within the SQL analytics\\nendpoint.\\nWhen using the SQL analytics endpoint, the selected access mode determines how data\\nsecurity is enforced. Fabric supports two distinct access models, each offering different benefits\\ndepending on your operational and compliance needs:\\nUser identity mode: Enforces security using OneLake roles and policies. In this mode, the\\nSQL analytics endpoint passes the signed-in user’s identity to OneLake, and read access is\\ngoverned entirely by the security rules defined within OneLake. SQL-level permissions\\non tables are supported, ensuring consistent governance across tools like Power BI,'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='on tables are supported, ensuring consistent governance across tools like Power BI,\\nnotebooks, and lakehouse.\\nDelegated identity mode: Provides full control through SQL. In this mode, the SQL\\nanalytics endpoint connects to OneLake using the identity of the workspace or item\\nowner, and security is governed exclusively by SQL permissions defined inside the\\ndatabase. This model supports traditional security approaches including GRANT, REVOKE,\\ncustom roles, Row-Level Security, and Dynamic Data Masking.\\nEach mode supports different governance models. Understanding their implications is essential\\nfor choosing the right approach in your Fabric environment.\\nHere’s a clear and concise comparison table focused on how and where you set security in user\\nidentity mode versus delegated identity mode—broken down by object type and data access\\npolicies:\\nAccess modes in SQL analytics endpoint\\nComparison between access modes\\nﾉ Expand table'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Security target User identity mode Delegated identity mode\\nTables Access is controlled by OneLake security roles.\\nSQL GRANT/REVOKE isn't allowed.\\nFull control using SQL\\nGRANT/REVOKE.\\nViews Use SQL GRANT/REVOKE to assign\\npermissions.\\nUse SQL GRANT/REVOKE to\\nassign permissions.\\nStored procedures Use SQL GRANT EXECUTE to assign\\npermissions.\\nUse SQL GRANT EXECUTE to\\nassign permissions.\\nFunctions Use SQL GRANT EXECUTE to assign\\npermissions.\\nUse SQL GRANT EXECUTE to\\nassign permissions.\\nRow-Level Security\\n(RLS)\\nDefined in OneLake UI as part of OneLake\\nsecurity roles.\\nDefined using SQL CREATE\\nSECURITY POLICY.\\nColumn-Level\\nSecurity (CLS)\\nDefined in OneLake UI as part of OneLake\\nsecurity roles.\\nDefined using SQL GRANT\\nSELECT with column list.\\nDynamic Data\\nMasking (DDM)\\nNot supported in OneLake security. Defined using SQL ALTER TABLE\\nwith MASKED option.\\nIn user identity mode, the SQL analytics endpoint uses a passthrough authentication\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"In user identity mode, the SQL analytics endpoint uses a passthrough authentication\\nmechanism to enforce data access. When a user connects to the SQL analytics endpoint, their\\nEntra ID identity is passed through to OneLake, which performs the permission check. All read\\noperations against tables are evaluated using the security rules defined within the OneLake\\nLakehouse, not by any SQL-level GRANT or REVOKE statements.\\nThis mode lets you manage security centrally, ensuring consistent enforcement across all Fabric\\nexperiences, including Power BI, notebooks, lakehouse, and SQL analytics endpoint. It's\\ndesigned for governance models where access should be defined once in OneLake and\\nautomatically respected everywhere.\\nIn user identity mode:\\nTable access is governed entirely by OneLake security. SQL GRANT/REVOKE statements\\non tables are ignored.\\nRLS (Row-Level Security), CLS (Column-Level Security), and Object-Level Security are all\\ndefined in the OneLake experience.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='defined in the OneLake experience.\\nSQL permissions are allowed for nondata objects like views, stored procedures, and\\nfunctions, enabling flexibility for defining custom logic or user-facing entry points to data.\\nUser identity mode in OneLake security'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Write operations aren't supported at the SQL analytics endpoint. All writes must occur\\nthrough the Lakehouse UI and are governed by workspace roles (Admin, Member,\\nContributor).\\nUsers with the Admin, Member, or Contributor role at the workspace level aren't subject to\\nOneLake security enforcement. These roles have elevated privileges and will bypass RLS, CLS,\\nand OLS policies entirely. Follow these requirements to ensure OneLake security is respected:\\nAssign users the Viewer role in the workspace, or\\nShare the Lakehouse or SQL analytics endpoint with users using read-only permissions.\\nOnly users with read-only access have their queries filtered according to OneLake security\\nroles.\\nIf a user belongs to multiple OneLake roles, the most permissive role defines their effective\\naccess. For example:\\nIf one role grants full access to a table and another applies RLS to restrict rows, the RLS\\nwill not be enforced.\\nThe broader access role takes precedence. This behavior ensures users aren't\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='will not be enforced.\\nThe broader access role takes precedence. This behavior ensures users aren\\'t\\nunintentionally blocked, but it requires careful role design to avoid conflicts. It\\'s\\nrecommended to keep restrictive and permissive roles mutually exclusive when enforcing\\nrow- or column-level access controls.\\nFor more information, see the data access control model for OneLake security.\\n） Important\\nThe SQL Analytics Endpoint requires a one-to-one mapping between item permissions\\nand members in a OneLake security role to sync correctly. If you grant an identity access\\nto a OneLake security role, that same identity needs to have Fabric Read permission to the\\nlakehouse as well. For example, if a user assigns \"user123@microsoft.com\" to a OneLake\\nsecurity role then \"user123@microsoft.com\" must also be assigned to that lakehouse.\\nWorkspace role behavior\\nRole precedence: Most permissive access wins'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"A critical component of user identity mode is the security sync service. This background\\nservice monitors changes made to security roles in OneLake and ensures those changes are\\nreflected in the SQL analytics endpoint.\\nThe security sync service is responsible for the following:\\nDetecting changes to OneLake roles, including new roles, updates, user assignments, and\\nchanges to tables.\\nTranslating OneLake-defined policies (RLS, CLS, OLS) into equivalent SQL-compatible\\ndatabase role structures.\\nEnsuring shortcut objects (tables sourced from other lakehouses) are properly validated\\nso that the original OneLake security settings are honored, even when accessed remotely.\\nThis synchronization ensures that OneLake security definitions stay authoritative, eliminating\\nthe need for manual SQL-level intervention to replicate security behavior. Because security is\\ncentrally enforced:\\nYou can't define RLS, CLS, or OLS directly using T-SQL in this mode.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"centrally enforced:\\nYou can't define RLS, CLS, or OLS directly using T-SQL in this mode.\\nYou can still apply SQL permissions to views, functions, and stored procedures using\\nGRANT or EXECUTE statements.\\nScenario Behavior in user\\nidentity mode\\nBehavior in\\ndelegated\\nmode\\nCorrective\\naction\\nNotes\\nRLS policy\\nreferences a\\ndeleted or\\nrenamed\\ncolumn\\nError: *Row-level\\nsecurity policy\\nreferences a column\\nthat no longer\\nexists.*Database\\nenters error state\\nuntil policy is fixed.\\nError: Invalid\\ncolumn name\\n<column\\nname>\\nUpdate or\\nremove one or\\nmore affected\\nroles, or restore\\nthe missing\\ncolumn.\\nThe update will need to be\\nmade in the lakehouse\\nwhere the role was\\ncreated.\\nCLS policy\\nreferences a\\nError: *Column-level\\nsecurity policy\\nError: Invalid\\ncolumn name\\nUpdate or\\nremove one or\\nThe update will need to be\\nmade in the lakehouse\\nSecurity sync between OneLake and SQL analytics\\nendpoint\\nSecurity sync errors & resolution\\nﾉ Expand table\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Scenario Behavior in user\\nidentity mode\\nBehavior in\\ndelegated\\nmode\\nCorrective\\naction\\nNotes\\ndeleted or\\nrenamed\\ncolumn\\nreferences a column\\nthat no longer\\nexists.*Database\\nenters error state\\nuntil policy is fixed.\\n<column\\nname>\\nmore affected\\nroles, or restore\\nthe missing\\ncolumn.\\nwhere the role was\\ncreated.\\nRLS/CLS policy\\nreferences a\\ndeleted or\\nrenamed table\\nError: Security policy\\nreferences a table\\nthat no longer exists.\\nNo error\\nsurfaced;\\nquery fails\\nsilently if\\ntable is\\nmissing.\\nUpdate or\\nremove one or\\nmore affected\\nroles, or restore\\nthe missing\\ntable.\\nThe update will need to be\\nmade in the lakehouse\\nwhere the role was\\ncreated.\\nDDM (Dynamic\\nData Masking)\\npolicy\\nreferences a\\ndeleted or\\nrenamed\\ncolumn\\nDDM not supported\\nfrom OneLake\\nSecurity, must be\\nimplemented\\nthrough SQL.\\nError: Invalid\\ncolumn name\\n<column\\nname>\\nUpdate or\\nremove one or\\nmore affected\\nDDM rules, or\\nrestore the\\nmissing column.\\nUpdate the DDM policy in\\nthe SQL Analytics\\nEndpoint.\\nSystem error\\n(unexpected\\nfailure)'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Update the DDM policy in\\nthe SQL Analytics\\nEndpoint.\\nSystem error\\n(unexpected\\nfailure)\\nError: An unexpected\\nsystem error\\noccurred. Try again or\\ncontact support.\\nError: An\\ninternal error\\nhas occurred\\nwhile\\napplying\\ntable changes\\nto SQL.\\nRetry operation;\\nif issue persists,\\ncontact\\nMicrosoft\\nSupport.\\nN/A\\nUser doesn't\\nhave\\npermission on\\nthe artifact\\nError: User doesn't\\nhave permission on\\nthe artifact\\nError: User\\ndoesn't have\\npermission\\non the\\nartifact\\nProvide user with\\nobjectID\\n{objectID}\\npermission to the\\nartifact.\\nThe object ID must be an\\nexact match between the\\nOneLake security role\\nmember and the Fabric\\nitem permissions. If a\\ngroup is added to the role\\nmembership, then that\\nsame group must be given\\nthe Fabric Read\\npermission. Adding a\\nmember from that group\\nto the item does not count\\nas a direct match.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='Scenario Behavior in user\\nidentity mode\\nBehavior in\\ndelegated\\nmode\\nCorrective\\naction\\nNotes\\nUser principal\\nis not\\nsupported.\\nError: User principal\\nis not supported.\\nError: User\\nprincipal is\\nnot\\nsupported.\\nPlease remove\\nuser {username}\\nfrom role\\nDefaultReader\\nThis error occurs if the\\nuser is no longer a valid\\nEntra ID, such as if the user\\nhas left your organization\\nor been deleted. Remove\\nthem from the role to\\nresolve this error.\\nOneLake security is enforced at the source of truth, so security sync disables ownership\\nchaining for tables and views involving shortcuts. This ensures that source system permissions\\nare always evaluated and honored, even for queries from another database.\\nAs a result:\\nUsers must have valid access on both the shortcut source (current Lakehouse or SQL\\nanalytics endpoint) and the destination where the data physically resides.\\nIf the user lacks permission on either side, queries will fail with an access error.'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"If the user lacks permission on either side, queries will fail with an access error.\\nWhen designing your applications or views that reference shortcuts, ensure that role\\nassignments are properly configured on both ends of the shortcut relationship.\\nThis design preserves security integrity across Lakehouse boundaries, but it introduces\\nscenarios where access failures might occur if cross-Lakehouse roles aren't aligned.\\nIn Delegated Identity Mode, the SQL analytics endpoint uses the same security model that\\nexists today in Microsoft Fabric. Security and permissions are managed entirely at the SQL\\nlayer, and OneLake roles or access policies aren't enforced for table-level access. When a user\\nconnects to the SQL analytics endpoint and issues a query:\\nSQL validates access based on SQL permissions (GRANT, REVOKE, RLS, CLS, DDM, roles,\\netc.).\\nIf the query is authorized, the system proceeds to access data stored in OneLake.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='etc.).\\nIf the query is authorized, the system proceeds to access data stored in OneLake.\\nThis data access is performed using the identity of the Lakehouse or SQL analytics\\nendpoint owner—also known as the item account.\\nShortcuts behavior with security sync\\nDelegated mode in OneLake security'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"In this model:\\nThe signed-in user isn't passed through to OneLake.\\nAll enforcement of access is assumed to be handled at the SQL layer.\\nThe item owner is responsible for having sufficient permissions in OneLake to read the\\nunderlying files on behalf of the workload.\\nBecause this is a delegated pattern, any misalignment between SQL permissions and OneLake\\naccess for the owner results in query failures. This mode provides full compatibility with:\\nSQL GRANT/REVOKE at all object levels\\nSQL-defined Row-Level Security, Column-Level Security, and Dynamic Data Masking\\nExisting T-SQL tooling and practices used by DBAs or applications\\nThe access mode determines how data access is authenticated and enforced when querying\\nOneLake through SQL analytics endpoint. You can switch between user identity mode and\\ndelegated identity mode using the following steps:\\n1. Navigate to your Fabric workspace and open your lakehouse. From top right hand corner,\\nswitch from lakehouse to SQL analytics endpoint.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"switch from lakehouse to SQL analytics endpoint.\\n2. From the top navigation, go to Security tab and select the one of the following OneLake\\naccess modes:\\nUser identity – Uses the signed-in user's identity. It enforces OneLake roles.\\nDelegated identity – Uses the item owner's identity; enforces only SQL permissions.\\n3. A pop-up launches to confirm your selection. Select Yes to confirm the change.\\nSwitching to user identity mode\\nSQL RLS, CLS, and table-level permissions are ignored.\\nOneLake roles must be configured for users to maintain access.\\nHow to change the OneLake access mode\\nConsiderations when switching between modes\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Only users with Viewer permissions or shared read-only access will be governed by\\nOneLake security.\\nExisting SQL Roles are deleted and can't be recovered.\\nSwitching to delegated identity mode\\nOneLake roles and security policies are no longer applied.\\nSQL roles and security policies become active.\\nThe item owner must have valid OneLake access, or all queries may fail.\\nApplies only to readers: OneLake Security governs users accessing data as Viewers. Users\\nin other workspace roles (Admin Member, or Contributor) bypass OneLake Security and\\nretain full access.\\nSQL objects do not inherit ownership: Shortcuts are surfaced in SQL analytics endpoint\\nas tables. When accessing these tables, directly or through views, stored procedures, and\\nother derived SQL objects don't carry object-level ownership; all permissions are checked\\nat runtime to prevent the security bypass.\\nShortcut changes trigger validation downtime: When a shortcut target changes (for\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Shortcut changes trigger validation downtime: When a shortcut target changes (for\\nexample, rename, URL update), the database enters single-user mode briefly while the\\nsystem validates the new target. During this period queries are blocked, these operations\\na fairly quick, but sometimes depending on different internal process can take up to 5\\nminutes to synchronize.\\nCreating schema shortcuts might cause a known error that affects validation and\\ndelays metadata sync.\\nDelayed permission propagation: Permission changes aren't instantaneous. Switching\\nbetween security modes (User Identity vs. Delegated) may require time to propagate\\nbefore taking effect, but should take less than 1 minute.\\nControl-plane dependency: Permissions can't be applied to users or groups that don't\\nalready exist in the workspace control plane. You either need to share the source item, or\\nthe user must be member of Viewer workspace role. Note that the exact same object ID\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='the user must be member of Viewer workspace role. Note that the exact same object ID\\nmust be in both places. A group and a member of that group do not count as a match.\\nMost-permissive access prevails: When users belong to multiple groups or roles, the\\nmost permissive effective permission is honored Example: If a user has both DENY\\nthrough one role and GRANT through another, the GRANT takes precedence.\\nLimitations'),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Delegated mode limitations: In Delegated mode, metadata sync on shortcut tables can\\nfail if the source item has OneLake Security policies that don't grant full table access to\\nthe item owner.\\nDENY behavior: When multiple roles apply to a single shortcut table, the intersection of\\npermissions follows SQL Server semantics: DENY overrides GRANT. This can produce\\nunexpected access results.\\nExpected error conditions: Users may encounter errors in scenarios such as:\\nShortcut target renamed or invalid\\nExample: If the source of table was deleted.\\nRLS (Row-Level Security) misconfiguration\\nSome expressions for RLS filtering aren't supported in OneLake and it might allow\\nunauthorized data access.\\nDropping the column used on the filter expression invalidates the RLS and\\nMetadata Sync will be stale until the RLS is fixed on OneLake Security Panel.\\nFor Public Preview, we only support single expression tables. Dynamic RLS and\\nMulti-Table RLS aren't supported at the moment.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"Multi-Table RLS aren't supported at the moment.\\nColumn-Level Security (CLS) limitations\\nCLS works by maintaining an allowlist of columns. If an allowed column is removed\\nor renamed, the CLS policy becomes invalid.\\nWhen CLS is invalid, metadata sync is blocked until the CLS rule is fixed in the\\nOneLake Security panel.\\nMetadata or permission sync failure\\nIf there are changes to the table, like renaming a column, security isn't replicated on\\nthe new object, and you receive UI errors showing that the column doesn't exist.\\nTable renames do not preserve security policies: If OneLake Security (OLS) roles are\\ndefined on Schema level, those roles remain in effect only as long as the table name is\\nunchanged. Renaming the table breaks the association, and security policies won't be\\nmigrated automatically. This can result in unintended data exposure until policies are\\nreapplied.\\nOneLake security roles can't have names longer than 124 characters; otherwise, security\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"reapplied.\\nOneLake security roles can't have names longer than 124 characters; otherwise, security\\nsync can't synchronize the roles.\"),\n",
       " Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content=\"OneLake security roles are propagated on the SQL analytics endpoint with the OLS_\\nprefix.\\nUser changes on the OLS_ roles are not supported, and can cause unexpected behaviors.\\nMail enabled security groups and distribution lists are not supported.\\nThe owner of the lakehouse must be a member of the admin, member, or contributor\\nworkspace roles; otherwise, security isn't applied to the SQL analytics endpoint.\\nThe owner of the lakehouse cannot be a service principal for security sync to work.\\nBest practices to secure data in OneLake\\nOneLake security access control model\\nLast updated on 10/30/2025\\nRelated content\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c146a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2b8d7",
   "metadata": {},
   "source": [
    "#### **STEP 3: Creating Embeddings for the Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0879d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96b2b7",
   "metadata": {},
   "source": [
    "#### **STEP 4: Create and Store Embeddings in Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d1a4391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a71fd",
   "metadata": {},
   "source": [
    "#### **STEP 5: Semantic Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81785255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'fabric-onelake.pdf', 'developer': 'Microsoft'}, page_content='OneLake, the OneDrive for data\\nArticle• 07/25/2024\\nOneLake is a single, unified, logical data lake for your whole organization. A data Lake\\nprocesses large volumes of data from various sources. Like OneDrive, OneLake comes\\nautomatically with every Microsoft Fabric tenant and is designed to be the single place\\nfor all your analytics data. OneLake brings customers:\\nOne data lake for the entire organization\\nOne copy of data for use with multiple analytical engines\\nBefore OneLake, it was easier for customers to create multiple lakes for different\\nbusiness groups rather than collaborating on a single lake, even with the extra overhead\\nof managing multiple resources. OneLake focuses on removing these challenges by\\nimproving collaboration. Every customer tenant has exactly one OneLake. There can\\nnever be more than one and if you have Fabric, there can never be zero. Every Fabric\\ntenant automatically provisions OneLake, with no extra resources to set up or manage.'),\n",
       " Document(metadata={'developer': 'Microsoft', 'source': 'fabric-onelake.pdf'}, page_content='For more information on APIs and endpoints, see OneLake access and APIs. For\\nexamples of OneLake integrations with Azure, see Azure Synapse Analytics, Azure\\nstorage explorer, Azure Databricks, and Azure HDInsight articles.\\nOneLake is the OneDrive for data. Just like OneDrive, you can easily explore OneLake\\ndata from Windows using the OneLake file explorer for Windows. You can navigate all\\nyour workspaces and data items, easily uploading, downloading, or modifying files just\\nlike you do in Office. The OneLake file explorer simplifies working with data lakes,\\nallowing even nontechnical business users to use them.\\nFor more information, see OneLake file explorer.\\nOneLake aims to give you the most value possible out of a single copy of data without\\ndata movement or duplication. You no longer need to copy data just to use it with\\nanother engine or to break down silos so you can analyze the data with data from other\\nsources.\\n\\uf80a\\nOneLake file explorer for Windows\\nOne copy of data'),\n",
       " Document(metadata={'developer': 'Microsoft', 'source': 'fabric-onelake.pdf'}, page_content='Tell us about your PDF experience.\\nOneLake in Microsoft Fabric\\ndocumentation\\nOneLake is a single, unified, logical data lake for the whole organization. OneLake comes\\nautomatically with every Microsoft Fabric tenant with no infrastructure to manage.\\nAbout OneLake\\nｅOVERVIEW\\nWhat is OneLake?\\nOneLake security\\nOneLake catalog\\nOneLake access and APIs\\n｀DEPLOY\\nImplement medallion lakehouse architecture\\nｂGET STARTED\\nCreate a lakehouse with OneLake\\nOneLake file explorer\\nFind data in the OneLake catalog\\nUse Iceberg tables in OneLake\\nOneLake shortcuts\\nｐCONCEPT\\nWhat are shortcuts?\\nｂGET STARTED\\nCreate a shortcut\\nｃHOW-TO GUIDE')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"Why do we need OneLake?\", k= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cfb62",
   "metadata": {},
   "source": [
    "#### **Talk to LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae85010",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = vectorstore.similarity_search(\"What is AI?\", k= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19929ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI, or artificial intelligence, is the capability of computational systems to perform tasks that are typically associated with human intelligence. This includes learning, reasoning, problem-solving, perception, and decision-making. It’s a field of computer science that studies and develops methods and software that let machines perceive their environment and use learning and intelligence to take actions aimed at achieving defined goals.\n",
      "\n",
      "Key points:\n",
      "- AI spans technologies like neural networks, statistics, operations research, and economics, and it draws on insights from psychology, linguistics, philosophy, neuroscience, and more.\n",
      "- Some researchers aim for artificial general intelligence (AGI): AI that can perform virtually any cognitive task as well as a human.\n",
      "\n",
      "Common applications:\n",
      "- Web search engines (e.g., Google Search)\n",
      "- Recommendation systems (YouTube, Amazon, Netflix)\n",
      "- Virtual assistants (Siri, Google Assistant, Alexa)\n",
      "- Autonomous vehicles (Waymo)\n",
      "- Generative and creative tools (language models, AI art)\n",
      "- Strategic game play and analysis (chess, Go)\n",
      "\n",
      "Note: Many AI technologies are deeply integrated into everyday systems and are not always labeled as \"AI\" explicitly.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(f\"What is AI? You can answer using the following context: {context}\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30b1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
